<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic1107cgiks">
<title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 2.0: Deploying the KVM Region in a
    Sacramento Deployment </title>
<prolog>
  <metadata>
    <othermeta name="layout" content="default"/>
    <othermeta name="product-version" content="HP Helion OpenStack Carrier Grade 2.0"/>
    <othermeta name="role" content="Storage Administrator"/>
    <othermeta name="role" content="Storage Architect"/>
    <othermeta name="role" content="Michael B"/>
    <othermeta name="product-version1" content="HP Helion OpenStack Carrier Grade 2.0"/>
  </metadata>
</prolog>
<body>
    <section id="intro">
      <p>After the lifecycle manager is up and running and HP Helion OpenStack is installed, use the
        following steps to deploy the KVM region.</p>
      <p>The process for installing the KVM region involves the following tasks:<ul
          id="ul_fj4_mct_g5">
          <li><xref href="#topic1107cgiks/controller-0" format="dita"/></li>
          <!--<li><xref href="#topic1107cgiks/kvm-custom-patch" format="dita"/></li>-->
        <li><xref href="#topic1107cgiks/kvm-custom-patch" format="dita">Step 2: Apply KVM region
              patches</xref></li>
          <li><xref href="#topic1107cgiks/install-cloud" format="dita"/></li>
          <li><xref href="#topic1107cgiks/unlock" format="dita"/></li>
        </ul></p>
    </section>
    <section id="controller-0">
      <title>Step 1: Bring Up Controller-0 in the KVM Region</title>
      <ol id="ol_mf2_knq_35">
        <li>Make sure the server you will use for controller-0 is powered on and the other servers
          to be used in KVM region are shutdown.</li>
        <li>Launch the iLO interface for the server.</li>
        <li>Execute the following command to use the <codeph>bootimage_full.iso</codeph> to boot the
          controller node.<codeblock>mount bootimage_full.iso</codeblock></li>
        <li>Follow the install wizard. Select the Graphics mode for the controller only. Do not
          select <codeph>Controller+Compute</codeph>.<p>After the installation is complete, you will
            need to reboot the server. After the reboot:</p></li>
      </ol>
      <ol id="controller-0-2">
        <li>Log in as user name <codeph>wrsroot</codeph> and password <codeph>wrsroot</codeph>. Make
          sure you change the password.</li>
        <li>Temporarily assign an IP address to the <codeph>PXE NIC - eth0</codeph>. Use the IP you
          have reserved for the KVM PXE.
          <codeblock>ip addr add &lt;CIDR> dev eth0
ifconfig eth0 up</codeblock></li>
        <li> Set the default gateway to the PXE network gateway
          <codeblock>route add default gw &lt;CIDR_gateway_IP&gt;</codeblock></li>
        <li>Copy the following KVM revion license files from the lifecycle manager to the
            <codeph>/home/wrsroot/</codeph> directory of the <codeph>controller-0</codeph>. <ul
            id="ul_mpn_hnr_bt">
            <li><codeph>license.lic</codeph> Located in the
                <codeph>/root/cg-hlm/windriver-files</codeph> directory</li>
            <li><codeph>region_config</codeph> Located in the
                /<codeph>var/hlm/clouds/&lt;cloud-name&gt;/desired_state/&lt;cloud-name&gt;/001/base/stage/windriver-config</codeph>
              directory</li>
            <li><codeph>cakey.pem</codeph> Located in the
                <codeph>/var/hlm/clouds/&lt;cloud-name&gt;/desired_state/&lt;cloud-name&gt;/001/base/stage/windriver-config</codeph>
              directory</li>
          </ul><p>These files are loaded to the lifecycle manager during the installation
            process.</p></li>
      </ol>
    </section>
    <!--<section id="kvm-custom-patch-old"><title>Step 2: Deploy the KVM Region Custom Patch</title><p>This patch addresses a number of issues in the KVM region.</p><p><ol><li>On KVM region <codeph>controller-0</codeph>, create a directory for the patch:<codeblock>mkdir -p /tmp/patches</codeblock></li><li>Copy the following patch file to the <codeph>/tmp/patches</codeph> directory: <ul id="ul_ald_spz_35"><li><codeph>TS_HP_15_Custom_PATCH_0001_2.patch</codeph><p>You should have downloaded this patch file in the Prerequisites. </p></li></ul></li><li>Execute the following commands: <codeblock>sudo sw-patch upload-dir /tmp/patches
sudo sw-patch query
sudo sw-patch query-hosts
sudo sw-patch apply TS_HP_15_Custom_PATCH_0001_2.patch
source /etc/nova/openrc</codeblock><p>The <codeph>upload-dir</codeph> command registers all patches in the specific directory into an internal database. Once registered, you can delete the patch files from the controller as the patches have been added to the database.</p></li><li>Patch the standby controller:<codeblock>system host-patch-reboot &lt;standby-controller> </codeblock>Assuming controller-0 is active controller:<codeblock>system host-patch-reboot controller-1</codeblock></li><li>After the patched controller node comes up, switch the active controller:<codeblock>system host-swact &lt;active-controller></codeblock><p>Assuming controller-0 is active controller:<codeblock>system host-swact controller-0</codeblock></p></li><li>Exit the SSH session and login to the newly active controller.</li><li>Execute the following commands:<codeblock>source /etc/nova/openrc</codeblock></li><li>Patch the standby controller:<codeblock>system host-patch-reboot &lt;standby-controller> </codeblock></li><li>Reboot the compute nodes:<codeblock>system host-patch-reboot compute-0
system host-patch-reboot compute-1</codeblock></li><li>Make controller-0 active:<codeblock>system host-swact controller-1</codeblock></li></ol></p></section>-->
  <section id="kvm-custom-patch"><title>Step 2: Apply KVM region patches</title>Perform the following steps
      to deploy two required patches.<ol>
        <li>On KVM region <codeph>controller-0</codeph>, create a directory for the
          patch:<codeblock>mkdir -p /tmp/patches</codeblock></li>
        <li>Copy the following patches file to the <codeph>/tmp/patches</codeph> directory: <table
            frame="all" rowsep="1" colsep="1" id="table_wf2_1pt_jv">
            <tgroup cols="2">
              <colspec colname="c1" colnum="1" colwidth="306px"/>
              <colspec colname="c2" colnum="2" colwidth="250px"/>
              <thead>
                <row>
                  <entry>File to copy</entry>
                  <entry>Is in download file</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0001_2.patch</codeph>
                  </entry>
                  <entry>HCG2.0_WR002_Patch005.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0002.patch</codeph>
                  </entry>
                  <entry>HCG2.0_WR003_Patch007.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0003.patch</codeph>
                  </entry>
                  <entry>HCG2.0_WR004_Patch017.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0004.patch</codeph>
                  </entry>
                  <entry>HCG2.0_WR005_Patch019.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0005.patch</codeph>
                  </entry>
                  <entry>HCG2.0_WR006_Patch025.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0006.patch</codeph>
                  </entry>
                  <entry>HCG2.0_WR007_Patch027.tar.gz</entry>
                </row>
              </tbody>
            </tgroup>
          </table><p>You should have downloaded this patch file in the Prerequisites. The download
            file contains a README file that lists the issues fixed/addressed in each
          patch.</p></li>
        <li>Execute the following command to registers all patches in the specific directory into an
          internal database:
            <codeblock>sudo sw-patch upload-dir /tmp/patches                </codeblock><p>Once
            registered, you can delete the patch files from the controller as the patches have been
            added to the database.</p></li>
        <li>Execute the following command to apply all registered patches to the nodes in this
          cluster:<codeblock>sudo sw-patch apply --all</codeblock></li>
        <li>Execute the following command to deploy the
          patch:<codeblock>sudo sw-patch install-local</codeblock>Wait for the <i>Patch installation
            is complete</i> message. This will take few minutes and you will be prompted for
          reboot.</li>
        <li>Reboot the controller:<codeblock>sudo reboot</codeblock></li>
      </ol></section>
    <section id="install-cloud">
      <title>Step 3: Configure the KVM region cloud</title>
      <ol>
        <li>In the iLO web interface, launch a console session to the <codeph>controller-0</codeph>
          node.</li>
        <li>Execute the following command to install the KVM region cloud:<p><b>Important:</b>
            Always execute this command from a console window, not an SSH
              session</p><codeblock>sudo config_region region_config</codeblock><p><image
                href="../../../media/CGH-install-KVM.png" width="500" id="image_c2w_4kk_bt"/></p>Ignore
          the message which displays during <codeph>config_region</codeph> process.
              <codeblock>Step 9 of 29 [####  ]dm-6 WRITE SAME failed. Manually zeroing. </codeblock><p><image
                href="../../../media/CGH-install-kvm-error.png" width="500"/></p></li>
        <li>After <codeph>controller-0</codeph> is deployed, add and configure the remaining nodes
          as <codeph>controller-1</codeph> and <codeph>compute-'n'</codeph>.
            <codeblock>system host-add --hostname controller-1 --personality controller --mgmt_mac &lt;mgmt_mac&gt; --bm_mac &lt;bm_mac&gt; --bm_ip &lt;ilo_ip&gt; --bm_type ilo4 --bm_username &lt;ilo_user&gt; --bm_password &lt;ilo_password&gt;
system host-add --hostname &lt;unique-compute-name&gt; --personality compute --mgmt_mac &lt;mgmt_mac&gt; --bm_mac &lt;ilo_mac&gt; --bm_ip &lt;ilo_ip&gt; --bm_type ilo4 --bm_username &lt;ilo_user&gt;  --bm_password &lt;ilo_password&gt;</codeblock><p>Adding
              <codeph>-c tty0</codeph> to either of these commands will set the system console to
              <codeph>tty0</codeph>, which is the first virtual terminal on the screen of a Linux
            system. If not, the console is set to <codeph>ttyS0</codeph>, which is a serial port.
            You will not see the console messages as the system is being installed or booted up
            after the install unless you use serial port connections for all
              servers.</p><p><b>Note:</b> When adding a controller, the name is assigned in order.
            Unique controller names are ignored.</p><p><image
              href="../../../media/CGH-install-kvm-host-add.png" id="image_tv5_wz1_ct" width="300"
            /></p></li>
        <li>In the iLO web interface, set the controller and compute nodes to one-time PXE booting
          from the network.</li>
        <li>Restart the nodes.</li>
        <li><xref href="../carrier-grade-install-launch-horizon.dita#topic10581cgilh">Access the
            Horizon dashboard</xref> using the CAN network IP (HTTPS). </li>
        <li>Select <b>regionkvm</b> in the <xref
          href="../../OperationsGuide/Dashboard/CGH-2-User-region-selector.dita#jow1432757989702"
            >region selector</xref>.</li>
        <li>Click <b>Admin</b> then <b>Inventory</b> to monitor the status of nodes being PXE
          booted. After a succesful PXE boot, the <b>Operational State</b> should be <b>Disabled</b>
          and the <b>Availability State</b> should be <b>Online</b>. <p>
            <codeblock>system host-list
+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | controller-1 | controller  | locked         | disabled    | offline      |
+----+--------------+-------------+----------------+-------------+--------------+</codeblock>
          </p></li>
        <li>Execute the following command from controller-0 console to unlock
              controller-1:<codeblock>system host-unlock controller-1</codeblock><p><b>Note:</b>The
            Horizon Dashboard will be running on the first IP address of the CLM network range
            (refer to the <codeph>ip_start_address</codeph> value provided for the CLM network in
            the <codeph>definition.json</codeph> during the non-KVM Region deployment). </p><p>The
            log-in credentials for Horizon are as
            follows:</p><codeblock>username: admin 
password:  &lt;random></codeblock><p>The Horizon
            password is randomly generated during the installation. You can locate the password in
            the <codeph>stackrc</codeph> file on any of the non-KVM region controllers.</p></li>
      </ol>
    </section>
    <section id="unlock"><title>Step 4: Configure and unlock the compute controllers</title><p>After
        the controller-1 and the compute nodes come up successfully after PXE boot, execute the
        following commands from controller-0 to unlock each compute node in the inventory.</p><ul
        id="ul_f1c_dk1_h5">
        <li><xref href="#topic1107cgiks/kvm-esx-unlock" format="dita">For a Non-Bonded
            Environment</xref></li>
        <li><xref href="#topic1107cgiks/nic-unlock" format="dita">For a Bonded
          Environment</xref></li>
      </ul><p id="kvm-esx-unlock">
        <b>For a Non-Bonded Environment</b></p>After the controller-1 and the compute nodes come up
      successfully after PXE boot, execute the following commands from controller-0 to unlock the
      compute nodes.<p>If you are using a non-bonded NIC environment, perform these steps to create
        the interfaces.:</p><ol id="ol_cjh_rys_gv">
        <li>From the KVM region controller, lock the compute-1
          node.<codeblock>system host-lock compute-1</codeblock></li>
        <li>Execute the following command to create an interface from compute-0 to the
          infrastructure
          network:<codeblock>system host-if-add -V &lt;vlan-id> -nt infra compute-0 &lt;name> vlan pxeboot0</codeblock>Where:
            <ul id="ul_edw_czs_gv">
            <li><codeph>vlan-id</codeph> is the VLAN ID;</li>
            <li><codeph>infra</codeph> is the type of network;</li>
            <li><codeph>compute-0</codeph> is the name of the compute node where the interface will
              be created;</li>
            <li><codeph>name</codeph> is the name of the interface;</li>
            <li><codeph>vlan</codeph> is the type of interface;</li>
            <li><codeph>pxeboot0</codeph> is the name of the interface to attach.</li>
          </ul> For
          example:<codeblock>system host-if-add -V 539 -nt infra compute-0 infra vlan pxeboot0</codeblock></li>
        <li>Execute the following command to assign the infra IP address to
            compute-0:<codeblock>system host-addr-add compute-0 infra &lt;IP_address></codeblock><p>For
            example:<codeblock>system host-addr-add compute-0 infra 10.70.22.81 24</codeblock></p></li>
        <li>Specify MTU for the compute-0
          node.<codeblock>system host-if-modify &lt;host-name or ID> -n &lt;interface_name> -nt &lt;network_type> --ipv4-mode=static eth-x -m &lt;MTU Size in bytes> </codeblock>Where
            <codeph>eth-x</codeph> is the interface on which untagged TUL is presented.<note>For a
            VXLAN network, the frame is either 54 or 74 bytes long, depending on whether IPv4 or
            IPv6 protocol is used. This is because, in addition to the Ethernet header and CRC, the
            payload is enclosed by an IP header (20 bytes for IPv4 or 40 bytes for IPv6), a UDP
            header (8 bytes), and a VXLAN header (8 bytes). The additional IP, UDP, and VXLAN
            headers are invisible to the <codeph>vrs-data</codeph> interface, which expects a frame
            only 18 bytes larger than the MTU. To accommodate the larger frames on a VXLAN network,
            you must specify a larger nominal MTU on the <codeph>vrs-data</codeph> interface. For
            simplicity, and to avoid issues with stacked VLAN tagging, some third party vendors
            recommend rounding up by an additional 100 bytes for calculation purposes. For example,
            to attach to a VXLAN provider network with an MTU of 1500, a <codeph>vrs-data</codeph>
            interface with an MTU of 1600 is recommended.</note><p>For
            example:<codeblock>system host-if-modify compute-0 -n vrs -nt data-vrs --ipv4-mode=static eth4 -m 1600</codeblock></p></li>
        <li>Assign the VRS interface IP address to compute-0
            <codeblock>system host-addr-add compute-0 vrs &lt;ipv4/ipv6_address> &lt;prefix_length></codeblock><p>For
            example:<codeblock>system host-addr-add compute-0 vrs 10.70.5.83 24</codeblock></p></li>
        <li>Assign the VSC Controller IP address to compute-0.
            <codeblock>system host-update &lt;host-name or ID> vsc_controllers=&lt;ipv4/ipv6_address>,&lt;ipv4/ipv6_address> </codeblock><p>For
            example:
            <codeblock>system host-update compute-0 vsc_controllers=10.70.5.31,10.70.5.32</codeblock></p></li>
        <li>Unlock the compute-0 node.<codeblock>system host-unlock compute-0</codeblock></li>
        <li>Lock the compute-1 node.<codeblock>system host-lock compute-1</codeblock></li>
        <li>Repeat commands above to create interfaces on compute-1 to the infrastructure network
          and the data network. <p>For
          example:</p><codeblock>system host-if-add –V 722 –nt infra compute-1 infra vlan pxeboot0
system host-addr-add compute-1 infra 10.70.22.81 24
system host-if-modify compute-1 -n vrs -nt data-vrs --ipv4-mode=static eth4 -m 1600
system host-addr-add compute-1 vrs 10.70.5.83 24
system host-update compute-1 vsc_controllers=10.70.5.31,10.70.5.32</codeblock></li>
        <li>Unlock the compute-1 node.<codeblock>system host-unlock compute-1</codeblock></li>
      </ol><p id="nic-unlock">
        <b>For bonded NIC environments</b></p><p>If you are using a bonded NIC environment, after
        configuring and unlocking the compute nodes, perform these steps:</p><ol id="ol_nr5_ddw_mt">
        <li>Use the following commands to delete the MGMT interface: <ol id="ol_zfq_5h1_tt">
            <li>Obtain the compute name or ID number:<codeblock>system host-list</codeblock></li>
            <li>Obtain the UUID of the MGMT interface to
              remove:<codeblock>system host-if-list</codeblock></li>
            <li>Remove the PXE boot flag off the
              interface:<codeblock>system host-if-modify -nt "none" compute-0 pxeboot0</codeblock></li>
            <li>Delete the
              interface:<codeblock>system host-if-delete compute-0 95e7b495-1454-49d1-8123-301215503e86      </codeblock></li>
          </ol></li>
        <li>Execute the following commands to create a bonded interface<ol id="ol_xyc_fww_dv">
            <li>From the KVM region controller, lock the compute-0
              node.<codeblock>system host-lock compute-0</codeblock></li>
            <li>Create a bonded interface for the control plane:
              <codeblock>system host-if-add -a 802.3ad -x layer2 -nt pxeboot compute-0 bond0 ae none eth0 eth1</codeblock></li>
            <li>Create a bonded interface to the management
              network:<codeblock>system host-if-add -V &lt;vlan-id> -nt mgmt compute-0 mgmt vlan bond0</codeblock>Where:
                <ul id="ul_q4r_4jv_hv">
                <li><codeph>vlan-id</codeph> is the VLAN ID;</li>
                <li><codeph>mgmt</codeph> is the type of network;</li>
                <li><codeph>compute-0</codeph> is the name of the compute node where the interface
                  will be created;</li>
                <li><codeph>mgmt</codeph> is the name of the interface;</li>
                <li><codeph>vlan</codeph> is the type of interface;</li>
                <li><codeph>bond0</codeph> is the name of the bonded interface.</li>
              </ul></li>
            <li>Create a bonded interface to the infrastructure interface using the variables
              defined in the previous
              step:<codeblock>system host-if-add -V &lt;vlan-id> -nt infra compute-0 &lt;name> vlan bond0</codeblock></li>
            <li>Execute the following command to assign the infra IP address to
              compute-0:<codeblock>system host-addr-add compute-0 infra 172.16.74.244 24</codeblock>Where:
                <ul id="ul_qt5_yjv_hv">
                <li><codeph>infra</codeph> is the type of network;</li>
              </ul></li>
          </ol></li>
        <li>Use the following commands to configure MTU:
            <codeblock>system host-if-modify -m 1600 compute-0 eth2
system host-if-modify -m 1600 compute-0 eth3
system host-if-add -a balanced -x layer2 -nt "none" compute-0 bondvrs ae "none" eth2 eth3 -m 1600</codeblock><note>For
            a VXLAN network, the frame is either 54 or 74 bytes long, depending on whether IPv4 or
            IPv6 protocol is used. This is because, in addition to the Ethernet header and CRC, the
            payload is enclosed by an IP header (20 bytes for IPv4 or 40 bytes for IPv6), a UDP
            header (8 bytes), and a VXLAN header (8 bytes). The additional IP, UDP, and VXLAN
            headers are invisible to the <codeph>vrs-data</codeph> interface, which expects a frame
            only 18 bytes larger than the MTU. To accommodate the larger frames on a VXLAN network,
            you must specify a larger nominal MTU on the <codeph>vrs-data</codeph> interface. For
            simplicity, and to avoid issues with stacked VLAN tagging, some third party vendors
            recommend rounding up by an additional 100 bytes for calculation purposes. For example,
            to attach to a VXLAN provider network with an MTU of 1500, a <codeph>vrs-data</codeph>
            interface with an MTU of 1600 is recommended.</note></li>
        <li>Use one of the following steps to configure a static IP address for the bonded NIC
          interface or assign an IP address from an IP address pool:<ul id="ol_uqd_l5z_st">
            <li>To assign a static IP:
              <codeblock>system host-if-modify compute-0 -nt data-vrs --ipv4-mode=static bondvrs
system host-addr-add compute-0 bondvrs 10.30.6.244 24</codeblock></li>
            <li>To assign an IP address from a pool:<p>
                <codeblock>system addrpool-add vrspool 10.30.6.0 24 --order random --ranges 10.30.6.240-10.30.6.249
system host-if-modify compute-0 bondvrs -nt data-vrs --ipv4-mode=pool --ipv4-pool=vrspool
system host-if-modify compute-1 bondvrs -nt data-vrs --ipv4-mode=pool --ipv4-pool=vrspool</codeblock>
              </p></li>
          </ul></li>
        <li>Assign the VRS interface IP address to
          compute-0:<codeblock>system host-update compute-0 vsc_controllers=10.30.6.31,10.30.6.32</codeblock></li>
        <li>Unlock the compute-0 node.<codeblock>system host-unlock compute-0</codeblock></li>
        <li>Lock the compute-1 node.<codeblock>system host-lock compute-1</codeblock></li>
        <li>Repeat the steps above on
          compute-1:<codeblock>system host-if-add -a 802.3ad -x layer2 -nt pxeboot compute-1 bond0 ae none eth0 eth1
system host-if-add -V 532 -nt mgmt compute-1 mgmt vlan bond0
system host-if-add -V 539 -nt infra compute-1 infra vlan bond0
system host-addr-add compute-1 infra 172.16.74.244 24
system host-if-modify -m 1600 compute-0 eth2
system host-if-modify -m 1600 compute-0 eth3
system host-if-add -a balanced -x layer2 -nt "none" compute-0 bondvrs ae "none" eth2 eth3 -m 1600</codeblock></li>
        <li>Use one of the following steps to configure a static IP address for the bonded NIC
          interface or assign an IP address from an IP address pool:<ul id="ul_lnk_lt3_kv">
            <li>To assign a static IP:
              <codeblock>system host-if-modify compute-0 -nt data-vrs --ipv4-mode=static bondvrs
system host-addr-add compute-0 bondvrs 10.30.6.244 24</codeblock></li>
            <li>To assign an IP address from a pool:<p>
                <codeblock>system addrpool-add vrspool 10.30.6.0 24 --order random --ranges 10.30.6.240-10.30.6.249
system host-if-modify compute-0 bondvrs -nt data-vrs --ipv4-mode=pool --ipv4-pool=vrspool
system host-if-modify compute-1 bondvrs -nt data-vrs --ipv4-mode=pool --ipv4-pool=vrspool</codeblock>
              </p></li>
          </ul></li>
        <li>Assign the VRS interface IP address to
          compute-0:<codeblock>system host-update compute-0 vsc_controllers=10.30.6.31,10.30.6.32</codeblock></li>
        <li>Unlock the compute-1 node.<codeblock>system host-unlock compute-1</codeblock></li>
      </ol></section>
    <section id="next-step">
      <title>Next Steps</title>
      <p><xref href="../carrier-grade-install-post.dita#topic1107cgipd">Post-Installation
        Tasks</xref></p>
    </section>
  </body>
</topic>
