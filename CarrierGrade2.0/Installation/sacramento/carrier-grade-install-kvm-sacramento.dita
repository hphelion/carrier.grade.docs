<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic1107cgiks">
<title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 2.0: Deploying the KVM Region in a
    Sacramento Deployment </title>
<prolog>
  <metadata>
    <othermeta name="layout" content="default"/>
    <othermeta name="product-version" content="HP Helion OpenStack Carrier Grade 2.0"/>
    <othermeta name="role" content="Storage Administrator"/>
    <othermeta name="role" content="Storage Architect"/>
    <othermeta name="role" content="Michael B"/>
    <othermeta name="product-version1" content="HP Helion OpenStack Carrier Grade 2.0"/>
  </metadata>
</prolog>
<body>
    <section id="intro">
      <p>After the lifecycle manager is up and running and HP Helion OpenStack is installed, use the
        following steps to deploy the KVM region.</p>
      <p>The process for installing the KVM region involves the following tasks:<ul
          id="ul_fj4_mct_g5">
          <li><xref href="#topic1107cgiks/controller-0" format="dita"/></li>
          <!--<li><xref href="#topic1107cgiks/kvm-custom-patch" format="dita"/></li>-->
        <li><xref href="#topic1107cgiks/kvm-custom-patch" format="dita">Step 2: Apply KVM region
              patches</xref></li>
          <li><xref href="#topic1107cgiks/install-cloud" format="dita"/></li>
          <li><xref href="#topic1107cgiks/unlock" format="dita"/></li>
        </ul></p>
    </section>
    <section id="controller-0">
      <title>Step 1: Bring Up Controller-0 in the KVM Region</title>
      <p>For installing controller-0 on a baremetal system with no OS:</p>
      <ol id="ol_mf2_knq_35">
        <li> On your IE browser ( recommended IE 11), enter the IP address to access the iLO
          console.</li>
        <li>Login with your unique iLO user name and password associated with your server.<p><image
              id="image_utc_bqz_1x"
              href="../../../media/carrier.grade.docs/2.1/CGH-2.1-ILO-login-install-KVM-1.png"
              width="600"/></p><p/></li>
        <li>The initial overview of the iLO console is displayed.<p><image id="image_n35_gb1_bx"
              href="../../../media/carrier.grade.docs/2.1/CGH-2.1-ILO-initial-system-overview-install-KVM-2.png"
              width="600"/></p></li>
        <li>Select <b>Remote Console</b> on the left hand side of the ILO interface and click
            <b>Launch</b>.<p><image id="image_hdz_2c1_bx"
              href="../../../media/carrier.grade.docs/2.1/CGH-2.1-ILO-remote-console-menu-install-KVM-3.png"
              width="600"/></p></li>
        <li>A "Security Warning" window appears, click Run to run the iLO remote console
              application.<p><image
              href="../../../media/carrier.grade.docs/2.1/CGH-2.1-IE-application-sec-warn-install-KVM-4.png"
              width="600" id="image_km5_4md_bx"/></p></li>
        <li>The "iLO Integrated Remote Console" application window is displayed.<p><image
              id="image_dy2_jf1_bx"
              href="../../../media/carrier.grade.docs/2.1/CGH-2.1-ILO-remote-console-initial-screen-Install-KVM-5.png"
              width="600"/></p></li>
        <li>Select the "Virtual Drives" to mount the image file (CD-ROM/DVD).<p><image
              id="image_cs5_zj1_bx"
              href="../../../media/carrier.grade.docs/2.1/CGH-2.1-remote-console-select-VirtDrive-install-KVM-6.png"
              width="600"/></p></li>
        <li>Select the ISO image (bootimage_avs_only) and click <b>Open</b>.<p><image
              id="image_bct_gl1_bx"
              href="../../../media/carrier.grade.docs/2.1/CGH-2.1-ILO-select-virtCD-image-file-install-KVM-8.png"
              width="600"/></p></li>
        <li>After mounting the boot image, select the "Momentary Press" under "Power Switch" menu.<p>
            <note>The server that is used for controller-0 should not be powered on until after the
              virtual CD-ROM image file is mounted through ILO using the controls and features of
              the Virtual Drive menu in the ILO remote console. Use the "Power Switch" menu at the
              top of the remote console app to power on the system, allow the BIOS post and then
              boot to the ISO image of the KVM region installer. </note>
          </p><p><image id="image_al4_b41_bx"
              href="../../../media/carrier.grade.docs/2.1/CGH-2.1-ILO-remote-console-poweronsyst-install-KVM-9.png"
              width="600"/></p></li>
        <li>Follow the installer wizard. Select the "Graphics Text Controller Node Install" for the
          controller only. Do not select Controller+Compute. <p><image id="image_mtj_fp1_bx"
              href="../../../media/carrier.grade.docs/2.1/CGH-2.1-WR-installer-screen-install-KVM-10.png"
              width="600"/></p></li>
        <li>If the server does not automatically boot from CD-ROM, press F11 during the BIOS post to
          get to the Boot Options menu and further choose the "One Time Boot to CD-ROM" from the
          options in the default boot menu.<p><image id="image_php_qp1_bx"
              href="../../../media/carrier.grade.docs/2.1/CGH-2.1-syst-bios-bootmenu-install-KVM-11.png"
              width="600"/></p></li>
        <li>Once the system is turned on, the following text is displayed from the BIOS post
              operations.<p><image id="image_zfp_1q1_bx"
              href="../../../media/carrier.grade.docs/2.1/CGH-2.1-system-bios-install-KVM-12.png"
              width="600"/></p><note>If you pressed F11 to navigate to the boot menu and chose to
            boot the ISO mounted image through the virtual drive, then you will be directed to the
            KVM region installer wizard as shown in the Step 10 where you can select the "Graphics
            Text Controller Node Install" for the controller only.</note></li>
      </ol>
      <p>After the installation is complete, you will need to reboot the server. After the
        reboot:</p>
      <ol id="controller-0-2">
        <li>Log in as user name <codeph>wrsroot</codeph> and password <codeph>wrsroot</codeph>. Make
          sure you change the password.</li>
        <li>Temporarily assign an IP address to the <codeph>PXE NIC - eth0</codeph>. Use the IP you
          have reserved for the KVM PXE.
          <codeblock>ip addr add &lt;CIDR> dev eth0
ifconfig eth0 up</codeblock></li>
        <li> Set the default gateway to the PXE network gateway
          <codeblock>route add default gw &lt;CIDR_gateway_IP&gt;</codeblock></li>
        <li>Copy the following KVM region license files from the lifecycle manager to the
            <codeph>/home/wrsroot/</codeph> directory of the <codeph>controller-0</codeph>. <ul
            id="ul_mpn_hnr_bt">
            <li><codeph>license.lic</codeph> Located in the
                <codeph>/root/cg-hlm/windriver-files</codeph> directory</li>
            <li><codeph>region_config</codeph> Located in the
                /<codeph>var/hlm/clouds/&lt;cloud-name&gt;/desired_state/&lt;cloud-name&gt;/001/base/stage/windriver-config</codeph>
              directory</li>
            <li><codeph>cakey.pem</codeph> Located in the
                <codeph>/var/hlm/clouds/&lt;cloud-name&gt;/desired_state/&lt;cloud-name&gt;/001/base/stage/windriver-config</codeph>
              directory</li>
          </ul><p>These files are loaded to the lifecycle manager during the installation
            process.</p></li>
      </ol>
    </section>
    <!--<section id="kvm-custom-patch-old"><title>Step 2: Deploy the KVM Region Custom Patch</title><p>This patch addresses a number of issues in the KVM region.</p><p><ol><li>On KVM region <codeph>controller-0</codeph>, create a directory for the patch:<codeblock>mkdir -p /tmp/patches</codeblock></li><li>Copy the following patch file to the <codeph>/tmp/patches</codeph> directory: <ul id="ul_ald_spz_35"><li><codeph>TS_HP_15_Custom_PATCH_0001_2.patch</codeph><p>You should have downloaded this patch file in the Prerequisites. </p></li></ul></li><li>Execute the following commands: <codeblock>sudo sw-patch upload-dir /tmp/patches
sudo sw-patch query
sudo sw-patch query-hosts
sudo sw-patch apply TS_HP_15_Custom_PATCH_0001_2.patch
source /etc/nova/openrc</codeblock><p>The <codeph>upload-dir</codeph> command registers all patches in the specific directory into an internal database. Once registered, you can delete the patch files from the controller as the patches have been added to the database.</p></li><li>Patch the standby controller:<codeblock>system host-patch-reboot &lt;standby-controller> </codeblock>Assuming controller-0 is active controller:<codeblock>system host-patch-reboot controller-1</codeblock></li><li>After the patched controller node comes up, switch the active controller:<codeblock>system host-swact &lt;active-controller></codeblock><p>Assuming controller-0 is active controller:<codeblock>system host-swact controller-0</codeblock></p></li><li>Exit the SSH session and login to the newly active controller.</li><li>Execute the following commands:<codeblock>source /etc/nova/openrc</codeblock></li><li>Patch the standby controller:<codeblock>system host-patch-reboot &lt;standby-controller> </codeblock></li><li>Reboot the compute nodes:<codeblock>system host-patch-reboot compute-0
system host-patch-reboot compute-1</codeblock></li><li>Make controller-0 active:<codeblock>system host-swact controller-1</codeblock></li></ol></p></section>-->
  <section id="kvm-custom-patch"><title>Step 2: Apply KVM region patches</title>Perform the
      following steps to deploy the required patches. These steps are valid for any new install
      where software is loaded onto controller-0 and config_region script has not been executed on
      the Controller-0 in the KVM region.<note>The <codeph>upload-dir</codeph> operation registers
        all patches in the specific directory into the database. Once registered, you do not need to
        retain the files anymore; they are available in the database. The <codeph>apply
          --all</codeph> operation tells the system to apply all registered patches to the nodes in
        this cluster . The <codeph>install-local</codeph> operation tells the system to install all
        necessary patches on the un-configured local node.</note><ol>
        <li>Log into KVM region <codeph>controller-0</codeph> .<note>Run the steps from console
            only.</note></li>
        <li>Change the password when/if prompted.</li>
        <li>Create a directory for the patch:<codeblock>mkdir -p /tmp/patches</codeblock></li>
        <li>Copy the following patches file to the <codeph>/tmp/patches</codeph> directory. For
          information on the issues fixed with each patch, refer to the README that comes with each
          patch: <table frame="all" rowsep="1" colsep="1" id="table_wf2_1pt_jv">
            <tgroup cols="2">
              <colspec colname="c1" colnum="1" colwidth="324px"/>
              <colspec colname="c2" colnum="2" colwidth="250px"/>
              <thead>
                <row>
                  <entry>File to copy</entry>
                  <entry>Is in download file</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0001_2.patch</codeph>
                  </entry>
                  <entry>HCG_2.0_WR002_Patch005.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0002.patch</codeph>
                  </entry>
                  <entry>HCG_2.0_WR003_Patch007.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0003.patch</codeph>
                  </entry>
                  <entry>HCG_2.0_WR004_Patch017.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0004.patch</codeph>
                  </entry>
                  <entry>HCG_2.0_WR005_Patch019.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0005.patch</codeph>
                  </entry>
                  <entry>HCG_2.0_WR006_Patch025.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0006.patch</codeph>
                  </entry>
                  <entry>HCG_2.0_WR007_Patch027.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0007.patch</codeph>
                  </entry>
                  <entry>HCG_2.0_WR008_Patch029.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0008.patch</codeph></entry>
                  <entry>HCG_2.0_WR009_Patch030.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_VRS_0001.patch</codeph></entry>
                  <entry>DCN 3.2R7 download (This patch doesn't apply for Denver topology)</entry>
                </row>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0009.patch</codeph></entry>
                  <entry>HCG_2.0_WR010_Patch032.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0010.patch</codeph></entry>
                  <entry>HCG_2.0_WR011_Patch033.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0011.patch</codeph></entry>
                  <entry>HCG_2.0_WR012_Patch035.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0012.patch</codeph></entry>
                  <entry>HCG_2.0_WR013_Patch036.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>TS_HP_15_Custom_PATCH_0013.patch</codeph></entry>
                  <entry>HCG_2.0_WR014_Patch038.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>[TS_HP_15_Custom_PATCH_0014.patch],
                      [TS_HP_15_Custom_PATCH_0015.patch]</codeph></entry>
                  <entry>HCG_2.0_WR014_WR015_Patch040.tar.gz</entry>
                </row>
                <row>
                  <entry><codeph>[TS_HP_15_Custom_PATCH_0016.patch]</codeph></entry>
                  <entry>HCG_2.0_WR016_Patch041.tar.gz</entry>
                </row>
              </tbody>
            </tgroup>
          </table><p>
            <note type="caution">Do not install KVM<b> patch 14-15 and patch 16</b> from Horizon.
              Install ONLY from CLI. The nodes go to unstable state if patch is installed from
              Horizon and the nodes have to be recovered during that time. </note>
          </p><note type="important"> DCN 3.2R7 environments only. Locate the
              <codeph>hcg2_dcn_32r7.tar.gz</codeph> file you downloaded in the <b>Download the
              Installation Packages</b> steps and extract the patch file. </note><p>You should have
            downloaded this patch file in the Prerequisites. The download file contains a README
            file that lists the issues fixed/addressed in each patch.</p></li>
        <li>Execute the following command to register all patches in the specific directory into an
          internal database:
            <codeblock>sudo sw-patch upload-dir /tmp/patches                </codeblock><p>Once
            registered, you can delete the patch files from the controller as the patches have been
            added to the database.</p></li>
        <li>Execute the following command to apply all registered patches to the nodes in this
          cluster:<codeblock>sudo sw-patch apply --all</codeblock></li>
        <li>Execute the following command to deploy the
          patch:<codeblock>sudo sw-patch install-local</codeblock>Wait for the <i>Patch installation
            is complete</i> message. This will take few minutes and you will be prompted for
          reboot.</li>
        <li>Reboot the controller:<codeblock>sudo reboot</codeblock></li>
        <li>Continue setting up controller-0 and the rest of the cloud as normal.</li>
      </ol></section>
    <section id="install-cloud">
      <title>Step 3: Configure the KVM region cloud</title>
      <ol>
        <li>In the iLO web interface, launch a console session to the <codeph>controller-0</codeph>
          node.</li>
        <li>Execute the following command to install the KVM region cloud:<p><b>Important:</b>
            Always execute this command from a console window, not an SSH
              session</p><codeblock>sudo config_region region_config</codeblock><p><image
                href="../../../media/CGH-install-KVM.png" width="500" id="image_c2w_4kk_bt"/></p>Ignore
          the message which displays during <codeph>config_region</codeph> process.
              <codeblock>Step 9 of 29 [####  ]dm-6 WRITE SAME failed. Manually zeroing. </codeblock><p><image
                href="../../../media/CGH-install-kvm-error.png" width="500"/></p></li>
        <li>After <codeph>controller-0</codeph> is deployed, add and configure the remaining nodes
          as <codeph>controller-1</codeph> and <codeph>compute-'n'</codeph>. When adding a
          controller, the name is assigned in order. Unique controller names are ignored.<codeblock>system host-add --hostname controller-1 --personality controller --mgmt_mac &lt;mgmt_mac&gt; --bm_mac &lt;bm_mac&gt; --bm_ip &lt;ilo_ip&gt; --bm_type ilo4 --bm_username &lt;ilo_user&gt; --bm_password &lt;ilo_password&gt;
system host-add --hostname &lt;unique-compute-name&gt; --personality compute --mgmt_mac &lt;mgmt_mac&gt; --bm_mac &lt;ilo_mac&gt; --bm_ip &lt;ilo_ip&gt; --bm_type ilo4 --bm_username &lt;ilo_user&gt;  --bm_password &lt;ilo_password&gt;</codeblock>
          <image
            href="../../../media/carrier.grade.docs/2.1/CGH-2.1-Configure-the-KVM-Region-cloud.png"
            width="800" id="image_lkm_2ck_fx"/>
          <note>If you include <codeph>-c tty0</codeph> to either of these commands, the system
            console will be set to <codeph>tty0</codeph> which is the first virtual terminal on the
            screen of a Linux system. If you do not specify this value, the default setting of the
            console will be <codeph>ttyS0</codeph>, which is a serial port. You will not be able to
            see console messages as the system is being installed or booted up after the install
            unless you are using serial port connections for all servers</note><note>If the console
            output setting needs to be changed after a node has already been defined using the above
              <codeph>system host-add</codeph> syntax, it can be accomplished in two ways: Either
            delete and re-add the node or just alter the console setting for that node. To delete
            the node the <codeph>system host-list</codeph> command should be used to confirm the
            node ids and names.<p><image
                href="../../../media/carrier.grade.docs/2.1/CGH-2.1-system-host-list-install-kvm.png"
                width="800" id="image_wbb_1k3_hx"/></p></note><p>Confirm the <b>host ID</b> or
              <b>hostname</b> and then use the <codeph>system host-delete</codeph> command to delete
            the node definition. This can be done using the <b>host ID</b> or the
            hostname.<codeblock>[root@controller-0 ~(keystone_admin)]# system host-delete 5 
Deleted host 5
[root@controller-0 ~(keystone_admin)]# system host-delete compute-1 
Deleted host compute-1</codeblock></p><p>You
            can also change the actual setting of the console output using the <codeph>system
              host-update</codeph> command. Even in this case, the actual <b>host ID</b> or
              <b>hostname</b> will be used along with the name of the attribute that will be
            updated. During this time, the console attribute setting changes from the default value
            of <b>ttyS0,115200</b> to
            <b>tty0</b>.</p><codeblock>[root@controller-0 ~(keystone_admin)]# system host-update compute-1 console=tty0
+---------------------+--------------------------------------+
| Property            | Value                                |
+---------------------+--------------------------------------+
| action              | none                                 |
| administrative      | locked                               |
| availability        | offline                              |
| bm_ip               | 10.1.69.27                           |
| bm_mac              | 10:60:4b:a0:e4:7d                    |
| bm_type             | ilo4                                 |
| bm_username         | cghelion                             |
| boot_device         | sda                                  |
| capabilities        | {}                                   |
| console             | tty0                                 |
| created_at          | 2016-09-07T19:34:08.593609+00:00     |
| cstatus             | None                                 |
| hostname            | compute-1                            |
| iconfig_applied     | None                                 |
| iconfig_fini        | None                                 |
| iconfig_target      | None                                 |
| id                  | 8                                    |
| install_output      | text                                 |
| invprovision        | None                                 |
| location            | {}                                   |
| mgmt_ip             | None                                 |
| mgmt_mac            | 14:58:d0:52:ab:02                    |
| operational         | disabled                             |
| personality         | compute                              |
| reserved            | False                                |
| rootfs_device       | sda                                  |
| serialid            | None                                 |
| task                | None                                 |
| ttys_dcd            | None                                 |
| updated_at          | None                                 |
| uptime              | 0                                    |
| uuid                | 1ae1d25f-8797-4ec5-b5c4-fbd7f4d4a7d0 |
| vim_progress_status | None                                 |
+---------------------+--------------------------------------+</codeblock><p>The
            current values of all the attributes of a particular host can be viewed using the
              <codeph>system host-show</codeph>
          command.</p><codeblock>[root@controller-0 ~(keystone_admin)]# system host-show compute-1
+---------------------+--------------------------------------+
| Property            | Value                                |
+---------------------+--------------------------------------+
| action              | none                                 |
| administrative      | locked                               |
| availability        | offline                              |
| bm_ip               | 10.1.69.27                           |
| bm_mac              | 10:60:4b:a0:e4:7d                    |
| bm_type             | ilo4                                 |
| bm_username         | cghelion                             |
| boot_device         | sda                                  |
| capabilities        | {u'bm_region': u'External'}          |
| console             | tty0                                 |
| created_at          | 2016-09-07T19:34:08.593609+00:00     |
| cstatus             | None                                 |
| hostname            | compute-1                            |
| iconfig_applied     | None                                 |
| iconfig_fini        | None                                 |
| iconfig_target      | None                                 |
| id                  | 8                                    |
| install_output      | text                                 |
| invprovision        |                                      |
| location            | {}                                   |
| mgmt_ip             | None                                 |
| mgmt_mac            | 14:58:d0:52:ab:02                    |
| operational         | disabled                             |
| personality         | compute                              |
| reserved            | False                                |
| rootfs_device       | sda                                  |
| serialid            | None                                 |
| task                | None                                 |
| ttys_dcd            | None                                 |
| updated_at          | 2016-09-07T19:34:40.369802+00:00     |
| uptime              | 0                                    |
| uuid                | 1ae1d25f-8797-4ec5-b5c4-fbd7f4d4a7d0 |
| vim_progress_status | None                                 |
+---------------------+--------------------------------------+</codeblock></li>
        <li>In the iLO web interface, set the controller and compute nodes to one-time PXE booting
          from the network.</li>
        <li>Restart the nodes.</li>
        <li><xref href="../carrier-grade-install-launch-horizon.dita#topic10581cgilh">Access the
            Horizon dashboard</xref> using the CAN network IP (HTTPS). </li>
        <li>Select <b>regionkvm</b> in the <xref
          href="../../OperationsGuide/Dashboard/CGH-2-User-region-selector.dita#jow1432757989702"
            >region selector</xref>.</li>
        <li>Click <b>Admin</b> then <b>Inventory</b> to monitor the status of nodes being PXE
          booted. After a succesful PXE boot, the <b>Operational State</b> should be <b>Disabled</b>
          and the <b>Availability State</b> should be <b>Online</b>. <p>
            <codeblock>system host-list </codeblock>
            <codeblock>+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | controller-1 | controller  | locked         | disabled    | online       |
| 3  | comp1        | compute     | locked         | disabled    | online       |
| 4  | comp2        | compute     | locked         | disabled    | online       |
| 5  | comp3        | compute     | locked         | disabled    | online       |
+----+--------------+-------------+----------------+-------------+--------------+</codeblock>
          </p></li>
        <li>Execute the following command from controller-0 console to unlock
              controller-1:<codeblock>system host-unlock controller-1</codeblock><p><b>Note:</b>The
            Horizon Dashboard will be running on the first IP address of the CLM network range
            (refer to the <codeph>ip_start_address</codeph> value provided for the CLM network in
            the <codeph>definition.json</codeph> during the non-KVM Region deployment). </p><p>The
            log-in credentials for Horizon are as
            follows:</p><codeblock>username: admin 
password:  &lt;random></codeblock><p>The Horizon
            password is randomly generated during the installation. You can locate the password in
            the <codeph>stackrc</codeph> file on any of the non-KVM region controllers.</p></li>
      </ol>
    </section>
    <section id="unlock"><title>Step 4: Configure and unlock the compute controllers</title><p>After
        the controller-1 and the compute nodes come up successfully after PXE boot, execute the
        following commands from controller-0 to unlock each compute node in the inventory.</p><ul
        id="ul_f1c_dk1_h5">
        <li><xref href="#topic1107cgiks/kvm-esx-unlock" format="dita">For a Non-Bonded
            Environment</xref></li>
        <li><xref href="#topic1107cgiks/nic-unlock" format="dita">For a Bonded
          Environment</xref></li>
      </ul><p id="kvm-esx-unlock">
        <b>For a Non-Bonded Environment</b></p>After the controller-1 and the compute nodes come up
      successfully after PXE boot, execute the following commands from controller-0 to unlock the
      compute nodes.<p>If you are using a non-bonded NIC environment, perform these steps to create
        the interfaces.:</p><ol id="ol_cjh_rys_gv">
        <li>From the KVM region controller, lock the compute-1
          node.<codeblock>system host-lock compute-1</codeblock></li>
        <li>Execute the following command to create an interface from compute-0 to the
          infrastructure
          network:<codeblock>system host-if-add -V &lt;vlan-id> -nt infra compute-0 &lt;name> vlan pxeboot0</codeblock>Where:
            <ul id="ul_edw_czs_gv">
            <li><codeph>vlan-id</codeph> is the VLAN ID;</li>
            <li><codeph>infra</codeph> is the type of network;</li>
            <li><codeph>compute-0</codeph> is the name of the compute node where the interface will
              be created;</li>
            <li><codeph>name</codeph> is the name of the interface;</li>
            <li><codeph>vlan</codeph> is the type of interface;</li>
            <li><codeph>pxeboot0</codeph> is the name of the interface to attach.</li>
          </ul> For
          example:<codeblock>system host-if-add -V 539 -nt infra compute-0 infra vlan pxeboot0</codeblock></li>
        <li>Execute the following command to assign the infra IP address to
            compute-0:<codeblock>system host-addr-add compute-0 infra &lt;IP_address></codeblock><p>For
            example:<codeblock>system host-addr-add compute-0 infra 10.70.22.81 24</codeblock></p></li>
        <li>Specify MTU for the compute-0
          node.<codeblock>system host-if-modify &lt;host-name or ID> -n &lt;interface_name> -nt &lt;network_type> --ipv4-mode=static eth-x -m &lt;MTU Size in bytes> </codeblock>Where
            <codeph>eth-x</codeph> is the interface on which untagged TUL is presented.<note>For a
            VXLAN network, the frame is either 54 or 74 bytes long, depending on whether IPv4 or
            IPv6 protocol is used. This is because, in addition to the Ethernet header and CRC, the
            payload is enclosed by an IP header (20 bytes for IPv4 or 40 bytes for IPv6), a UDP
            header (8 bytes), and a VXLAN header (8 bytes). The additional IP, UDP, and VXLAN
            headers are invisible to the <codeph>vrs-data</codeph> interface, which expects a frame
            only 18 bytes larger than the MTU. To accommodate the larger frames on a VXLAN network,
            you must specify a larger nominal MTU on the <codeph>vrs-data</codeph> interface. For
            simplicity, and to avoid issues with stacked VLAN tagging, some third party vendors
            recommend rounding up by an additional 100 bytes for calculation purposes. For example,
            to attach to a VXLAN provider network with an MTU of 1500, a <codeph>vrs-data</codeph>
            interface with an MTU of 1600 is recommended.</note><p>For
            example:<codeblock>system host-if-modify compute-0 -n vrs -nt data-vrs --ipv4-mode=static eth4 -m 1600</codeblock></p></li>
        <li>Assign the VRS interface IP address to compute-0
            <codeblock>system host-addr-add compute-0 vrs &lt;ipv4/ipv6_address> &lt;prefix_length></codeblock><p>For
            example:<codeblock>system host-addr-add compute-0 vrs 10.70.5.83 24</codeblock></p></li>
        <li>Assign the VSC Controller IP address to compute-0.
            <codeblock>system host-update &lt;host-name or ID> vsc_controllers=&lt;ipv4/ipv6_address>,&lt;ipv4/ipv6_address> </codeblock><p>For
            example:
            <codeblock>system host-update compute-0 vsc_controllers=10.70.5.31,10.70.5.32</codeblock></p></li>
        <li>Unlock the compute-0 node.<codeblock>system host-unlock compute-0</codeblock></li>
        <li>Lock the compute-1 node.<codeblock>system host-lock compute-1</codeblock></li>
        <li>Repeat commands above to create interfaces on compute-1 to the infrastructure network
          and the data network. <p>For
          example:</p><codeblock>system host-if-add –V 722 –nt infra compute-1 infra vlan pxeboot0
system host-addr-add compute-1 infra 10.70.22.81 24
system host-if-modify compute-1 -n vrs -nt data-vrs --ipv4-mode=static eth4 -m 1600
system host-addr-add compute-1 vrs 10.70.5.83 24
system host-update compute-1 vsc_controllers=10.70.5.31,10.70.5.32</codeblock></li>
        <li>Unlock the compute-1 node.<codeblock>system host-unlock compute-1</codeblock></li>
      </ol><p id="nic-unlock">
        <b>For bonded NIC environments</b></p><p>If you are using a bonded NIC environment, after
        configuring and unlocking the compute nodes, perform these steps:</p><ol id="ol_nr5_ddw_mt">
        <li>Use the following commands to delete the MGMT interface: <ol id="ol_zfq_5h1_tt">
            <li>Obtain the compute name or ID number:<codeblock>system host-list</codeblock></li>
            <li>Obtain the UUID of the MGMT interface to
              remove:<codeblock>system host-if-list</codeblock></li>
            <li>Remove the PXE boot flag off the
              interface:<codeblock>system host-if-modify -nt "none" compute-0 pxeboot0</codeblock></li>
            <li>Delete the
              interface:<codeblock>system host-if-delete compute-0 95e7b495-1454-49d1-8123-301215503e86      </codeblock></li>
          </ol></li>
        <li>Execute the following commands to create a bonded interface<ol id="ol_xyc_fww_dv">
            <li>From the KVM region controller, lock the compute-0
              node.<codeblock>system host-lock compute-0</codeblock></li>
            <li>Create a bonded interface for the control plane:
              <codeblock>system host-if-add -a 802.3ad -x layer2 -nt pxeboot compute-0 bond0 ae none eth0 eth1</codeblock></li>
            <li>Create a bonded interface to the management
              network:<codeblock>system host-if-add -V &lt;vlan-id> -nt mgmt compute-0 mgmt vlan bond0</codeblock>Where:
                <ul id="ul_q4r_4jv_hv">
                <li><codeph>vlan-id</codeph> is the VLAN ID;</li>
                <li><codeph>mgmt</codeph> is the type of network;</li>
                <li><codeph>compute-0</codeph> is the name of the compute node where the interface
                  will be created;</li>
                <li><codeph>mgmt</codeph> is the name of the interface;</li>
                <li><codeph>vlan</codeph> is the type of interface;</li>
                <li><codeph>bond0</codeph> is the name of the bonded interface.</li>
              </ul></li>
            <li>Create a bonded interface to the infrastructure interface using the variables
              defined in the previous
              step:<codeblock>system host-if-add -V &lt;vlan-id> -nt infra compute-0 &lt;name> vlan bond0</codeblock></li>
            <li>Execute the following command to assign the infra IP address to
              compute-0:<codeblock>system host-addr-add compute-0 infra 172.16.74.244 24</codeblock>Where:
                <ul id="ul_qt5_yjv_hv">
                <li><codeph>infra</codeph> is the type of network;</li>
              </ul></li>
          </ol></li>
        <li>Use the following commands to configure MTU:
            <codeblock>system host-if-modify -m 1600 compute-0 eth2
system host-if-modify -m 1600 compute-0 eth3
system host-if-add -a balanced -x layer2 -nt "none" compute-0 bondvrs ae "none" eth2 eth3 -m 1600</codeblock><note>For
            a VXLAN network, the frame is either 54 or 74 bytes long, depending on whether IPv4 or
            IPv6 protocol is used. This is because, in addition to the Ethernet header and CRC, the
            payload is enclosed by an IP header (20 bytes for IPv4 or 40 bytes for IPv6), a UDP
            header (8 bytes), and a VXLAN header (8 bytes). The additional IP, UDP, and VXLAN
            headers are invisible to the <codeph>vrs-data</codeph> interface, which expects a frame
            only 18 bytes larger than the MTU. To accommodate the larger frames on a VXLAN network,
            you must specify a larger nominal MTU on the <codeph>vrs-data</codeph> interface. For
            simplicity, and to avoid issues with stacked VLAN tagging, some third party vendors
            recommend rounding up by an additional 100 bytes for calculation purposes. For example,
            to attach to a VXLAN provider network with an MTU of 1500, a <codeph>vrs-data</codeph>
            interface with an MTU of 1600 is recommended.</note></li>
        <li>Use one of the following steps to configure a static IP address for the bonded NIC
          interface or assign an IP address from an IP address pool:<ul id="ol_uqd_l5z_st">
            <li>To assign a static IP:
              <codeblock>system host-if-modify compute-0 -nt data-vrs --ipv4-mode=static bondvrs
system host-addr-add compute-0 bondvrs 10.30.6.244 24</codeblock></li>
            <li>To assign an IP address from a pool:<p>
                <codeblock>system addrpool-add vrspool 10.30.6.0 24 --order random --ranges 10.30.6.240-10.30.6.249
system host-if-modify compute-0 bondvrs -nt data-vrs --ipv4-mode=pool --ipv4-pool=vrspool
system host-if-modify compute-1 bondvrs -nt data-vrs --ipv4-mode=pool --ipv4-pool=vrspool</codeblock>
              </p></li>
          </ul></li>
        <li>Assign the VRS interface IP address to
          compute-0:<codeblock>system host-update compute-0 vsc_controllers=10.30.6.31,10.30.6.32</codeblock></li>
        <li>Unlock the compute-0 node.<codeblock>system host-unlock compute-0</codeblock></li>
        <li>Lock the compute-1 node.<codeblock>system host-lock compute-1</codeblock></li>
        <li>Repeat the steps above on
          compute-1:<codeblock>system host-if-add -a 802.3ad -x layer2 -nt pxeboot compute-1 bond0 ae none eth0 eth1
system host-if-add -V 532 -nt mgmt compute-1 mgmt vlan bond0
system host-if-add -V 539 -nt infra compute-1 infra vlan bond0
system host-addr-add compute-1 infra 172.16.74.244 24
system host-if-modify -m 1600 compute-0 eth2
system host-if-modify -m 1600 compute-0 eth3
system host-if-add -a balanced -x layer2 -nt "none" compute-0 bondvrs ae "none" eth2 eth3 -m 1600</codeblock></li>
        <li>Use one of the following steps to configure a static IP address for the bonded NIC
          interface or assign an IP address from an IP address pool:<ul id="ul_lnk_lt3_kv">
            <li>To assign a static IP:
              <codeblock>system host-if-modify compute-0 -nt data-vrs --ipv4-mode=static bondvrs
system host-addr-add compute-0 bondvrs 10.30.6.244 24</codeblock></li>
            <li>To assign an IP address from a pool:<p>
                <codeblock>system addrpool-add vrspool 10.30.6.0 24 --order random --ranges 10.30.6.240-10.30.6.249
system host-if-modify compute-0 bondvrs -nt data-vrs --ipv4-mode=pool --ipv4-pool=vrspool
system host-if-modify compute-1 bondvrs -nt data-vrs --ipv4-mode=pool --ipv4-pool=vrspool</codeblock>
              </p></li>
          </ul></li>
        <li>Assign the VRS interface IP address to
          compute-0:<codeblock>system host-update compute-0 vsc_controllers=10.30.6.31,10.30.6.32</codeblock></li>
        <li>Unlock the compute-1 node.<codeblock>system host-unlock compute-1</codeblock></li>
      </ol></section>
    <section id="next-step">
      <title>Next Steps</title>
      <p><xref href="../carrier-grade-install-post.dita#topic1107cgipd">Post-Installation
        Tasks</xref></p>
    </section>
  </body>
</topic>
