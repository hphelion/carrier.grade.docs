<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_z3v_rwn_lw">
  <title>Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 2.0: Recovery of MySQL and
    RabbitMQ</title>
  <body>
    <p><b>Recovery of MySQL</b></p>
    <p>A default deployment of HPE Helion OpenStack Carrier Grade includes a MySQL database instance
      on each controller, configured as a MySQL cluster. There are two ways to manually recover a
      failed MySQL cluster. The recovery options should be followed in the order they are
      specified.</p>
    <p><b>Manual Recovery of MySQL Cluster</b></p>
    <p><b>Option 1</b></p>
    <p>Execute the following steps on the lifecycle manager:</p>
    <ol id="ol_w2n_4vd_bv">
      <li>Execute the following command to stop MySQL on all controller
        nodes:<codeblock>/etc/init.d/mysql stop</codeblock></li>
      <li>Find the sequence number on all
          nodes:<codeblock>sudo cat /var/lib/mysql/grastate.dat

GALERA saved state
version: 2.1
uuid:    5bd38b4a-70e9-11e4-ad52-7647a787e29a
seqno:   1419441      ------>  sequence number
cert_index:</codeblock><p>The
          seqno may be specified as -1.</p><p>
          <codeblock>sudo cat /mnt/state/var/lib/mysql/grastate.dat

# GALERA saved state
version: 2.1
uuid: 5bd38b4a-70e9-11e4-ad52-7647a787e29a
seqno: -1
cert_index:</codeblock>
        </p><p>In this case run the following command to find the sequence number (mysql service
          must be stopped to run this):</p><p>
          <codeblock>sudo /usr/bin/mysqld_safe --wsrep-recover
141127 12:30:18 mysqld_safe Logging to '/mnt/state/var/log/mysql/error.log'.
141127 12:30:18 mysqld_safe Starting mysqld daemon with databases from /mnt/state/var/lib/mysql/
141127 12:30:18 mysqld_safe Skipping wsrep-recover for 5bd38b4a-70e9-11e4-ad52-7647a787e29a:
1420242 pair141127 12:30:18 mysqld_safe Assigning 5bd38b4a-70e9-11e4-ad52-7647a787e29a:1420242 to wsrep_start_position ---> 1420242 is the sequence number
141127 12:30:21 mysqld_safe mysqld from pid file /var/run/mysqld/mysqld.pid ended</codeblock>
        </p></li>
      <li>Start MySQL on the node with the highest sequence number (in case all the nodes except one
        results in -1 in sequence number, start the one that has
        number:<codeblock>sudo /etc/init.d/mysql bootstrap-pxc</codeblock></li>
      <li>Start mysql on the other controller
        nodes<codeblock>sudo /etc/init.d/mysql start</codeblock></li>
    </ol>
    <p><b>Option 2</b></p>
    <p>If Option 1 fails to bring back the cluster, execute the following steps:</p>
    <p>
      <ol id="ol_mbm_byd_bv">
        <li>Log into the failed controller. </li>
        <li>Execute the following command to purge
          MySQL:<codeblock>apt-get purge xinetd
apt-get purge python-mysqldb
backup /var/lib/mysql
apt-get purge percona-xtradb-cluster-server-5.5</codeblock></li>
        <li>On lifecycle manager, edit the
            <codeph>/var/hlm/clouds/&lt;cloudname>/desired_state/&lt;cloudname>/001/base/stage/ansible/hosts</codeph>
          file to comment the working controller. For
          example:<codeblock>[BASE-CCP-T1]
#BASE-CCP-T1-M1-NETCLM ansible_ssh_host=10.200.51.103
BASE-CCP-T1-M2-NETCLM ansible_ssh_host=10.200.51.104
BASE-CCP-T1-M3-NETCLM ansible_ssh_host=10.200.51.105

[BASE-CCP-T1]
#BASE-CCP-T1-M1-NETCLM ansible_ssh_host=10.200.51.103
BASE-CCP-T1-M2-NETCLM ansible_ssh_host=10.200.51.104
BASE-CCP-T1-M3-NETCLM ansible_ssh_host=10.200.51.105</codeblock></li>
        <li>Execute the following
          command:<codeblock>hlm deploy â€“c &lt;cloudname&gt; -t FND-MDB</codeblock></li>
        <li>Modify the <codeph>/etc/mysql/my.cnf</codeph> to make sure the <i>wsrep_cluster_address
          </i> variable contains all the nodes participating in the MySQL cluster. <p>If any of the
            node is found to be missing add it to the <i>wsrep_cluster_address </i> variable. For
            example:</p><p><i>wsrep_cluster_address =
              gcomm://BASE-CCP-T1-M1-NETCLM,BASE-CCP-T1-M2-NETCLM,BASE-CCP-T1-M3-NETCLM</i></p></li>
        <li>Execute the following command to restart MySQL on the failed nodes:
          <codeblock>service mysql restart</codeblock></li>
      </ol>
    </p>
    <p/>
    <p><b>Manual Recovery of RabbitMQ Cluster</b></p>
    <p>The following steps represent a general procedure for resetting an entire RabbitMQ cluster
      (that cannot be otherwise recovered). There may be a specific set of steps to follow for a
      particular RabbitMQ cluster problem. </p>
    <ol id="ol_ks5_tw5_1v">
      <li>Perform the following tasks on each RabbitMQ node to make sure each node is removed from
        the cluster. For HP Helion OpenStack Carrier Grade with a fully HA configuration will have
        three nodes.<ol id="ol_ls5_tw5_1v">
          <li>Execute the following command to stop the RabbitMQ Erlang
            application<codeblock>sudo rabbitmqctl stop_app</codeblock></li>
          <li>Execute the following command to sync all data in the cluster and remove the node from
            the cluster. This task cleans the local RabbitMQ Mnesia
            database.<codeblock>sudo rabbitmqctl reset</codeblock></li>
          <li>Execute the following command to kill all RabbitMQ
            processes<codeblock>sudo pkill rabbitmq</codeblock></li>
        </ol></li>
      <li>Select one of the controller nodes (for example: controller0) and start up rabbitmq
        service:<codeblock>sudo service rabbitmq-server start</codeblock></li>
      <li>On the other two controller nodes (for example: controller1 and controller2), start up the
        RabbitMQ service and form a cluster by joining with
          controller0:<codeblock>sudo service rabbitmq-server start
sudo rabbitmqctl stop_app
sudo rabbitmqctl join_cluster rabbit@[controller#_hostname]
sudo rabbitmqctl start_app</codeblock><p>Where
          controller#_hostname is the controller number and the name of the host system. </p></li>
    </ol>
  </body>
</topic>
