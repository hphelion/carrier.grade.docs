<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic10581cgikog">
<title>HPE Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 1.1: Installing the KVM + ESX
    Deployment</title>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
</metadata>
</prolog>
<body>
  <p>The first phase of the HPE Helion Openstack Carrier Grade installation involves creating a virtual machine for the Helion Lifecycle Management (HLM) and then deploying the HPE Helion Openstack cloud.</p>
    <p>The installation uses Ansible playbooks, which are files that contain which contains the
      steps that should be run.</p>

<section id="prepare"> <title>Prepare the system for deployment</title>
  <p>Use the following steps to prepare the server on which the HLM VM will be deployed (the lifecycle manager host):</p>
<ol>

  <li>Edit the <codeph>/root/infra-ansible-playbooks/group_vars/all</codeph> file for your
          environment. For information on each variable, refer to the comments in the file with each
          variable. Make sure the <codeph>hlm_kvm_host</codeph> value is configured properly.
    <p>The <codeph>group_vars/all</codeph> file should appear similar to the following for a KVM + ESX deployment:</p>
  <codeblock>
    ############################################# Variables for HLM  #################################################################
    #These variables are relevant in both All Virtual and BareMetal scenarios.
    
    #Set this to 1 if you will not deploy a dcn cloud via this HLM node and will deploy only ovs cloud
    ovs_cloud_only:  0
    
    #Set this to 'bm' if cloud is being deployed over baremetal.
    #Set this to 'av' if the cloud is all virtual
    cloud_type: 'bm'
    
    #These are hlm related variables that must be changed according to your Baremetal Env
    hlm_login_id:       root
    hlm_password:       cghelion
    
    #The following variables are for CLM network IP details for HLM
    hlm_clmstaticip:    10.20.3.100
    hlm_clmnetmask:     255.255.255.0
    hlm_clmgateway:     10.20.3.1
    
    #The variables starting with cobbler_ are inputs that are usually given to initcobbler.sh. Set accordingly.
    cobbler_pxestartip: 10.20.1.100
    cobbler_pxeendip:   10.20.1.200
    cobbler_pxestaticip: 10.20.1.51
    cobbler_pxenetmask: 255.255.255.0
    
    #Set the location of your images that will be used by libvirt
    imagelocation: /var/lib/libvirt/images
    
    #Set the location of your infra-ansible-playbooks
    ansible_dir: ~/infra-ansible-playbooks
    
    ##################################################################################################################################
    
    ############################################# Variables for DCN ##################################################################
    #Set the location of dcn bits on KVM
    #There must be 4 debs, 2 tar.gz files, 1 vsc qcow2
    #You must copy your VSD qcow2 to the imagelocation on the KVM if you want to provision it on the same KVM as the HLM
    dcn_dir: ~/cg/dcn
    ##################################################################################################################################
    
    ############################################# Variables for VSD ##################################################################
    #Ignore these variables if creating an OVS cloud. This section is relevant only for DCN cloud - in both BM and All Virtual cases
    #These variables are used for VSD Configuration
    #If you have already configured a VSD and ignore the following variables
    
    dns_domain_name: dcn-one.helion.cg
    dns_address: 10.1.2.44
    vsd_address: 10.20.5.21
    vsd_gateway: 10.20.5.1
    vsd_netmask: 255.255.255.0
    vsd_name: vsd1
    vsdimagename: VSD-3.0.0_HP_r3.0_36
    upstream_ntp_servers:
    - 10.1.2.44
    - 16.110.135.123
    ###################################################################################################################################
  </codeblock></li>
  <li id="hosts">Check the hosts file <codeph>/root/infra-ansible-playbooks/hosts</codeph> file to
    enter the IP address of the vibr0 interface in the <codeph>hlm_kvm_host</codeph> field, as shown in the following example. Make sure the DCM network
          details are correct. Also, verify the CLM IP address in this file.</li>
</ol>
<codeblock>  [hlm_kvm_host]
  192.168.122.1 </codeblock>
</section>
<section id="deploy-the-hlm-and-vsd-vm">
      <title>Deploy the HLM Virtual Machine</title>
  <p>Use the following steps to deploy the lifecycle manager VM on the lifecycle manager Host using Ansible playbooks.</p>
      <ol>
        <li>Make sure the Ansible playbook file is not in executable mode.</li>
        <li>Execute the following
          command:<codeblock>ansible-playbook -i hosts setup_hlm_onBM.yml</codeblock> The command
          will do the following:<ul>
            <li>Copy both installation files (.tar) to the lifecycle manager host, decrypt, and extract the
              files.</li>
            <li>Execute the <codeph>updatepackages</codeph> command.</li>
            <li>Execute the <codeph>prepareenv</codeph> command.</li>
            <li>Execute the <codeph>Init cobbler</codeph> command.</li>
            <li>Execute the <codeph>Importiso</codeph> command.<p>You will see similar message when
                the playbook is run successful:</p></li>
          </ul>
          <image href="../../media/CGH-ansible-playbook-run.png" id="image_bfp_pjm_vs">
            <alt>Sample Ansible Playbook run output</alt>
          </image></li>
        <li>Locate the IP address for the HLM VM in the
          <codeph>/root/infra-ansible-playbooks/group_vars/all</codeph> file of the lifecycle manager host under
          <codeph>hlm_clmstaticip</codeph> field.</li> 
      </ol>
      <p>For KVM + ESX deployments, proceed with <xref href="#topic10581cgikog/vsd" format="dita"/>.</p>    
<p>For KVM deployments, skip the following sections and proceed to <xref
          href="#topic10581cgikog/configure-a-json-file-for-installation" format="dita"/>.</p>

</section>
  
<section id="vsd"> <title>Configuring the VSD node, creating user and applying License</title>
  <p>During the <xref href="#topic10581kgikog/deploy-the-hlm-and-vsd-vm" format="dita"/> process, the
        Ansible playbook run creates a virtual machine is created to host the VSD node of DCN.</p>
<p>You need to log into the VSD node (using SSH), then create a default user and apply the required license.</p>
  
<p>After logging into the VDS node, execute the following command to launch a console window:</p>
<codeblock>virsh console vsd</codeblock>
<p>You should see the status as below from VSD VM.</p>
<p>
  <image href="../../media/CGH-install-virsh-vsd.png"/>
</p>
</section>

  <!--<section id="vsd-performance-workaround"> <title>VSD Performance workaround</title>
<p>By default VSD has only 8G memory. For better performance behavior, you can update the VSD memory to 16G.</p>
<ol>
<li>From lifecycle manager host, execute the following command to power down the VSD node:
          VM:<codeblock>virsh destroy vsd</codeblock></li>
<li>Execute the following command to edit the memory setting in the VSD XML
          file:<codeph>virsh edit vsd 

Current value 
    &lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;8388608&lt;/currentMemory&gt;

Change to 
    &lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;16777216&lt;/currentMemory&gt;
</codeph></li>
<li>Save the value<codeph>virsh save vsd
</codeph></li>
<li>Use the following command to start the
            VM:<codeph>virsh start vsd
</codeph><p>It can take 15 minutes or
            more to sync with NTP and to get all the other VSD services up.</p></li>
</ol>
</section>-->
<section id="launch-vsd-dashboard"> <title>Launch VSD Dashboard</title>
  <p>On the lifecycle manager host:</p>
<ol>
<li>Using an browser such as Chrome or FireFox, navigate to the DCM IP of VSD.</li>
<li>In the log in page, enter the default
          credentials:<codeblock>User Name: Csproot 
Password: csproot 
Org: csp 
VSD Server : auto 
</codeblock></li>
</ol>
      <p/>
      <p>Follow the instructions in the pdf to get the license - <b>Get confirmation from
        Nayana</b></p>
      <p><xref
          href="https://wiki.hpcloud.net/display/HCG/HP+Helion+OpenStack+Carrier+Grade+%28NFVi%29+Home#HPHelionOpenStackCarrierGrade%28NFVi%29Home-HPHelionOpenStackCarrierGradeDCN/Nuage"
          format="html" scope="external"
          >https://wiki.hpcloud.net/display/HCG/HP+Helion+OpenStack+Carrier+Grade+%28NFVi%29+Home#HPHelionOpenStackCarrierGrade%28NFVi%29Home-HPHelionOpenStackCarrierGradeDCN/Nuage</xref></p>
</section>
<section id="applying-the-license"> <title>Applying the License</title>
<p>To install the required DCN license:</p>
<ol>
<li>From VSD Dashboard, click the <b>Open VSP Configuration</b> tab on the top right corner of the
          dashboard.</li>
<li>Click the <b>Licenses</b> tab and click <b>+</b>.</li>
<li>Copy and paste the license that you have receive into the screen that displays.</li>
</ol>
</section>
<section id="create-user-for-plugin-login"> <title>Create User for Plugin Login</title>
<p>You must create a user and add it to CMS Group.</p>
<ol>
<li>From VSD Dashboard, click the <b>Open VSP Configuration</b> tab on the top right corner of the
          dashboard.</li>
<li>Click the <b>CSP Users</b> tab and click <b>+</b>.</li>
<li>Create a user named <codeph>OSadmin</codeph> with the password <codeph>OSadmin</codeph>.</li>
<li>Add the user to the <codeph>CMS Group</codeph>.</li>
</ol>
</section>
  <section><title>Install the DVswitch</title></section>
  
  <section><title>Install the VRSVapp</title></section>
  <section id="configure-a-json-file-for-installation"> <title>Configure a JSON file for installation</title>
<p>The HPE Helion OpenStack deployment requires a JSON file. Use the following steps to install and
        edit the file.</p>
<ol>
<li>Login to HLM VM.
            <codeblock>ssh &lt;HLM_VM_IP>
where: HLM_VM_IP is the IP of the HLM VM. </codeblock><p>Use
            the default credentials:
          <codeblock>User Name: root
Password: cghelion</codeblock></p></li>
    
<li>On the HLM VM, change to the home directory.<codeblock>cd ~</codeblock></li>
<li>Provision and configure your HPE Helion OpenStack
            VM.<codeblock>hnewcloud  &lt;cloudname&gt; &lt;denver&gt;</codeblock><p>Where:</p><ul>
            <li><codeph>&lt;cloudname&gt;</codeph> is the name of the cloud to create</li>
              <li><codeph>&lt;denver&gt;</codeph> is the name of the template to use. The
              installation kit includes the fully-tested <codeph>denver</codeph> template.</li>
          </ul><p>The command creates the <codeph>&lt;cloudname&gt;</codeph> directory, which will
            contain a JSON template file <codeph>node-provision.json</codeph>. This template
            supplies input values to the <codeph>hprovision</codeph> script, later in the
            installation.</p></li>
<li>Edit <codeph>node-provision.json</codeph> file based on following guidelines:<table>
            <tgroup cols="2">
              <colspec colname="col1" colsep="1" rowsep="1"/>
              <colspec colname="col2" colsep="1" rowsep="1"/>
              <thead>
                <row>
                  <entry colsep="1" rowsep="1">Field</entry>
                  <entry colsep="1" rowsep="1">Baremetal</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>name</entry>
                  <entry>Name of the system you want to add</entry>
                </row>
                <row>
                  <entry>Pxe-mac-address</entry>
                  <entry>MAC address of the interface you want to PXE boot onto. This is not same as
                    iLO MAC address,</entry>
                </row>
                <row>
                  <entry>Pxe-interface</entry>
                  <entry>The name of the interface on which PXE boot should occur. For example: <codeph>eth0</codeph></entry>
                </row>
                <row>
                  <entry>pm_type</entry>
                  <entry>ipmilan </entry>
                </row>
                <row>
                  <entry>pm_ip</entry>
                  <entry>Power management IP (iLO ip)</entry>
                </row>
                <row>
                  <entry>pm_user</entry>
                  <entry>Power management user (iLO username)</entry>
                </row>
                <row>
                  <entry>pm_pass</entry>
                  <entry>Power management password (iLO password)</entry>
                </row>
                <row>
                  <entry>node_group</entry>
                  <entry>Enter the same value as <codeph>node-type</codeph> in the <codeph>nodes.json</codeph>
                    file used during cloud deployment. For example: `CCN-001-001`.</entry>
                </row>
                <row>
                  <entry>failure_zone, vendor, model, os_partition_size, data_partition_size</entry>
                  <entry>Enter the same value as for these fields an in the <codeph>nodes.json</codeph> file used
                    during cloud deployment</entry>
                </row>
              </tbody>
            </tgroup>
          </table><p>To see a sample <codeph>node-provision.json</codeph> file, see <xref
              href="../../CarrierGrade/Installation/carrier-grade-install-pb-hlm-vm-json.dita">Create the HLM Virtual Machine</xref>.</p></li>
</ol>
</section>
<section id="pxe-boot"> <title>Configure PXE boot</title>
<p>After you edit the <codeph>node-provision.json</codeph> file, you must enable one-time PXE boot
        on the servers set the correct boot order. Execute the following on the HLM VM:</p>
<ol>
<li>Use the following command to install the <codeph>python-hpilo</codeph> module on HLM
  VM:<codeblock>pip install python-hpilo</codeblock><p>
            <codeph>python-hpilo</codeph> is a python library and command-line tool for
          iLO.</p></li>
<li>Copy the <codeph>ilopxebootonce.py</codeph> from the
            <systemoutput>/root/cg-hlm/dev-tools/ilopxebootonce.py</systemoutput> to the directory
          where you have the <codeph>node-provision.json</codeph> file.</li>
<li>Execute the following script:
  <codeblock>python ilopxebootonce.py node-provision.json</codeblock></li>
</ol>
<p>After the script is run, the <codeph>Current One-Time Boot Option</codeph> is set to <codeph>Network Device 1</codeph> on all the servers listed in <codeph>node-provision.json</codeph> file.</p>
</section>
<section id="create-a-new-cloud-template-and-bring-the-cloud-nodes-up"> <title>Create new cloud template and bring the cloud nodes up</title>
<ol>
<li>Use the following script to start the provisioning of the HPE Helion OpenStack
            cloud:<p>hprovision <codeph>&lt;cloudname&gt;</codeph>
          </p><p>Where:</p><ul>
            <li><codeph>&lt;cloudname&gt;</codeph> is the name of the cloud to create</li>
          </ul><p>This script will PXE boot the nodes specified in
              <codeph>node-provision.json</codeph> file. The script also tracks the PXE boot
            completion process and will create the <codeph>nodes.json</codeph> file in the
            directory. </p></li>
        <li>Make sure the nodes are booted up.</li>
  <li>Update the <codeph>node-provision.json</codeph> file used in the <xref type="section"
            href="#topic10581cgikog/pxe-boot">previous step</xref>.<p>a. Change to the
              <codeph>&lt;cloudname&gt;</codeph>
            directory:</p><codeblock>cd ~/&lt;cloudname&gt;</codeblock><p>b. Once the baremetal
            nodes are provisioned, make sure the <codeph>nodes.json</codeph>  file is generated and
            that you can establish a password-less SSH connection to these nodes from HLM
          VM.</p></li>
<li>Modify the <codeph>environment.json</codeph> file to configure the VLANs and network addresses
          as appropriate for your environment. Set the following for the CLM, CAN, and BLS
          network:<codeblock>"cidr": 
"start-address": </codeblock> The three controller nodes
          should have CLM, CAN, EXT, BLS on eth0 and TUL on eth1. <!--Hiding for RC0 
              <p>The two compute nodes should have CLM, EXT, BLS on eth0 and TUL on eth1.</p>
           -->
  <p> 
            <b>Example:</b>
          </p>

  <pre>{</pre><pre>    "product": {</pre><pre>        "version": 1</pre><pre>    },</pre><pre> </pre><pre>    "node-type": [</pre><pre>        {</pre><pre>            "name": "CCN",</pre><pre>            "interface-map": [</pre><pre>                {</pre><pre>                    "name": "INTF0",</pre><pre>                    "ethernet-port-map": {</pre><pre>                        "interface-ports": [ "eth0" ]</pre><pre>                    },</pre><pre>                    "logical-network-map": [</pre><pre>                        {</pre><pre>                            "name": "CLM",</pre><pre>                            "type": "vlan",</pre><pre>                            "segment-id": "502",</pre><pre>                            "network-address": {</pre><pre>                                "cidr": "10.50.2.0/24",</pre><pre>                                "start-address": "10.50.2.10",</pre><pre>                                "gateway": "10.50.2.1"</pre><pre>                            }</pre><pre>                        },</pre><pre>                        {</pre><pre>                            "name": "CAN",</pre><pre>                            "type": "vlan",</pre><pre>                            "segment-id": "504",</pre><pre>                            "network-address": {</pre><pre>                                "cidr": "10.50.4.0/24",</pre><pre>                                "start-address": "10.50.4.10"</pre><pre>                            }</pre><pre>                        },</pre><pre>                        {</pre><pre>                            "name": "BLS",</pre><pre>                            "type": "vlan",</pre><pre>                            "segment-id": "76",</pre><pre>                            "network-address": {</pre><pre>                                "cidr": "172.16.76.0/19",</pre><pre>                                "start-address": "172.16.76.10"</pre><pre>                            }</pre><pre>                        }</pre><pre>                    ]</pre><pre>                }</pre><pre>            ]</pre><pre>        }</pre><pre>    ]</pre><pre>}</pre><p>

            <b>NOTE:</b> The configuration processor assigns the first address of the CLM
            address range to itself for serving python and debian repositories. Make sure that you
            set the first IP address of the CLM range for the eth2 (CLM) address of the HLM
            node.</p></li>
<li>Modify the <codeph>definition.json</codeph> file: <ol>
            <li>Set the number of compute systems to 2.
              <codeblock>"count": 2, //number of computes in the resource pool</codeblock></li>
  <li>Update the <codeph>ansible-vars</codeph> section with all the information based on your
              setup.</li>
  <li>Make sure you have two NTP entries in the <codeph>upstream_ntp_servers</codeph> fields in the
                <codeph>definition.json</codeph> file as seen in the following example. If you have
              only one NTP server in your environment, specify the same NTP server twice. Example:
              <codeblock>{
    "product": {
      "version": 1
      },
    
    "cloud": {
      "name": "b44tb4",
      "nickname": "b44tb4",
      "server-config": "nodes.json",
      "environment": "environment.json",
      "network-config": ".hos/lnet-control-data.json"
    },
    
    "failure-zones": [
      {
        "name": "fz1"
      },
      {
        "name": "fz2"
      },
      {
        "name": "fz3"
      }
    ],
    
    "control-planes": [
      {
        "file": ".hos/ccp-1x3-ss.json",
        "resource-nodes": []
      }
    ],
    
    "ansible-vars": {
      "dns_address": "10.1.2.44",
      "dns_domain_name": "helion.cg",
      "ldap_url": "10.1.2.44",
      "ldap_username": "admin",
      "ldap_password": "admin",
      "ldap_domain": "dc=helioncg,dc=local",
      "ldap_ou": "CGTestUsers",
      "ldap_nova_password": "nova",
      "ldap_nova_user": "nova",
      "ldap_neutron_password": "neutron",
      "ldap_neutron_user": "neutron",
      "ldap_cinder_password": "cinder",
      "ldap_cinder_user": "cinder",
      "ldap_glance_password": "glance",
      "ldap_glance_user": "glance",
      "ldap_enabled": 1,
      "upstream_ntp_servers": [
        "10.1.2.44",
        "16.110.135.123",
        "2.debian.pool.ntp.org"
      ],
      "ssl_cert_file": "ca.crt",
      "ssl_key_file": "cakey.pem",
      "ssl_passphrase": "cghelion"
     },
    
    "wr-vars": {
      "database_storage": 50,
      "backup_storage": 300,
      "image_storage": 250,
      "region_name": "RegionOne",
      "logical_interface": [
        {
          "lag_interface": "N",
          "interface_mtu": 1500,
          "interface_ports": [
              "eth0"
          ],
           "network": [
              {
                  "ip_start_address": "10.50.2.51",
                  "ip_end_address": "10.50.2.99",
                  "name": "CLM"
              },
              {
                  "ip_start_address": "172.16.76.150",
                  "ip_end_address": "172.16.76.199",
                  "name": "BLS"
               },
               {
                  "ip_start_address": "10.50.4.51",
                  "ip_end_address": "10.50.4.99",
                  "gateway": "10.50.4.1",
                  "name": "CAN"
                }
            ]
        }
    ],
    "pxeboot_cidr": "10.50.1.0/24",
    "license_file_name": "license.lic"
    }
 }
</codeblock></li>
          </ol></li>
  
  <li>Use the following steps to modify to the <codeph>cinder/blocks</codeph> directory for your cloud: 
  <ul>
    <li>Change to the <codeph>cinder/blocks</codeph> directory:
      <codeblock>cd ~/&lt;cloudname>/services/cinder/blocks</codeblock>
      Where &lt;cloudname> is the name you assigned to the cloud.</li>
    <li>Copy the <codeph>cinder_conf_default.hp3parSample</codeph> file to the
                <codeph>cinder_conf_default</codeph> file and edit the file to configure to 3PAR
              settings. For example:
              <codeblock>hp3par_api_url=https://&lt;hp3par_ip>:8080/api/v1
hp3par_username=&lt;hp3par_user>
hp3par_password=&lt;hp3par_user_password
hp3par_cpg=bronze
san_ip=&lt;san_ip>
san_login=&lt;san_user>
san_password=&lt;san_password>
hp3par_iscsi_ips=&lt;iscsi_ip1>,&lt;iscsi_ip2>,&lt;iscsi_ip3>,&lt;iscsi_ip4>
volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
hp3par_debug=False
hp3par_iscsi_chap_enabled=false</codeblock></li></ul></li>            
<li>Once you have correctly edited all the json Cloud Model files, run the HPE Helion OpenStack
          configuration processor: <codeblock>hcfgproc -d definition.json</codeblock><p>The
              <codeph>hcfgproc</codeph> script gets installed in <codeph>/usr/local/bin</codeph> by
            the <codeph>prepare-env</codeph> script. The script generates a <codeph>clouds/</codeph>
            directory within the directory. </p></li>
<li>Review the CloudDiagram, <codeph>hosts.hf</codeph>, and
            <codeph>net/interfaces.d/eth.cfg</codeph> files to make sure the network settings are
          correct.</li>
<li>Initialize network interfaces on all the cloud nodes using the following command:
            <codeblock><codeph>hnetinit &lt;cloudname&gt;</codeph></codeblock><p>You can run this
            command from any directory.</p><p>After this command completes, all cloud nodes and CLM
            network interfaces should be set correctly.</p></li>

<li>Use the following command to deploy the cloud:
            <codeblock><codeph>hdeploy &lt;cloudname&gt;</codeph></codeblock><p>Once cloud
            deployment is successfully complete, there will be 3 controller nodes in the non-KVM
            region.</p></li>

</ol>



</section>
<section id="next-step"> <title>Next Step</title>
<p>
        <xref href="carrier-grade-install-kvm-cloud-GA.dita">Deploying the KVM Region</xref>
      </p>
<p>
  <xref type="section" href="#topic10581cgikog"> Return to Top </xref>
</p>
<!-- ===================== horizontal rule ===================== -->
</section>
</body>
</topic>
