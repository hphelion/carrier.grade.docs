<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic10581cgik">
  <title>HPE Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 1.1: Installing the KVM
    Deployment</title>
  <prolog>
    <metadata>
      <othermeta name="layout" content="default"/>
      <othermeta name="product-version" content="HPE Helion Openstack Carreir Grade 1.1"/>
      <othermeta name="role" content="Storage Administrator"/>
      <othermeta name="role" content="Storage Architect"/>
      <othermeta name="role" content="Michael B"/>
      <othermeta name="product-version1" content="HPE Helion Openstack Carreir Grade 1.1"/>
    </metadata>
  </prolog>
  <body>
    <p>
      <!--UNDER REVISION-->
      <!--./CarrierGrade/Installation/carrier-grade-install-pb-hlm-vm.md-->
      <!--permalink: /helion/openstack/carrier/install/pb/hlm-vm/--></p>
    <p>After the Helion Lifecycle Management (HLM) is installed, the next task in installing the
        <xref href="carrier-grade-install-pb-overview.dita#topic1925/install-option">KVM
        deployment</xref> is to  deploy the HPE Helion Openstack cloud.</p>
    
    <section>
      <title>Log into the HLM VM</title>
      <p>Log into the HLM VM you created in the previous page.</p>
      <ol>
        <li>Login to HLM VM.
            <codeblock>ssh &lt;HLM_VM_IP>
where: HLM_VM_IP is the CLM IP of the HLM VM. </codeblock><p>Use
            the <codeph>cghelion</codeph> credentials:
            <codeblock>User Name: cghelion
Password: cghelion</codeblock></p><p><b>Important:</b>
            After logging in with the default password, make sure you change the password for the
              <codeph>cghelion</codeph> user. </p></li>
        <li>Execute the following command to switch to the root
          user:<codeblock>sudo su -</codeblock></li>
        <li>On the HLM VM, change to the home directory.<codeblock>cd ~</codeblock></li>
      </ol></section>
    
    <section id="configure-a-json-file-for-installation">
          <title>Provision the new cloud</title>
          <ol>
        <li>Provision and configure your HPE Helion OpenStack
            VM.<codeblock>hnewcloud  &lt;cloudname&gt; denver</codeblock><p>Where:</p><ul>
            <li><codeph>&lt;cloudname&gt;</codeph> is the name of the cloud to create: <p>The cloud
                name can be any combination of up to 63 digits, letters, and the <codeph>-</codeph>
                  chracter:<ul id="ul_lz5_rbf_bt">
                  <li>
                    <p>Cannot start or end with the <codeph>-</codeph> chracter;</p>
                  </li>
                  <li>
                    <p>Must be greater than 0 characters;</p>
                  </li>
                  <li>
                    <p>No restriction on the number of letters or digits (0 or more).</p>
                  </li>
                </ul></p></li>
            <li><codeph>denver</codeph> is the name of the template to use. The installation kit
              includes the <codeph>denver</codeph> template.</li>
          </ul><p>The command creates the <codeph>&lt;cloudname&gt;</codeph> directory, which will
            contain a JSON template file <codeph>node-provision.json</codeph>. This template
            supplies input values to the <codeph>hprovision</codeph> script, later in the
            installation.</p></li>
        <li>The HPE Helion OpenStack deployment requires a JSON file. Edit
            <codeph>node-provision.json</codeph> file to change only the following fields:<table
            id="table_ycw_qrn_xs">
            <tgroup cols="2">
              <colspec colname="col1" colsep="1" rowsep="1"/>
              <colspec colname="col2" colsep="1" rowsep="1"/>
              <thead>
                <row>
                  <entry colsep="1" rowsep="1">Field</entry>
                  <entry colsep="1" rowsep="1">Description</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>Pxe-mac-address</entry>
                  <entry>MAC address of the interface you want to PXE boot onto. This is not same as
                    iLO MAC address.</entry>
                </row>
                <row>
                  <entry>pm_ip</entry>
                  <entry>Power management IP (iLO ip)</entry>
                </row>
                <row>
                  <entry>pm_user</entry>
                  <entry>Power management user (iLO username)</entry>
                </row>
                <row>
                  <entry>pm_pass</entry>
                  <entry>Power management password (iLO password)</entry>
                </row>
                <row>
                  <entry>failure_zone, vendor, model, os_partition_size, data_partition_size</entry>
                  <entry>Enter the same value as for these fields an in the
                      <codeph>nodes.json</codeph> file used during cloud deployment</entry>
                </row>
              </tbody>
            </tgroup>
          </table><p>To see a sample <codeph>node-provision.json</codeph> file, see<xref
            href="carrier-grade-install-pb-kvm-only-json.dita">Sample JSON File for the
              HLM Virtual Machine Installation</xref>. </p></li>
        <li>Modify the <codeph>environment.json</codeph> file to configure the VLANs and network
          addresses as appropriate for your environment. Set the following for the CLM, CAN, and BLS
          network:<codeblock>"cidr": 
"start-address": </codeblock> The three controller nodes
          should have CLM, CAN, EXT, BLS on eth0 and TUL on eth1. <!--Hiding for RC0 
              <p>The two compute nodes should have CLM, EXT, BLS on eth0 and TUL on eth1.</p>
           -->
          <p>
            <b>Example:</b>
          </p><p><codeblock>{
    "product": {
        "version": 1
    },
 
    "node-type": [
        {
            "name": "CCN",
            "interface-map": [
                {
                    "name": "INTF0",
                    "ethernet-port-map": {
                        "interface-ports": [ "eth0" ]
                    },
                    "logical-network-map": [
                        {
                            "name": "CLM",
                            "type": "vlan",
                            "segment-id": "502",
                            "network-address": {
                                "cidr": "10.50.2.0/24",
                                "start-address": "10.50.2.10",
                                "gateway": "10.50.2.1"
                            }
                        },
                        {
                            "name": "CAN",
                            "type": "vlan",
                            "segment-id": "504",
                            "network-address": {
                                "cidr": "10.50.4.0/24",
                                "start-address": "10.50.4.10"
                            }
                        },
                        {
                            "name": "BLS",
                            "type": "vlan",
                            "segment-id": "76",
                            "network-address": {
                                "cidr": "172.16.76.0/19",
                                "start-address": "172.16.76.10"
                            }
                        }
                    ]
                }
            ]
        }
    ]
}</codeblock>
            <b>NOTE:</b> The configuration processor assigns the first address of the CLM
            address range to itself for serving python and debian repositories. Make sure that you
            set the first IP address of the CLM range for the eth2 (CLM) address of the HLM
            node.</p></li>
        <li>Modify the <codeph>definition.json</codeph> file: <ol id="ol_t3x_btn_xs">
            <li>Set the number of compute systems to 2.
              <codeblock>"count": 2, //number of computes in the resource pool</codeblock></li>
            <li>Update the <codeph>ansible-vars</codeph> section with all the information based on
              your setup.</li>
            <li>Make sure you have two NTP entries in the <codeph>upstream_ntp_servers</codeph>
              fields in the <codeph>definition.json</codeph> file as seen in the following example.
              If you have only one NTP server in your environment, specify the same NTP server
              twice. Example:
              <codeblock>{
    "product": {
      "version": 1
      },
    
    "cloud": {
      "name": "b44tb4",
      "nickname": "b44tb4",
      "server-config": "nodes.json",
      "environment": "environment.json",
      "network-config": ".hos/lnet-control-data.json"
    },
    
    "failure-zones": [
      {
        "name": "fz1"
      },
      {
        "name": "fz2"
      },
      {
        "name": "fz3"
      }
    ],
    
    "control-planes": [
      {
        "file": ".hos/ccp-1x3-ss.json",
        "resource-nodes": []
      }
    ],
    
    "ansible-vars": {
      "dns_address": "10.1.2.44",
      "dns_domain_name": "helion.cg",
      "ldap_url": "10.1.2.44",
      "ldap_username": "admin",
      "ldap_password": "admin",
      "ldap_domain": "dc=helioncg,dc=local",
      "ldap_ou": "CGTestUsers",
      "ldap_nova_password": "nova",
      "ldap_nova_user": "nova",
      "ldap_neutron_password": "neutron",
      "ldap_neutron_user": "neutron",
      "ldap_cinder_password": "cinder",
      "ldap_cinder_user": "cinder",
      "ldap_glance_password": "glance",
      "ldap_glance_user": "glance",
      "ldap_enabled": 1,
      "upstream_ntp_servers": [
        "10.1.2.44",
        "16.110.135.123",
        "2.debian.pool.ntp.org"
      ],
      "ssl_cert_file": "ca.crt",
      "ssl_key_file": "cakey.pem",
      "ssl_passphrase": "cghelion"
     },
    
    "wr-vars": {
      "database_storage": 50,
      "backup_storage": 300,
      "image_storage": 250,
      "region_name": "RegionOne",
      "logical_interface": [
        {
          "lag_interface": "N",
          "interface_mtu": 1500,
          "interface_ports": [
              "eth0"
          ],
           "network": [
              {
                  "ip_start_address": "10.50.2.51",
                  "ip_end_address": "10.50.2.99",
                  "name": "CLM"
              },
              {
                  "ip_start_address": "172.16.76.150",
                  "ip_end_address": "172.16.76.199",
                  "name": "BLS"
               },
               {
                  "ip_start_address": "10.50.4.51",
                  "ip_end_address": "10.50.4.99",
                  "gateway": "10.50.4.1",
                  "name": "CAN"
                }
            ]
        }
    ],
    "pxeboot_cidr": "10.50.1.0/24",
    "license_file_name": "license.lic"
    }
 }
</codeblock></li>
          </ol></li>
      </ol>
    </section>
    <section id="pxe-boot">
      <title>Configure PXE boot</title>
      <p>After you edit the <codeph>node-provision.json</codeph> file, you must enable one-time PXE
        boot on the servers set the correct boot order. Execute the following on the HLM VM:</p>
      <ol>
        <li>Copy the <codeph>ilopxebootonce.py</codeph> from the
            <systemoutput>/root/cg-hlm/dev-tools/ilopxebootonce.py</systemoutput> to the directory
          where you have the <codeph>node-provision.json</codeph> file.</li>
        <li>Execute the following script:
          <codeblock>python ilopxebootonce.py node-provision.json</codeblock></li>
      </ol>
      <p>After the script is run, the <codeph>Current One-Time Boot Option</codeph> is set to
          <codeph>Network Device 1</codeph> on all the servers listed in
          <codeph>node-provision.json</codeph> file.</p>
    </section>
    <section id="create-a-new-cloud-template-and-bring-the-cloud-nodes-up">
      <title>Provision the cloud nodes</title>
      <ol>
        <li>Use the following script to start the provisioning of the HPE Helion OpenStack cloud:
            <codeblock>hprovision &lt;cloudname&gt; </codeblock><p>Where:</p><ul>
            <li><codeph>&lt;cloudname&gt;</codeph> is the name of the cloud to create</li>
          </ul><p>This script will PXE boot the nodes specified in
              <codeph>node-provision.json</codeph> file. The script also tracks the PXE boot
            completion process and will create the <codeph>nodes.json</codeph> file in the
            directory. </p></li>
        <li>Make sure the nodes are booted up.<p>a. Change to the <codeph>&lt;cloudname&gt;</codeph>
            directory:</p><codeblock>cd ~/&lt;cloudname&gt;</codeblock><p>b. Once the baremetal
            nodes are provisioned, make sure the <codeph>nodes.json</codeph> file is generated and
            that you can establish a password-less SSH connection to these nodes from HLM
          VM.</p></li>

        <li>Configure the back-end drivers to enable management of the OpenStack Block Storage
          volumes in the KVM region equipped with 3PAR and/or VSA storage arrays. The HPE Block
          Storage (Cinder) service allows you to configure multiple storage back-ends. Use the
          following steps to configure back-end support:<ol>
            <li>Change to the <codeph>cinder/blocks</codeph> directory:
              <codeblock>cd ~/&lt;cloudname>/services/cinder/blocks</codeblock> Where &lt;cloudname>
              is the name you assigned to the cloud. The directory contains several sample Cinder
              configuration files that you can edit, depending upon which storage method(s) you are
              using.</li>
            <li>Depending upon they type of storage you are using, edit one of the following files:
                <ul id="ul_xcf_v24_1t">
                <li>3PAR - if you plan to have only HPE StoreServ (3PAR) storage attached to the KVM
                  region, edit the <codeph>cinder_conf.hp3parSample</codeph> file.</li>
                <li>VSA - if you plan to have only HPE StoreVirtual VSA storage attached to the KVM
                  region, edit the <codeph>cinder_conf.vsasample</codeph> file.</li>
                <li>3PAR and VSA - if you plan to have HPE StoreServ (3PAR) and HPE StoreVirtual VSA
                  storage attached to the KVM region, edit the
                    <codeph>cinder_conf.multiBackendSample</codeph> file, which contains variables
                  for both storage types. </li>
              </ul><p>Use the <codeph>enabled_backends</codeph> variable to list each of the
                back-ends you are using. You must specify at least one back-end for the KVM
                region.</p><table>
                <tgroup cols="3">
                  <colspec colname="col1" colsep="1" rowsep="1"/>
                  <colspec colname="col2" colsep="1" rowsep="1"/>
                  <colspec colname="col3" colsep="1" rowsep="1"/>
                  <thead>
                    <row>
                      <entry colsep="1" rowsep="1">Region/Hypervisor</entry>
                      <entry colsep="1" rowsep="1">Volume Backend</entry>
                      <entry colsep="1" rowsep="1">Backend Name</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>KVM</entry>
                      <entry>3PAR </entry>
                      <entry>hp3par</entry>
                    </row>
                    <row>
                      <entry>KVM</entry>
                      <entry>VSA</entry>
                      <entry>hplefthand</entry>
                    </row>
                  </tbody>
                </tgroup>
              </table><p><b>Note:</b> You can use either 3PAR and VSA in the KVM region or select
                both if you have respective storage arrays </p><p id="storage"><b>For 3PAR storage </b>: Edit the
                  <codeph>cinder_conf.hp3parSample</codeph> file to configure the 3PAR settings. For
                example:</p><codeblock>[DEFAULT]
enabled_backends=hp3par

[hp3par]
volume_backend_name=&lt;mybackendname1>
hp3par_api_url=https://&lt;management_ip>:8080/api/v1
hp3par_username=&lt;username>
hp3par_password=&lt;password>
hp3par_cpg=&lt;cpg>
san_ip=&lt;san_ip>
san_login=&lt;username>
san_password=&lt;password>
hp3par_iscsi_ips=&lt;iscsi_target_ips seperated by ,>
volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
hp3par_debug=False
hp3par_iscsi_chap_enabled=false
hp3par_snapshot_retention=48
hp3par_snapshot_expiration=72
</codeblock><p>
                <b>VSA storage</b>: Edit the <codeph>cinder_conf.vsasample</codeph> file to
                configure the VSA settings. For example:
                <codeblock>[DEFAULT]
enabled_backends=hplefthand
                            
[hplefthand]
volume_backend_name=&lt;mybackendname2>
hplefthand_username = &lt;username>
hplefthand_password = &lt;password>
hplefthand_clustername = &lt;Cluster Name>
hplefthand_api_url = https://&lt;Iscsi Virtual IP address>/lhos 
</codeblock></p><p><b>3PAR
                  and VSA storage</b>: If you are using both 3par and VSA, the configuration should
                look like:</p><p>
                <codeblock>[DEFAULT]
 enabled_backends=hp3par, hplefthand
[hp3par]
volume_backend_name=&lt;mybackendname1>
hp3par_api_url=https://&lt;management_ip>:8080/api/v1
hp3par_username=&lt;username>
hp3par_password=&lt;password>
hp3par_cpg=&lt;cpg>
san_ip=&lt;san_ip>
san_login=&lt;username>
san_password=&lt;password>
hp3par_iscsi_ips=&lt;iscsi_target_ips seperated by ,>
volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
hp3par_debug=False
hp3par_iscsi_chap_enabled=false
hp3par_snapshot_retention=48
hp3par_snapshot_expiration=72
                            
[hplefthand]
volume_backend_name=&lt;mybackendname2>
hplefthand_username = &lt;username>
hplefthand_password = &lt;password>
hplefthand_clustername = &lt;Cluster Name>
hplefthand_api_url = https://&lt;Iscsi Virtual IP address>/lhos 
</codeblock>
              </p></li>
            <li>Save the file to <codeph>cinder_conf</codeph>.</li>
          </ol></li>
        <li>After editing the JSON files, validate each JSON file to make sure there are no
          syntactical errors using the tool of your preference. For example, using the Python
          json.tool: <codeblock>python -m json.tool &lt;filename>.json</codeblock></li>
      </ol></section>
    <section id="patches">
      <title>Apply the Rabbit MQ Random Password patch</title>
      <p>For information, see the README in the download or  <xref
          href="carrier-grade-install-rabbitmq-patch.dita#topic10581cgirp">Applying the Random RabbitMQ
          Password Patch</xref>.</p>
    </section>
    
    <section><title>Apply the Reverse Path Filter Patch</title>After deploying the HPE OpenStack
      cloud, you might not be able to access the Horizon interface and the OpenStack services cannot
      be accessed over the CAN network. <p>To prevent this problem, install a patch to configure the
        kernel to set the reverse path filter (<codeph>rp_filter</codeph>) to 1 in the
          <codeph>all</codeph> and <codeph>default</codeph> configuration files. </p><p>For more
        information, see the README in the download or  <xref
          href="carrier-grade-install-rp-filter.dita#topic10581cgirf">Applying the Reverse Path Filter
          Patch</xref>.</p></section>
    
    <section><title>Apply the Helion Dynamic Password Patch</title>The HPE Helion OpenStack cloud
      deployment creates a default password to log into the Helion interface. You can deploy a patch
      to configure the installer to generate a dynamic user name and password for the Helion
      interface. The user name can password can be retrieved from a configuration files. <p>For more
        information, see the README in the download or  <xref
          href="carrier-grade-install-helion-password-patch.dita#topic10581cgihpp">Applying the Helion
          Dynamic Password Patch</xref>.</p></section>
    
    <section><title>Apply the NIC bonding patch</title><p>HPE Helion OpenStack Carrier Grade environments using NIC bonding, see the README in the download
        or <xref href="carrier-grade-install-nic-patch.dita#topic10581cginp">Applying the NIC Bonding
          Patch</xref> to install a required patch.</p></section>
    
    <section><title>Deploy the HPE Helion OpenStack cloud</title>
      <ol>
        <li>Once you have correctly edited all the json files and applied the appropriate patches, run the HPE Helion
          OpenStack configuration processor:
            <codeblock>hcfgproc -d definition.json</codeblock><p>The HPE Helion OpenStack
            configuration processor is a script, called <codeph>hcfgproc</codeph>, that is
            incorporated into the installation environment. </p></li>
        <li>Review the CloudDiagram, <codeph>hosts.hf</codeph>, and
            <codeph>net/interfaces.d/eth.cfg</codeph> files to make sure the network settings are
          correct.</li>
        <li>Initialize network interfaces on all the cloud nodes using the following command:
            <codeblock><codeph>hnetinit &lt;cloudname&gt;</codeph></codeblock><p>You can run this
            command from any directory.</p><p>After this command completes, all cloud nodes and CLM
            network interfaces should be set correctly.</p></li>

        <li>Use the following command to deploy the cloud:
            <codeblock><codeph>hdeploy &lt;cloudname&gt;</codeph></codeblock><p>Once cloud
            deployment is successfully complete, there will be 3 controller nodes in the non-KVM
            region.</p></li>
      </ol>
    </section>
    <section id="patch-after">
      <title>Configure LDAP to enable CLI and use Keystone v3</title>
      <p>By default, the HPE Helion OpenStack Carrier Grade services are configured to use Keystone
        v2 authorization. The services need to be modified to use Keystone v3.  Users will not be
        able to execute OpenStack CLI commands until the specific changes are made on all three
        controller nodes in the non-KVM region. </p>
      <p>For information, see <xref href="carrier-grade-install-config-ldap3.dita#topic10581cgipl">Configuring LDAP CLI Support</xref></p>
    </section>
    <section><title>Applying the HLM Port Security Patch</title>This patch secures all ports on the
      HLM VM. The standard protocol ports will allow inbound access only and the only the cloud
      nodes will have inbound access to the other ports of the HLM VM.<p>For more information, see
        the README in the download or <xref
          href="carrier-grade-install-secure-port-patch.dita#topic10581cgispp">Applying the HLM Port
          Security Patch</xref>.</p></section>    
    <section>
      <title>Disable the Root User</title>
      <p>For security purposes, we strongly recommend disabling root. After the
        <codeph>root</codeph> user is disabled, you can use another default user account,
        <codeph>cghelion</codeph> to access the HLM VM and any actions that require root access
        should be done using <codeph>sudo</codeph>.</p>
      <p>For more information, see <xref href="carrier-grade-install-disable-root.dita#topic10581cgidr"
        >Disabling the Root User</xref>.</p>
    </section>
    <section id="next-step">
      <title>Next Step</title>
      <p><xref href="carrier-grade-install-launch-horizon.dita#topic10581cgilh">Launching the Horizon Interface</xref></p>
      <!--<p><xref href="carrier-grade-install-kvm-cloud-GA.dita">Deploying the KVM Region</xref></p>-->
    </section>
  </body>
</topic>
