<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic10581c2is">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 2.1 Alpha: Deploying the Non-KVM Region
    Cloud in a Sacramento Deployment</title>
  <prolog>
    <metadata>
      <othermeta name="layout" content="default"/>
      <othermeta name="product-version" content="HP Helion OpenStack Carrier Grade 2.0"/>
      <othermeta name="role" content="Storage Administrator"/>
      <othermeta name="role" content="Storage Architect"/>
      <othermeta name="role" content="Michael B"/>
      <othermeta name="product-version1" content="HP Helion OpenStack Carrier Grade 2.0"/>
    </metadata>
  </prolog>
  <body>
    <p>After the lifecycle manager VM is installed, the next task in installing the Sacramento
      deployment is to deploy the HP Helion OpenStack cloud.
      <!--Most sections here are shared with the other install docs; use care when editing--></p>
    <section id="conref-hos-install">
      <title>Launch the HPE Helion OpenStack cloud</title>
      <p>Use the following steps on the lifecycle manager host to log on to the lifecycle
        manager:</p>
      <ol id="ol_ncd_wff_zt">
        <li>Log into the lifecycle manager. <codeblock>ssh &lt;CLM_IP></codeblock><p>where:
            HLM_VM_IP is the CLM IP of the lifecycle manager. Locate the IP address for the
            lifecycle manager in the <codeph>/root/infra-ansible-playbooks/group_vars/all</codeph>
            file under <codeph>hlm_clmstaticip</codeph> field.</p><p>Use the default credentials: </p><p>
            <codeblock>User Name: root
Password: cghelion</codeblock>
          </p><p><b>Important:</b> After logging in with the default password, make sure you change
            the password for the <codeph>root</codeph> user. </p></li>
        <li>Execute the following command to switch to the root
          user:<codeblock>sudo su -</codeblock></li>
        <li>Change to the home directory.<codeblock>cd ~</codeblock></li>
      </ol>
    </section>
    <section id="conref-provision">
      <title>Provision the new cloud</title>
      <ol id="ol_kwm_c1k_zt">
        <li>Execute the following command to provision and configure the HPE Helion OpenStack cloud.
            <codeblock>hlm define -t sacramento &lt;cloudname&gt;</codeblock><p>Where:
              <codeph>&lt;cloudname></codeph> is the name of the cloud to create.</p><p
            id="cloudname">The cloud name can be any combination of up to 40 digits, letters:<ul
              id="ul_lz5_rbf_bt">
              <li>
                <p>Cannot contain with the <codeph>-</codeph> character;</p>
              </li>
              <li>
                <p>Must be greater than 0 characters;</p>
              </li>
              <li>
                <p>No restriction on the number of letters or digits (0 or more).</p>
              </li>
            </ul></p><p>The command creates the <codeph>/var/hlm/clouds/&lt;cloudname></codeph>
            directory, which contains several JSON template files. </p><p><image
              href="../../../media/carrier.grade.docs/CGH-2-install-hlm-define.png"
              id="image_bp1_c45_45" width="600"/></p></li>
      </ol>
    </section>
    <section>
      <title>Copy the node-provision.json file</title>
      <p>Copy the <codeph>node-provision.json</codeph> from the lifecycle manager host to the
        &lt;cloud name> directory on the lifecycle manager. Execute the following command from the
        lifecycle manager host:
        <codeblock>~/ scp /cg/node-provision.json root@192.168.122.240:/var/hlm/clouds/&lt;cloud name&gt;</codeblock></p>
    </section>
    <section id="conref-node-provision-json">
      <title>Edit the node-provision JSON file</title>
      <p>On the lifecycle manager, modify the <codeph>node-provision.json</codeph> file in the
          <codeph>/var/hlm/clouds/&lt;cloudname></codeph> directory of the lifecycle manager. This
        file supplies input values to the <codeph>hprovision</codeph> script, later in the
        installation. </p>
      <ol id="ol_ub2_xfl_zs">
        <li>Change to the <codeph>&lt;cloudname&gt;</codeph>
          directory:<codeblock>cd /var/hlm/clouds/&lt;cloudname&gt;</codeblock></li>
        <li>Edit <codeph>node-provision.json</codeph> file to change only the following fields:
            <table id="table_i3b_mgf_zt">
            <tgroup cols="2">
              <colspec colname="col1" colsep="1" rowsep="1"/>
              <colspec colname="col2" colsep="1" rowsep="1"/>
              <thead>
                <row>
                  <entry colsep="1" rowsep="1">Field</entry>
                  <entry colsep="1" rowsep="1">Description</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>Pxe-mac-address</entry>
                  <entry>Specify the MAC address of the interface you want to PXE boot onto. This is
                    not same as iLO* MAC address.</entry>
                </row>
                <row>
                  <entry>pm_ip</entry>
                  <entry>Specify the power management IP address (iLO IP address).</entry>
                </row>
                <row>
                  <entry>pm_user</entry>
                  <entry>Create a power management user name (iLO user name).</entry>
                </row>
                <row>
                  <entry>pm_pass</entry>
                  <entry>Create a power management password (iLO password).</entry>
                </row>
                <row>
                  <entry>failure_zone, vendor, model, os_partition_size, data_partition_size</entry>
                  <entry>The values in these fields are added to the <codeph>nodes.json</codeph>
                    file used during cloud deployment. </entry>
                </row>
              </tbody>
            </tgroup>
          </table><p>*iLO is HPE Integrated Lights-Out, a server management tool embedded with the
            ProLiant servers. Consult your iLO documentation for information on how to locate the
            iLO IP address, user name, and password.</p></li>
        <li>Save and close the file.</li>
      </ol>
    </section>
    <p>Click here to see a <xref href="../json/carrier-grade-install-kvm-node-json-sac.dita">sample
        node-provision.json file</xref>.</p>
    <section id="conref-pxe-boot">
      <title>Configure PXE boot</title>
      <p>After you edit the <codeph>node-provision.json</codeph> file, you must enable one-time PXE
        boot on the servers to set the correct boot order. Execute the following on the lifecycle
        manager:</p>
      <ol id="ol_d5m_dhf_zt">
        <li>Copy the <codeph>ilopxebootonce.py</codeph> from the
            <codeph>/root/cg-hlm/dev-tools/ilopxebootonce.py</codeph> to the
            <codeph>&lt;cloudname></codeph> directory where you have the
            <codeph>node-provision.json</codeph> file.</li>
        <li>Execute the following script:
          <codeblock>python ilopxebootonce.py node-provision.json</codeblock></li>
      </ol>
      <p>After the script is run, the <codeph>Current One-Time Boot Option</codeph> is set to
          <codeph>Network Device 1</codeph> on all the servers listed in
          <codeph>node-provision.json</codeph> file.</p>
    </section>
    <section id="conref-json">
      <title>Edit JSON files</title>
      <p>The process for installing the HP Helion OpenStack Carrier Grade requires several JSON
        files.</p>
      <p>The following sections detail the files that need to be configured and contain sample JSON
        files that can be referred to and edited for your environment.</p>
      <p><b>Important:</b> Do not store backup of the JSON files inside your cloud directory or any
        where inside the <codeph>/var/hlm/clouds</codeph> directory. You can create a backup folder
        on <codeph>/root/</codeph> directory or on a remote system. <!--HCG-681--></p>
    </section>
    <section id="conref-def-json">
      <title>Edit the definition.json file</title>
      <p>Modify the <codeph>definition.json</codeph> file in the
          <codeph>/var/hlm/clouds/&lt;cloudname&gt;</codeph> directory of the lifecycle manager to
        define the number of ESX compute proxies required in your environment. You need one proxy
        per vCenter; make sure this value is set to
        2<!--set this value to the number of vCenters you have.-->, as shown in the following
        example.</p>
      <p>
        <ol id="ol_zzr_khl_zs">
          <li>Set the <codeph>count</codeph> value to 2 for an HA installation under <codeph>control
              planes</codeph>.<codeblock> "control-planes": [
    {
      "file": ".hos/ccp-dcn-esx.json",
      "resource-nodes": [
        {
          "count": 2,
          "file": ".hos/ccp-vrsg.json"
        }</codeblock></li>
        </ol>
      </p>
    </section>
    <p>Click here to see a <xref href="../json/carrier-grade-install-kvm-def-json-sac.dita">sample
        definition.json file</xref></p>
    <section id="conref-env-json">
      <title>Edit the environment.json file</title>
      <p>Modify the <codeph>environment.json</codeph> file in the
          <codeph>/var/hlm/clouds/&lt;cloudname&gt;</codeph> directory of the lifecycle manager to
        configure the VLANs and network addresses as appropriate for your environment. Configure the
        CLM (management), CAN (api), and BLS (blockstore) networks.</p>
      <p>
        <ul id="ul_smq_yqt_bt">
          <li>
            <p>For each network provide: </p>
            <codeblock>{
"name": "management",
"type": "vlan",
"segment-id": "1551",
"network-address": {
    "cidr": "10.x.x.x/24",
    "start-address": "10.x.x.x",
    "gateway": "10.x.x.x"
}</codeblock>
            <p><b>Note:</b> In the example, the IP addresses are masked for security purposes. Use
              values appropriate for your environment.</p>
          </li>
        </ul>
      </p>
    </section>
    <p>To see a sample <codeph>environment.json</codeph> file, see one of the following: <ul
        id="ul_eln_vcl_25">
      <li><xref href="../json/carrier-grade-install-kvm-env-json-sac.dita">For an Unbonded NIC
            Environment</xref></li>
      <li><xref href="../json/carrier-grade-install-kvm-env-json-bonded-sac.dita#topic4797cgikejbs">For a Bonded
            NIC Environment</xref></li>
      </ul></p>
    <section id="conref-ansible-json">
      <title>Edit the ansible.json file</title>
      <p>Modify the <codeph>ansible.json</codeph> file in the
          <codeph>/var/hlm/clouds/&lt;cloudname>/vars</codeph> directory of the lifecycle
        manager.</p>
    </section>
    <p>Click here to see a <xref href="../json/carrier-grade-install-kvm-ansible-json-sac.dita">sample
        ansible.json file</xref>.</p>
    <section id="conref-esx-json">
      <title>Edit the esx.json file</title>
      <p>Modify the <codeph>esx.json</codeph> file in the
          <codeph>/var/hlm/clouds/&lt;cloudname>/vars</codeph> directory on the lifecycle manager.
        This file is called by the script that installs the HPE Helion OpenStack cloud, later in the
        installation. </p>
    </section>
    <p>Click here to see a <xref href="../json/carrier-grade-install-kvm-esx-json-sac.dita">sample esx.json
        file</xref>.</p>
    <section id="conref-ldap-json">
      <title>Edit the ldap.json file</title>
      <p>Modify the <codeph>ldap.json</codeph> file in the
          <codeph>/var/hlm/clouds/&lt;cloudname>/vars</codeph> directory of the lifecycle
        manager.</p>
    </section>
    <p>Click here to see a <xref href="../json/carrier-grade-install-kvm-ldap-json-sac.dita">sample
        ldap.json file</xref>. </p>
    <section id="conref-dcn-json">
      <title>Edit the dcn.json file</title>
      <p>Modify the <codeph>dcn.json</codeph> file in the
          <codeph>/opt/share/hlm/1/site/services/dcn.json</codeph> directory of the lifecycle
        manager to remove <b>both</b> instances of the following code:
        <codeblock>      {
          "description": "Ingress 47 GRE",
          "firewall": {
            "direction": "in",
            "protocol": "gre"
            },
           "port": 47,
            "scope": "private"
      },    </codeblock></p>
    </section>
    <section id="conref-ccp-dcn-esx-json">
      <title>Edit the ccp-dcn-esx.json file</title>
      <p>Modify the <codeph>member-count</codeph> field in the <codeph>ccp-dcn-esx.json</codeph> in
        the <codeph>/var/hlm/clouds/&lt;cloudname&gt;/.hos/</codeph> directory of the lifecycle
        manager. Set the <codeph>member-count</codeph> field in the <codeph>tiers</codeph> section
        is set to the number of HPE DCN Virtualized Services Controller (VSC) instances:</p>
      <p><!--By default it will be “2”, change to “1” for Build#15.--></p>
      <codeblock> "tiers": [
        {
            "id": "1",
            "member-count": 2,     </codeblock>
    </section>
    <section id="conref-wr-json">
      <title>Edit the wr.json file</title>
      <p><b>For bonded NIC environments</b>, modify the <codeph>wr.json</codeph> file in the
          <codeph>/var/hlm/clouds/&lt;cloudname>/vars</codeph> directory on the lifecycle
        manager.</p>
    </section>
    <p>Click here to see a <xref href="../json/carrier-grade-install-kvm-wr-json-sac.dita">a sample wr.json
        file</xref>. </p>
    <section id="conref-mach-json">
      <title>Edit the machine_architecture.json file</title>
      <p><b>For bonded NIC environments</b>, modify the <codeph>machine_architecture.json</codeph>
        file in the <codeph>/opt/share/hlm/1/site/</codeph> directory of the lifecycle manager to
        fit your hardware model. You have to gather the correct PCI SLOT INFO and add ports
        accordingly on the hardware slots or cards used for networking.</p>
      <p>Click here to see a  <xref
        href="../json/carrier-grade-install-kvm-machine-json-bonded.dita#topic4797cgikmjb">sample
          machine_architecture.json file</xref>.</p>
    </section>
    <section>
      <title>Edit the ironic.json file</title>
      <p>Modify the <codeph>ironic.json</codeph> file in the
          <codeph>/var/hlm/clouds/&lt;cloudname>/vars</codeph> directory to set
          <codeph>ignore_tenant_network</codeph> to <codeph>True</codeph>. Where
          <codeph>&lt;cloudname></codeph> is the name of your
        cloud.<codeblock>{
  "product": {
        "version": 1
  },

  "property-groups": [
    {
       "name": "ironic-vars",
       "properties": {
         "ignore_tenant_network" : "False"
       }
    }
  ]
}</codeblock></p>
    </section>
    <section>
      <title>Edit the neutron-bm.json file</title>
      <p>Modify the <codeph>neutron-bm.json</codeph> file in the
          <codeph>/var/hlm/clouds/&lt;cloudname>/vars</codeph> directory to to specify networking
        information. Where <codeph>&lt;cloudname></codeph> is the name of your cloud.</p>
      <p>
        <codeblock>{
  "product": {
        "version": 1
  },

  "property-groups": [
    {
       "name": "neutron-vars",
       "properties": {
         "switch_channel_timeout": "180",
         "vlan_range": "330:339",
         "bm_interface": "eth1",
         "prov_network": {
            "vlan": "330",
            "cidr": "192.168.150.0/24",
            "ip_start_address": "192.168.150.50",
            "ip_end_address": "192.168.150.200",
            "ip_gateway": "192.168.150.1"
          }
       }
    }
  ]
}</codeblock>
      </p>
    </section>
    <section id="conref-verify-json">
      <title>Verify the JSON files</title>
      <p>After editing the JSON files, validate each JSON file to make sure there are no syntax
        errors using the tool of your preference. For example, using the Python json.tool: </p>
      <codeblock>python -m json.tool &lt;filename>.json</codeblock>
    </section>
    <section id="conref-esx-proxy">
      <title>Configure the ESX Compute Proxy</title>
      <p>The HPE Helion OpenStack Carrier Grade vCenter ESX compute proxy (compute proxy) is a
        driver that enables the Compute service to communicate with a VMware vCenter server. The HPE
        Helion OpenStack Compute Service (Nova) requires this driver to interface with VMware ESX
        hypervisor APIs. You can download the proxy installation files from the <xref
          href="https://helion.hpwsportal.com/catalog.html#/Home/Show" format="html"
          scope="external">Helion Download Network</xref>.
        <!-- Review with Michael Duncan all JSON files and proxy re: bonded set up --></p>
      <p>For instructions on installing the ESX compute proxy, see <xref
          href="../carrier-grade-install-esx-proxy.dita#topic10581cgiep">Deploy the ESX Compute
        Proxy</xref>.</p>
    </section>
    <section id="conref-vmware-workaround">
      <title>Deploy the vmWare Installation Workaround</title>
      <p>During the installation process, when powering on the ESX Compute Proxy VM, you might
        receive a <codeph>VMK_NO_MEMORY</codeph> error. This is a known problem with HP server and
        ESXi 5.x. To prevent this issue, you need to upgrade the version of the HP Agentless
        Management Service (AMS) that comes with the HP servers. </p>
      <p>For more information, see <xref href="../CGH-2-install-vmware-workaround.dita#topic10581c2ivw"
          >vmWare Installation Workaround</xref>.</p>
    </section>
    <section id="conref-create-cloud">
      <title>Provision the cloud nodes and bring the cloud nodes up</title>
      <ol id="ol_f1b_cmf_zt">
        <li>On the lifecycle manager, use the following script to start the provisioning of the HPE
          Helion OpenStack cloud:
            <codeblock>hlm provision -c &lt;cloudname&gt;          </codeblock><p>Where:
              <codeph>&lt;cloudname></codeph> is the name of the cloud you created. The script takes
            approximately 15 to 30 minutes.</p><p>This script prepares the servers for the HPE
            Helion OpenStack Carrier Grade cloud installation including the hLinux operating system.
            The script also PXE boots the nodes specified in <codeph>node-provision.json</codeph>
            file and tracks the PXE boot completion process. The script also creates the
              <codeph>nodes.json</codeph> file in the directory. </p><p>You can log in to the iLO
            server management tool for each of the nodes to monitor the boot process. Consult your
            iLO documentation for information on how to log into iLO. </p></li>
        <li>Make sure the nodes are booted up using iLO. For example:<p><image
          href="../../../media/CGH-2-install-cobbler.png" width="600" id="image_c5k_d4b_kt"
              /></p><p><b>Note:</b> In rare situations, a node might not boot due to an issue with
            network configuration. If this happens, you need to log into that node and manually boot
            using the Retry Auto-Configuration option. The provisioning will continue as you reboot
            that node. See <xref
              href="../../Troubleshooting/CGH-2-troubleshooting-install.dita#topic10581c2ti/provision"
              >Node fails to boot during HLM provision</xref>. </p>Once the baremetal nodes are
          provisioned, make sure the <codeph>nodes.json</codeph> file is generated to the
            <codeph>var/hlm/clouds/&lt;cloud name&gt;/</codeph> directory. The
            <codeph>nodes.json</codeph> file will have entries for your environment: <table
            frame="all" rowsep="1" colsep="1" id="table_c3c_43z_g5">
            <tgroup cols="2">
              <colspec colname="c1" colnum="1" colwidth="1.0*"/>
              <colspec colname="c2" colnum="2" colwidth="1.0*"/>
              <thead>
                <row>
                  <entry>Deployment</entry>
                  <entry>Nodes</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>Denver</entry>
                  <entry>3 controllers</entry>
                </row>
                <row>
                  <entry>Tahoe</entry>
                  <entry>3 controllers; 1 DCN host</entry>
                </row>
                <row>
                  <entry>Memphis</entry>
                  <entry>3 controllers, 1 DCN Hosts, 1 VRS-G and the 2 compute proxy nodes.</entry>
                </row>
                <row>
                  <entry>Sacramento</entry>
                  <entry>3 controllers, 1 DCN Hosts, 1 VRS-G and the 2 compute proxy nodes.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></li>
        <li>Verify that each node in the <codeph>nodes.json</codeph> file is installed and active.
            <ol id="ol_g1b_cmf_zt">
            <li>Ping proxy node from lifecycle manager with the IP using IP specified in
                <codeph>nodes.json</codeph> file.</li>
            <li>SSH to the compute proxy node from the lifecycle manager using IP specified in
                <codeph>nodes.json</codeph> file.</li>
          </ol></li>
      </ol>
    </section>
    <section id="conref-back-end">
      <title>Configure the back-end drivers</title>
      <p>Configure the back-end drivers to enable management of the OpenStack Block Storage volumes
        on vCenter-managed data stores and 3PAR and/or VSA storage arrays. The HPE Block Storage
        (Cinder) service allows you to configure multiple storage back-ends. A sample file provides
        all the variables needed to use ESX in the non-KVM region and HPE StoreVirtual VSA and/or
        HPE StoreServ (3PAR) attched to the KVM region. You need to edit this file for your
        environment and save as <codeph>cinder_conf</codeph>.
        <!--Section shared with Memphis only--></p>
      <p>Use the following steps to configure back-end support:</p>
      <ol id="ul_btr_l52_xs">
        <li>Change to the <codeph>/cinder/blocks</codeph> directory:
          <codeblock>cd /var/hlm/clouds/&lt;cloudname>/services/cinder/blocks</codeblock> Where
            <codeph>&lt;cloudname></codeph> is the name you assigned to the cloud. The directory
          contains several sample Cinder configuration files that you can edit, depending upon which
          storage method(s) you are using.</li>
        <li>Edit the <codeph>cinder_conf.multiBackendSample</codeph> file.<p>Use the
              <codeph>enabled_backends</codeph> variable to list each of the back-ends you are
            using. You must specify at least one back-end for the non-KVM region and one or more for
            the KVM region.</p><table id="backend">
            <tgroup cols="3">
              <colspec colname="col1" colsep="1" rowsep="1"/>
              <colspec colname="col2" colsep="1" rowsep="1"/>
              <colspec colname="col3" colsep="1" rowsep="1"/>
              <thead>
                <row>
                  <entry colsep="1" rowsep="1">Region/Hypervisor</entry>
                  <entry colsep="1" rowsep="1">Volume Backend</entry>
                  <entry colsep="1" rowsep="1">Backend Name</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>KVM</entry>
                  <entry>3PAR </entry>
                  <entry>hp3par</entry>
                </row>
                <row>
                  <entry>KVM</entry>
                  <entry>VSA</entry>
                  <entry>hplefthand</entry>
                </row>
                <row>
                  <entry>Non-KVM </entry>
                  <entry>ESX Datastores</entry>
                  <entry>vmdk </entry>
                </row>
              </tbody>
            </tgroup>
          </table><p>You can use either 3PAR and VSA in the KVM region or select both if you have
            respective storage arrays </p><p><b>Note:</b> The HP 3PAR Web Services API server must
            be enabled and running on the HP 3PAR storage system. For more information, see the
            OpenStack documentation at: <xref
              href="http://docs.openstack.org/liberty/config-reference/content/enable-hp-3par-fibre-channel.html"
              format="html" scope="external">Enable the HP 3PAR Fibre Channel and iSCSI
              drivers</xref>.</p><p id="storage">A typical configuration file that enables all three
            back ends appears similar to the following
          example:</p><codeblock>[DEFAULT]
enabled_backends=vmdk, hp3par, hplefthand
...                                
          
[hp3par]
volume_backend_name=&lt;mybackendname1>
hp3par_api_url=https://&lt;management_ip>:8080/api/v1
hp3par_username=&lt;username>
hp3par_password=&lt;password>
hp3par_cpg=&lt;cpg>
san_ip=&lt;san_ip>
san_login=&lt;username>
san_password=&lt;password>
hp3par_iscsi_ips=&lt;iscsi_target_ips seperated by ,>
volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
hp3par_debug=False
hp3par_iscsi_chap_enabled=false
hp3par_snapshot_retention=48
hp3par_snapshot_expiration=72
                            
[hplefthand]
volume_backend_name=&lt;mybackendname2>
hplefthand_username = &lt;username>
hplefthand_password = &lt;password>
hplefthand_clustername = &lt;Cluster Name>
hplefthand_api_url = https://&lt;Iscsi Virtual IP address>/lhos 
volume_driver = cinder.volume.drivers.san.hp.hp_lefthand_iscsi.HPLeftHandISCSIDriver
hplefthand_debug = false

[vmdk]
volume_backend_name = &lt;mybackendname3>
vmware_host_ip = &lt;hostip>
vmware_host_username = &lt;username>
vmware_host_password = &lt;password>
#vmware_volume_folder = &lt;volumes_folder>
#vmware_image_transfer_timeout_secs = 7200
#vmware_task_poll_interval = 0.5
#vmware_max_objects_retrieval = 100
volume_driver = cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDriver</codeblock></li>
        <li>Save the file to <codeph>cinder_conf</codeph>.</li>
      </ol>
    </section>
    <section>
      <title>Configure baremetal node ports into access mode</title>
      <p>Ensure that the switch ports connected to baremetal instance nodes are in access mode. This
        is a pre-requisite before provisioning baremetal instances. </p>
      <p>The following example assumes you are using an HP FlexFabric 5930 Switch:
        <codeblock>system-view
interface &lt;interface-name&gt; &lt;port&gt;
port link-type access 
save force quit 
quit </codeblock></p>
      <p><b>Example:</b></p>
      <codeblock>&lt;FTC-R146340-5930A&gt;system-view
System View: return to User View with Ctrl+Z.
[FTC-R146340-5930A]interface Ten-GigabitEthernet 1/0/9:4
[FTC-R146340-5930A-Ten-GigabitEthernet1/0/9:4]port link-type access
[FTC-R146340-5930A-Ten-GigabitEthernet1/0/9:4]save force
Validating file. Please wait...
Saved the current configuration to mainboard device successfully.
[FTC-R146340-5930A-Ten-GigabitEthernet1/0/9:4]quit
[FTC-R146340-5930A]quit
&lt;FTC-R146340-5930A&gt;</codeblock>
    </section>
    <section id="conref-config-proc">
      <title>Deploy the HPE Helion OpenStack cloud</title>
      <ol id="ol_bst_mmf_zt">
        <li>Run the HPE Helion OpenStack Carrier Grade configuration processor:
            <codeblock>hlm generate –c &lt;cloudname&gt;</codeblock><p>This command runs the
            configuration processor, a script (<codeph>hcfgproc</codeph>) that is incorporated into
            the installation environment. This command generates the necessary configuration for the
            cloud. </p><p>When the command completes without errors, you will a
              message:</p><p><image href="../../../media/CGH-2-install-proc-done.png" width="600"
              id="image_sqd_rpb_kt"/></p></li>
        <li>Review the CloudDiagram, <codeph>hosts.hf</codeph>, and
            <codeph>/etc/network/interfaces.d/eth.cfg</codeph> files to make sure the network
          settings are correct.</li>
        <li>Initialize network interfaces on all the cloud nodes using the following command:
            <codeblock>hlm netinit –c &lt;cloudname&gt;</codeblock><p>Where:
              <codeph>&lt;cloudname&gt;</codeph> is the name of the cloud you created.</p><p>After
            this command completes, all cloud nodes and CLM network interfaces are configured. The
            output for the command should appear similar to the following:</p><p><image
              href="../../../media/CGH-2-install-hnetinit-output.png" id="image_tft_r11_1t"
              width="400"/></p><p>The <codeph>hlm netinit</codeph> command also runs a ping on the
            cloud nodes. </p><p><image href="../../../media/CGH-2-install-hnetinit-ping.png"
              width="300" id="image_qrd_3gw_rt"/></p></li>
      </ol>
    </section>
    <section id="conref-hdeploy">
      <title>Deploy HPE Helion OpenStack</title>
      <ol id="ol_jxp_dnf_zt">
        <li>Use the following command to deploy the HPE Helion OpenStack cloud:
            <codeblock>hlm deploy –c &lt;cloudname&gt;</codeblock><p>This command takes a
            significant period of time. When this command completes without errors or warnings, the
            non-KVM cloud installation is complete. Use the following sections to configure the
            Horizon interface and configures virtual networking. </p></li>
        <li>Enter <codeph>exit</codeph> to leave the root shell.</li>
      </ol>
    </section>
    <p><b>Note:</b> This command also configures the ESX environment.</p>
    <section id="conref-ldap-keystone">
      <title>Configure LDAP to enable CLI and use Keystone v3</title>
      <p>By default, the HPE Helion OpenStack Carrier Grade services are configured to use Keystone
        v2 authorization. The services need to be modified to use Keystone v3. Users will not be
        able to execute OpenStack CLI commands until the specific changes are made on all three
        controller nodes in the non-KVM region. </p>
      <p>For information, see <xref href="../carrier-grade-install-config-ldap3.dita#topic10581cgicl"
          >Configuring LDAP CLI Support</xref></p>
    </section>
    <section id="conref-vsc-vm">
      <title>Verify the VSC VMs</title>
      <p>The installation process creates virtual machines for VSC. Use the following steps to
        verify that the VSC VMs are installed and are operational:</p>
      <ol id="ol_fkv_xjj_1t">
        <li>SSH to your VSC VM from the lifecycle manager host using the DCM IP. The default
          username and password: <codeph>admin/admin</codeph><p>
            <codeblock>ssh cghelion@&lt;CLM IP of DCN Host&gt;</codeblock>
          </p></li>
        <li>Log in as root: <codeblock>sudo -i</codeblock></li>
        <li>Execute the following command to launch the VSC
            console:<codeblock>virsh console vsc</codeblock><p>The default username and password are
              <codeph>admin/admin</codeph>.</p></li>
        <li>Execute the following command: <codeblock>admin display-config</codeblock></li>
        <li>Execute the following commands to verify the VMs are active:
          <codeblock>show vswitch-controller vsd
show vswitch-controller xmpp-server
ping router "management" &lt;vsd IP or domain name>  </codeblock></li>
      </ol>
    </section>
    <section id="conref-vrs-g"><title>Verify the VRS-G Node</title> The installation creates a VRS-G
      node as part of the cloud deployment. Use the following command to verify that the VRS-G is
      active: <codeblock>show vswitch-controller vswitches  </codeblock><p>The output for the
        command should appear similar to the following image:</p>
      <image href="../../../media/CGH-2-verify-vrsg-alpha.png" width="400" id="image_ip4_jnq_nt"/>
      <!-- <image href="../../media/CGH-install-verify-vrsg.png" id="image_x3t_jvp_vs"/> -->
      <p>Verify that the domain name and DNS server are listed in the
          <codeph>/etc/resolv.conf</codeph> file on all the cloud
        nodes.</p><codeblock>cat etc/resolv.conf          </codeblock><p/><image
          href="../../../media/CGH-install-resolv-conf.jpg" id="image_idc_5wp_vs">
        <alt>Verify VRS-G </alt>
      </image></section>
    <section id="next-step">
      <title>Next Step</title>
      <p><xref href="../CGH-2-install-infoblox.dita#topic15959c2ii">Installing Infoblox</xref>. Infoblox is
        an optional component that integrates with the OpenStack Neutron networking service, to
        provide IP Address Management (IPAM) networking services within OpenStack cloud platforms. </p>
      <p><xref href="../carrier-grade-install-launch-horizon.dita#topic10581cgilh">Launching the Horizon
          Interface</xref>. Launch the Horizon interface to load a required security certificate and
        test the non-KVM deployment.
        <!--<p><xref href="carrier-grade-install-kvm-cloud.dita">Deploying the KVM Region</xref></p>-->
      </p>
    </section>
  </body>
</topic>
