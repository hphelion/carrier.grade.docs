<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic10581c2ild">
<title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 2.1 Beta: Step 3 -Deploying the
    Lifecycle Manager in KVM Cluster (Denver Deployment)</title>
<prolog>
  <metadata>
    <othermeta name="layout" content="default"/>
    <othermeta name="product-version" content="HP Helion OpenStack Carrier Grade 2.0"/>
    <othermeta name="role" content="Storage Administrator"/>
    <othermeta name="role" content="Storage Architect"/>
    <othermeta name="role" content="Michael B"/>
    <othermeta name="product-version1" content="HP Helion OpenStack Carrier Grade 2.0"/>
  </metadata>
</prolog>
<body>
    <!--https://wiki.hpcloud.net/display/HCG/HCG2.1+RCP+WR+AVS+Denver+Cloud+HLM+and+HCG+Controller+Install+Wiki-->
    <p><b>If you are deploying a KVM cluster with multiple servers for Service Availability (SA),
        follow the steps in this section.</b></p>
    <p><b>Note:</b> If you are not deploying a KVM cluster, see <xref
        href="CGH-2-install-lifecycle-denver-standlaone.dita#topic10581c2ild">Deploying the
        Lifecycle Manager On a Standalone KVM</xref>.</p>
  <p>The first phase of the HP Helion OpenStack Carrier Grade installation involves creating a
      bootstrapping the lifecycle manager VM and installation services. The steps in this topic
      deploy the lifecycle manager on a remote server and configure the server as part of a KVM
      cluster. These steps make the cluster OpenSAF aware, which is further monitored by
      libvirtmon.</p>
  <p>The installation uses Ansible playbooks, which are files that contain scripts that execute
    required installation processes.</p>
    <bodydiv id="hlm">
      <section id="prepare">
        <title>Prepare the system for deployment</title>
        <p>Use the following steps to prepare the server on which the lifecycle manager will be
          deployed (the lifecycle manager host):</p>
        <ol id="ol_hx2_33q_f5">
          <li>Log into the lifecycle manager host using the default credentials</li>
          <li>Edit the <codeph>/root/infra-ansible-playbooks/group_vars/all</codeph> file for your
            environment. For information on each variable, refer to the comments in the file with
            each variable. <ul id="ul_tzp_3zz_s5">
              <li id="all1">Set the <codeph>ovs_cloud_only</codeph> variable to
                <codeph>3</codeph>;</li>
              <li id="all2">Set the <codeph>cloud_type</codeph> variable to
                <codeph>'av'</codeph>;</li>
              <li id="all3">Set the CLM IP network details according to your network setup;</li>
              <li id="all4">Set the Cobbler IP addresses according to your network setup;</li>
              <li>Set the <codeph>kvm_cluster</codeph> value in the <codeph>KVM Cluster
                  configuration</codeph> section to <codeph>1</codeph></li>
              <li>Enter the IP addresses for the host systems in the <codeph>KVM Cluster
                  configuration</codeph> section. The first host in the list is active. The other
                hosts are passive, ranked in the order listed in the file. </li>
            </ul><p id="all">The <codeph>group_vars/all</codeph> file should appear similar to the
              following (IP addresses masked for
              security):<codeblock>############################################# Variables for HLM  #################################################################
#These variables are relevant in both All Virtual and BareMetal scenarios.

#ovs_cloud_only: 0 #memphis
#ovs_cloud_only: 1 #ovs
#ovs_cloud_only: 2 #denver
#ovs_cloud_only: 3 #rcp
ovs_cloud_only:  3

#Set this to 'bm' if cloud is being deployed over baremetal.
#Set this to 'av' if the cloud is all virtual
cloud_type: 'av'

#These are hlm related variables that must be changed according to your Baremetal Env
hlm_login_id:       root
hlm_password:       cghelion

#The following variables are for CLM network IP details for HLM
hlm_clmstaticip:    10.x.x.x
hlm_clmnetmask:     255.255.255.0
hlm_clmgateway:     10.x.x.x

#The variables starting with cobbler_ are inputs that are usually given to initcobbler.sh. Set accordingly.
cobbler_pxestartip: 192.x.x.x
cobbler_pxeendip:   192.x.x.x
cobbler_pxestaticip: 192.x.x.x
cobbler_pxenetmask: 255.255.255.0

#Set the location of your images that will be used by libvirt
imagelocation: /var/lib/libvirt/images

#Set the location of your infra-ansible-playbooks
ansible_dir: ~/infra-ansible-playbooks

##################################################################################################################################

############################################# Variables for DCN ##################################################################
#Set the location of dcn bits on KVM
#There must be 4 debs, 2 tar.gz files, 1 vsc qcow2
#You must copy your VSD qcow2 to the imagelocation on the KVM if you want to provision it on the same KVM as the HLM
dcn_dir: ~/cg/dcn
##################################################################################################################################

############################################# Variables for VSD ##################################################################
#Ignore these variables if creating an OVS cloud. This section is relevant only for DCN cloud - in both BM and All Virtual cases
#These variables are used for VSD Configuration
#If you have already configured a VSD and ignore the following variables

dns_domain_name: helion.cg
dns_address: 10.x.x.x
vsd_address: 10.x.x.x
vsd_gateway: 10.x.x.x
vsd_netmask: 255.255.255.0
vsd_name: vsd
vsdimagename: VSD-3.0.0_HP_r3.0_36
upstream_ntp_servers:
   - 10.x.x.x
   - 10.x.x.x
###################################################################################################################################

########################################################### KVM Cluster configuration #############################################

#kvm_cluster: 0 #indicates do not configure kvm cluster
#kvm_cluster: 1 #indicates configure kvm cluster for openSAF
kvm_cluster: 1

#List of KVM hosts that will be participate in cluster if 'kvm_cluster' variable is set to 1
#IP Address for the KVM hosts must be defined in the order of Rank
##In the below list indicates 10.0.0.1=rank1, 10.0.0.2=rank2, 10.0.0.3=rank3...
kvm_hosts:
   - 10.0.0.1
   - 10.0.0.2
   - 10.0.0.3
   - 10.0.0.4
###################################################################################################################################
              </codeblock></p></li>
          <li id="3">Edit the <codeph>group_vars/all-virtual</codeph> file:<ul id="ul_tjd_tzz_s5">
            <li>Update the <codeph>root_user</codeph> and <codeph>root_password</codeph> variables
                with your root
                credentials;<codeblock>#Variables related to pxe booting the VMs - will be used by hprovision 
#The user has to be non-root kvm_ip: 192.168.122.1 
root_user: root 
root_pass: cghelion</codeblock></li>
            <li>Change the <codeph>cloud_topology</codeph> variable to
                <codeph>rcp</codeph>.<codeblock>#Cloud topology to be used. Provide the name of the cloud topology you wish to use in cloud deploy. 

cloud_topology: rcp</codeblock></li>
            </ul></li>
          <li id="4">Edit the <codeph>group_vars/all-kvm-net</codeph> file to make sure the VLAN tag IDs
            are correct according to your network configuration.:
            <codeblock>#vlan tag ids for various networks on base 
KVM clm_vlan_id: 3007 
bls_vlan_id: 210 
dcm_vlan_id: 211</codeblock></li>
        </ol>
      </section>
      <section id="disk-size"><title>Increase the hard disk size (optional)</title>
        
        <p>You can increase
          the hard disk size for the controllers. By default, each controller is assigned 200 GB. <!--ONLY CLI Install?--></p><p>
          <ol id="ol_bn1_5vb_y5">
            <li>On the lifecycle manager host, edit the <codeph>group_vars/all-virtual</codeph> file
              to set the disk and RAM size, as needed.<p>For
                example:<codeblock>#Specify how much RAM and Disk Size must be attached to the cloud VMs. Disk Size is in GBs
vm_ctrl_ram: 16777376
vm_ctrl_disksize: 200</codeblock></p></li>
            <li>Change to the following
              directory:<codeblock>cd ~/infra-ansible-playbooks/roles/SETUP-CLOUD-ON-AV/tasks/</codeblock></li>
            <li>Edit the <codeph>rcp_controller.yml</codeph> file to change the
                <codeph>+80G</codeph> value in the <codeph>QCOW2</codeph> line for each controllers:
                <!--NEED TO BE THE SAME ON ALL?--><p>For
              example:</p><codeblock>###################### Controller 1 #############################################################
-   name:           Create Qcow2 file
    command:        qemu-img create -f qcow2 -o preallocation=metadata {{ imagelocation }}/ccn1.qcow2 +180G</codeblock></li>
          </ol>
          </p></section>
      <section id="kvm-cluster">
        <title>Deploy the lifecycle manager VM</title>
        <p>Follow the steps in this section to deploy the lifecycle manager These steps configure
          the cluster for SA, which includes Libvert monitoring.</p>
        <p>In this scenario the lifecycle manager VM becomes active on the first host that you
          specified in the <codeph>OpenSAF configuration</codeph> in the
            <codeph>group_vars/all</codeph> file. The other hosts are <codeph>passive</codeph>,
          ranked in the order listed in the file.</p>
        <p>In order to deploy a KVM cluster, OpenSAF and libvirmon must be installed and operational
          on all of the hosts.</p>
        <p>To configure the lifecycle manger for a KVM cluster:</p>
        <p>
          <ol id="ol_mr1_g4g_1v">
            <li>On the lifecycle manager, make sure the hosts are specified in the
                <codeph>group_vars/all</codeph> file. </li>
            <li>Change to <codeph>infra-ansible-playbooks</codeph> directory
              <codeblock>~/ cd infra-ansible-playbooks</codeblock></li>
            <li>Execute following
              command:<codeblock>ansible-playbook -i hosts setup_kvm_cluster.yml</codeblock></li>
            <li id="hosts">Check the <codeph>infra-ansible-playbooks/hosts</codeph> file to enter the
              IP address of the <codeph>vibr0</codeph> interface in the
                <codeph>hlm_kvm_host</codeph> field, as shown in the following example.
              <codeblock>[hlm_kvm_host]
192.168.122.1 </codeblock></li>
          </ol>
        </p>
      </section>
    </bodydiv>
    <section>
      <title>Next Step</title>
      <p><xref href="CGH-2-install-denver-standard-cluster.dita#topic10581c2idclisa">Deploy the Standard
          Region</xref></p>
    </section>
  </body>
</topic>
