<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic10581c2ild">
<title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 2.1 Beta: Step 3 -Deploying the
    Lifecycle Manager in a Denver SA Deployment</title>
<prolog>
  <metadata>
    <othermeta name="layout" content="default"/>
    <othermeta name="product-version" content="HP Helion OpenStack Carrier Grade 2.0"/>
    <othermeta name="role" content="Storage Administrator"/>
    <othermeta name="role" content="Storage Architect"/>
    <othermeta name="role" content="Michael B"/>
    <othermeta name="product-version1" content="HP Helion OpenStack Carrier Grade 2.0"/>
  </metadata>
</prolog>
<body>
    <!--https://wiki.hpcloud.net/display/HCG/HCG2.1+RCP+WR+AVS+Denver+Cloud+HLM+and+HCG+Controller+Install+Wiki-->
    <p><b>If you plan to deploy a KVM cluster with multiple servers for Service Availability (SA),
        follow the steps n this section.</b></p>
  <p>The first phase of the HP Helion OpenStack Carrier Grade installation involves creating a
      bootstrapping the lifecycle manager VM and installation services. Use the steps below to
      deploy the lifecycle manager on a remote server and configure the server as part of a KVM
      cluster. These steps make the cluster OpenSAF aware, which is further monitored by
      libvirtmon.</p>
    <p>In this scenario the lifecycle manager VM becomes active on the first host that you specified
      in the <codeph>OpenSAF configuration</codeph> in the <codeph>group_vars/all</codeph> file. The
      other hosts are <codeph>passive</codeph>, ranked in the order listed in the file.</p>
  <p>The installation uses Ansible playbooks, which are files that contain scripts that execute
    required installation processes.</p>
    <p><b>Note:</b> If you are not deploy a KVM cluster with multiple servers for Service
      Availability (SA), see <xref href="CGH-2-install-lifecycle-denver-nonSAF.dita#topic10581c2ild"
        >Deploying the Lifecycle Manager in a Denver SA Deployment</xref>.</p>
    <bodydiv id="hlm">
      <section id="prepare">
        <title>Prepare the system for deployment</title>
        <p>Use the following steps to prepare the server on which the lifecycle manager will be
          deployed (the lifecycle manager host):</p>
        <ol id="ol_hx2_33q_f5">
          <li>Log into the lifecycle manager host using the default credentials</li>
          <li>Edit the <codeph>/root/infra-ansible-playbooks/group_vars/all</codeph> file for your
            environment. For information on each variable, refer to the comments in the file with
            each variable. <ul id="ul_tzp_3zz_s5">
              <li id="all1">Set the <codeph>ovs_cloud_only</codeph> variable to <codeph>3</codeph>;</li>
              <li id="all2">Set the <codeph>cloud_type</codeph> variable to <codeph>'av'</codeph>;</li>
              <li id="all3">
                <p>Set the CLM IP network details for to your network setup;</p>
              </li>
              <li id="all4">
                <p>Set the Cobbler IP addresses according to your network setup;</p>
              </li>
              <li>
                <p><b>Optional:</b> If you plan to deploy a KVM cluster with multiple servers for
                  HA, enter the IP addressed for the host systems in the <codeph>OpenSAF
                    configuration</codeph> in the <codeph>group_vars/all</codeph> file. The other
                  hosts are <codeph>passive</codeph>, ranked in the order listed in the file. Make
                  sure you follow the step in the <xref href="#topic10581c2ild/kvm-cluster"
                    format="dita"/> below.</p>
              </li>
            </ul>
            <p id="all">The <codeph>group_vars/all</codeph> file should appear similar to the following
              (IP addresses masked for
              security):<codeblock>############################################# Variables for HLM  #################################################################
#These variables are relevant in both All Virtual and BareMetal scenarios.

#ovs_cloud_only: 0 #memphis
#ovs_cloud_only: 1 #ovs
#ovs_cloud_only: 2 #denver
#ovs_cloud_only: 3 #rcp
ovs_cloud_only:  3

#Set this to 'bm' if cloud is being deployed over baremetal.
#Set this to 'av' if the cloud is all virtual
cloud_type: 'av'

#These are hlm related variables that must be changed according to your Baremetal Env
hlm_login_id:       root
hlm_password:       cghelion

#The following variables are for CLM network IP details for HLM
hlm_clmstaticip:    10.x.x.x
hlm_clmnetmask:     255.255.255.0
hlm_clmgateway:     10.x.x.x

#The variables starting with cobbler_ are inputs that are usually given to initcobbler.sh. Set accordingly.
cobbler_pxestartip: 10.x.x.x
cobbler_pxeendip:   10.x.x.x
cobbler_pxestaticip: 10.x.x.x
cobbler_pxenetmask: 255.255.255.0

#Set the location of your images that will be used by libvirt
imagelocation: /var/lib/libvirt/images

#Set the location of your infra-ansible-playbooks
ansible_dir: ~/infra-ansible-playbooks

##################################################################################################################################

############################################# Variables for DCN ##################################################################
#Set the location of dcn bits on KVM
#There must be 4 debs, 2 tar.gz files, 1 vsc qcow2
#You must copy your VSD qcow2 to the imagelocation on the KVM if you want to provision it on the same KVM as the HLM
dcn_dir: ~/cg/dcn
##################################################################################################################################

############################################# Variables for VSD ##################################################################
#Ignore these variables if creating an OVS cloud. This section is relevant only for DCN cloud - in both BM and All Virtual cases
#These variables are used for VSD Configuration
#If you have already configured a VSD and ignore the following variables

dns_domain_name: helion.cg
dns_address: 10.x.x.x
vsd_address: 10.x.x.x
vsd_gateway: 10.x.x.x
vsd_netmask: 255.255.255.0
vsd_name: vsd
vsdimagename: VSD-3.0.0_HP_r3.0_36
upstream_ntp_servers:
   - 10.x.x.x
   - 10.x.x.x
###################################################################################################################################
              </codeblock></p></li>
          <li id="3">Edit the <codeph>group_vars/all-virtual</codeph> file:<ul id="ul_tjd_tzz_s5">
            <li>Update the <codeph>root_user</codeph> and <codeph>root_password</codeph> variables
                with your root
                credentials;<codeblock>#Variables related to pxe booting the VMs - will be used by hprovision 
#The user has to be non-root kvm_ip: 192.168.122.1 
root_user: root 
root_pass: cghelion</codeblock></li>
            <li>Change the <codeph>cloud_topology</codeph> variable to
                <codeph>rcp</codeph>.<codeblock>#Cloud topology to be used. Provide the name of the cloud topology you wish to use in cloud deploy. 
#This topology MUST be an "all-virtual" cloud topology only. 
cloud_topology: rcp</codeblock></li>
            </ul></li>
          <li id="4">Edit the <codeph>group_vars/all-kvm-net</codeph> file to make sure the VLAN tag IDs
            are correct according to your network configuration.:
            <codeblock>#vlan tag ids for various networks on base 
KVM clm_vlan_id: 3007 
bls_vlan_id: 210 
dcm_vlan_id: 211</codeblock></li>
        </ol>
      </section>
      <section id="disk-size"><title>Increase the hard disk size (optional)</title>You can increase the hard disk
        size for the controllers. By default, each controller is assigned 200 GB.<p>
          <ol id="ol_bn1_5vb_y5">
            <li>On the lifecycle manager host, edit the <codeph>group_vars/all-virtual</codeph> file
              to set the disk and RAM size, as needed.<p>For
                example:<codeblock>#Specify how much RAM and Disk Size must be attached to the cloud VMs. Disk Size is in GBs
vm_ctrl_ram: 16777376
vm_ctrl_disksize: 200</codeblock></p></li>
            <li>Change to the following
              directory:<codeblock>cd ~/infra-ansible-playbooks/roles/SETUP-CLOUD-ON-AV/tasks/</codeblock></li>
            <li>Edit the <codeph>rcp_controller.yml</codeph> file to change the
                <codeph>+80G</codeph> value in the <codeph>QCOW2</codeph> line for each controllers:
                <!--NEED TO BE THE SAME ON ALL?--><p>For
              example:</p><codeblock>###################### Controller 1 #############################################################
-   name:           Create Qcow2 file
    command:        qemu-img create -f qcow2 -o preallocation=metadata {{ imagelocation }}/ccn1.qcow2 +180G</codeblock></li>
          </ol>
        </p></section>
      <section id="kvm-cluster">
        <title>Deploy the lifecycle manager for a KVM cluster (optional)</title>
        <p>If you plan to deploy a KVM cluster with multiple servers for Service Availability (SA),
          follow the steps n this section to deploy the lifecycle manager on a remote server and
          configures the server as part of a KVM cluster. These steps make the cluster OpenSAF
          aware, which is further monitored by libvirtmon.</p>
        <p>In this scenario the lifecycle manager VM becomes active on the first host that you
          specified in the <codeph>OpenSAF configuration</codeph> in the
            <codeph>group_vars/all</codeph> file. The other hosts are <codeph>passive</codeph>,
          ranked in the order listed in the file.</p>
        <p>In order to deploy a KVM cluster, OpenSAF and libvirmon must be installed and operational
          on all of the hosts.</p>
        <p>To configure the lifecycle manger for a KVM cluster:</p>
        <p>
          <ol id="ol_mr1_g4g_1v">
            <li>On the lifecycle manager, make sure the hosts are specified in the
                <codeph>group_vars/all</codeph> file. </li>
            <li>Change to the <codeph>/root/infra-ansible-playbooks</codeph> directory.</li>
            <li>Execute following
              command:<codeblock>ansible-playbook -i hosts setup_kvm_cluster.yml</codeblock></li>
          </ol>
        </p>
      </section>
      <section>
        <title>Deploy the Lifecycle Manager Virtual Machine</title>
        <p>Use the following steps on the lifecycle manager host to deploy the lifecycle manager on
          that host using Ansible playbooks.</p>
        <ol id="ol_jld_mvb_y5">
          <li>Change to <codeph>infra-ansible-playbooks</codeph> directory
            <codeblock>~/ cd infra-ansible-playbooks</codeblock></li>
          <li>Execute one of the following commands <ul id="ul_ozg_p5q_y5">
              <li><b>Use the HCG Dashboard to deploy the cloud:</b><p>Use this option if you intend
                  to deploy the Standard Region cloud using the HCG Dashboard.
                  <codeblock>ansible-playbook -i hosts setup_hlm_for_UI.yml</codeblock></p></li>
              <li>
                <p><b>Use the CLI to deploy the cloud in a non-OpenSAF environment</b></p>
                <p>Use the following command if you intend to deploy the Standard Region cloud using
                  the command line into a non-OpenSAF environment, or if you intend to deploy one
                  instance of HLM and add it to OpenSAF cluster manually.
                  <codeblock>ansible-playbook -i hosts setup_vcloud.yml</codeblock></p>
                <p>This command deploys and configures the lifecycle manager VM. </p>
              </li>
              <li><b>Use the CLI to deploy the cloud into an OpenSAF environment:</b><p>Use the
                  following command if you intend to deploy the Standard Region cloud using the
                  command line.
                  </p><codeblock>ansible-playbook -i hosts setup_kvm_cluster.yml</codeblock><p>This
                  command uses three Ansible roles, <codeph>SETUP-INFRA-ON-AV</codeph>,
                    <codeph>SETUP-CLOUD-ON-AV</codeph>, and <codeph>HLM-CFG</codeph>, to create and
                  configure the lifecycle manager VM and create the controller VMs. </p></li>
          </ul><p  id="trouble"><b>Note: </b>If you receive the following error during the Ansible deployment:
              <codeblock>Failed to fetch http://hlinux-deejay.us.rdlabs.hpecorp.net/hLinux/dists/cattleprod/InRelease 
Unable to find expected entry 'main/binary-i386/Packages' in Release file (Wrong sources.list entry or malformed file)
Some index files failed to download. They have been ignored, or old ones used instead.</codeblock>Run
              the following command:
              <!--AND RERUN ANSIBLE?--><codeblock>dpkg --remove-architecture i386</codeblock></p></li>
          <li id="hosts">Check the <codeph>infra-ansible-playbooks/hosts</codeph> file to enter the
            IP address of the <codeph>vibr0</codeph> interface in the <codeph>hlm_kvm_host</codeph>
            field, as shown in the following example.
            <codeblock>[hlm_kvm_host]
192.168.122.1 </codeblock></li>
        </ol>
      </section>
    </bodydiv>
  <section id="next-step">
    <title>Next Step</title>
    <p><xref href="CGH-2-install-denver.dita#topic10581c2id">Deploying the Non-KVM Region</xref></p>
  </section>
  </body>
</topic>
