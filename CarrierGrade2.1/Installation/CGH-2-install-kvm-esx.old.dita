<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic10581c2ikeo">
  <title>HPE Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 2.0: Installing the KVM + ESX<tm
      tmtype="reg"/> Deployment</title>
  <prolog>
    <metadata>
      <othermeta name="layout" content="default"/>
      <othermeta name="product-version" content="HPE Helion Openstack Carrier Grade 1.1"/>
      <othermeta name="role" content="Storage Administrator"/>
      <othermeta name="role" content="Storage Architect"/>
      <othermeta name="role" content="Michael B"/>
      <othermeta name="product-version1" content="HPE Helion Openstack Carrier Grade 1.1"/>
    </metadata>
  </prolog>
  <body>
    <p>After the <xref href="carrier-grade-install-hlm-vm.dita#topic10581cgihv">Helion Lifecycle
        Management (HLM) VM is installed</xref>, the next task in installing the <xref
        href="carrier-grade-install-overview.dita#topic1925cgio/install-option">KVM + ESX
        deployment</xref> is to deploy the HPE Helion OpenStack cloud and install the HPE Distributed
      Cloud Networking (DCN) components. You must install the VMware ESX<tm tmtype="reg"/> compute
      proxy.  </p>
    <p><b>Note:</b> If you are installing the KVM-only deployment, see <xref
        href="carrier-grade-install-kvm.dita#topic10581cgik">Installing the KVM Deployment</xref>.</p>
    <section id="prereqs"><title>HPE Distributed Cloud Networking Requirements</title><p>Before
        starting the HPE Helion OpenStack Carrier Grade installation, the VMware vSphere application
        and HPE Distributed Cloud Networking components must be fully installed and
          functioning:</p><p><b>DCN Requirements</b></p><p>HPE Helion Distributed Cloud Networking
        (DCN) must be deployed before starting the cloud deployment. Please refer to DCN
        documentation for details. </p><p>For a quick overview of the DCN components, see <xref
          href="carrier-grade-technical-overview-esx.dita#topic3485cgtoe/dcn">KVM + ESX Deployment
          Architecture Reference</xref></p><p>In production cloud deployments:</p><ul
        id="ul_hnw_pry_zs">
        <li>DCN should be deployed in HA mode (with 2 VSCs, 2 VRS-Gs).</li>
        <li>VSD should be deployed in HA mode (3+1 VSDs). </li>
        <li>VSD should be assigned a DCM IP accessible for cloud deployment. Refer to the <xref
            href="carrier-grade-technical-overview-esx.dita#topic3485cgtoe">architecture
          diagram</xref>.</li>
        <li>Verify that the IP address and domain name of VSD can be accessed using PING and DIG
          commands. </li>
        <li>Make sure all the VSD services are in PASS state.</li>
        <li><xref href="#topic10581c2ikeo/vsd" format="dita">Apply the VSD license</xref> based on single
          or clustered VSD setup. </li>
        <li><xref href="#topic10581c2ikeo" format="dita">Create the required VSD users</xref>.</li>
      </ul><p>Optionally, you can integrate LDAP into the DCN environment.</p><p><b>DNS
          Configuration</b></p><p>Configure appropriate entries for VSD into DNS so that the SRV
        record for <codeph>_xmpp-client</codeph> resolves. This is required for DCN Cloud. Refer to
        the DCN documentation examples of <codeph>BIND</codeph> commands and instructions for
        testing the DNS configuration, such as shown in the following example:</p><p><image
          href="../../media/CGH-2-install-DNS-config.png" width="400" id="image_eyr_z5z_jt"
      /></p>Make sure that reverse DNS resolution <b>is not enabled</b> the for xmpp Host
          record:<p><image href="../../media/CGH-2-install-DNS-config-example.png" width="400"
          id="image_ugq_rvz_jt"/></p></section>
    <section>
      <p><b>ESXi requirements</b></p>
      <p>ESX should be deployed before starting the cloud deployment. For more information, refer to
        the VMware product documentation.</p>
      <p>In production cloud deployments:</p>
      <ul id="ul_ejv_35y_zs">
        <li>ESX Cluster is installed and functional with external storage connected to all the hosts
          that are part of ESX cluster</li>
        <li>ESX Cluster with two (2) or more hosts installed with ESXi 5.5 U2 and above</li>
        <li>ESX Cluster should have HA, DRS &amp; Vmotion Enabled</li>
        <li>ESX Cluster should have Shared Storage</li>
        <li>vCenter is working with the supported versions. One vCenter is
          supported<!-- with HCG 2.0 Build#9-->.</li>
        <li>The CLM and TUL networks must be available to all ESX hosts VMNICs</li>
      </ul>
      <p><b>ESX Network Configuration Requriements</b></p>
      <p>In production cloud deployments, you will need to create the following three (3) port
        groups.</p>
      <ul id="ul_mdl_r5y_zs">
        <li>Cloud Lab Management (CLM VLAN)</li>
        <li>DVRS Datapath (TUL VLAN)</li>
        <li>Trunk Network (All VLANS)</li>
      </ul>
      <p>The names are case sensitive.</p>
      <p>In the VMware vCenter client, the port group list should appear as follows:</p>
      <p><image href="../../media/CGH-install-ESX-ports.png" id="image_os2_2vy_zs"/></p>
    </section>
    <section id="vmware">
      <title>Deploy the VMware vSphere Distributed Switch</title>
      <p>The VMware vSphere Distributed Switch you obtained from VMware must be installed. For more
        information, see <xref href="carrier-grade-install-vsphere-switch.dita#topic10581cgivs">Deploying
          the VMware vSphere Distributed Switch</xref>. </p>
    </section>
    <section id="vapp">
      <title>Deploy the VRSvApp</title>
      <p>The VRSvApp you obtained from DCN must be installed. For more information, see <xref
          href="CGH-2-install-vsvapp.dita#topic10581c2iv">Deploying the VRS vApp</xref></p>
    </section>
    <section id="vsd">
      <title>Verify VSD is running</title>
      <p>Make sure the VSD node is installed by logging into the VSD VM using SSH and running the
        following command:</p>
      <codeblock>service vsd status</codeblock>
      <p>You should see the status as below from VSD VM.</p>
      <p>
        <image href="../../media/CGH-install-virsh-vsd.png" width="400"/></p>
    </section>
    <section id="vsd-license"><title>Apply the VSD License</title>You should have recevied a license
      file when you purchased DCN. You need to apply that license on the KVM Host. For more
      information, see <xref href="carrier-grade-install-vsd-license.dita#topic10581cgivl"/>.</section>
    <section id="create-user-for-plugin-login">
      <title>Create an OSadmin user for VSD</title>
      <p>You must create an administrative user called OSadmin and add it to CMS Group. For more
        information, see <xref href="carrier-grade-install-vsd-user.dita#topic10581cgivu">Create an
          OSadmin User for VSD</xref>.</p>
    </section>
    <section id="hos-install">
      <title>Launch the HPE Helion OpenStack cloud</title>
      <p>Use the following steps on the KVM Host to log on to the lifecycle manager created in <xref
          href="carrier-grade-install-hlm-vm.dita#topic10581cgihv">Bootstrapping the HPE Helion Lifecycle
          Management Virtual Machine and Installation Services</xref>:</p>
      <ol>
        <li>Login to lifecycle manager. <codeblock>ssh &lt;HLM_VM_IP></codeblock><p>where: HLM_VM_IP is the CLM
            IP of the lifecycle manager. Locate the IP address for the lifecycle manager in the
              <codeph>/root/infra-ansible-playbooks/group_vars/all</codeph> file under
              <codeph>hlm_clmstaticip</codeph> field.</p><p>Use the default credentials: </p><p>
            <codeblock>User Name: root
Password: cghelion</codeblock>
          </p><p><b>Important:</b> After logging in with the default password, make sure you change
            the password for the <codeph>cghelion</codeph> user. </p><p><image
              href="../../media/CGH-2-login-hlm.png" width="400" id="image_nnb_fyb_3t"/></p></li>
        <li>Execute the following command to switch to the root
          user:<codeblock>sudo su -</codeblock></li>
        <li>Change to the home directory.<codeblock>cd ~</codeblock></li>
        <li>Execute the following command to provision and configure the HPE Helion OpenStack cloud.
            <codeblock>hlm define -t memphis &lt;cloudname&gt;</codeblock><p>Where::</p><ul>
            <li><codeph>&lt;cloudname&gt;</codeph> is the name of the cloud to create.<p>The cloud
                name can be any combination of up to 63 digits, letters, and the <codeph>-</codeph>
                  chracter:<ul id="ul_lz5_rbf_bt">
                  <li>
                    <p>Cannot start or end with the <codeph>-</codeph> chracter;</p>
                  </li>
                  <li>
                    <p>Must be greater than 0 characters;</p>
                  </li>
                  <li>
                    <p>No restriction on the number of letters or digits (0 or more).</p>
                  </li>
                </ul></p></li>
            <li><codeph>memphis</codeph> is the name of the template to use. The installation kit
              includes a template called <codeph>memphis</codeph> designed to install HPE Helion
              OpenStack, specific to the KVM + ESX deployment.</li>
          </ul><p>The command creates the <codeph>/var/hlm/clouds/&lt;cloudname&gt;</codeph>
            directory, which contains several JSON template files. </p></li>
      </ol>
    </section>
    <section id="configure-node-provision-json">
      <title>Configure the node-provision JSON file</title>
      <p>Modify the <codeph>node-provision.json</codeph> file in the
          <codeph>/var/hlm/clouds/&lt;cloudname></codeph> directory of the lifecycle manager. This file
        supplies input values to the <codeph>hprovision</codeph> script, later in the installation. </p>
      <p><b>Note:</b> To see a sample <codeph>node-provision.json</codeph> file, see <xref
          href="carrier-grade-install-kvm-node-json.dita">Sample node-provision.json File for
          Installing the KVM + ESX Topology</xref>.</p>
      <ol id="ol_ub2_xfl_zs">
        <li>Change to the <codeph>&lt;cloudname&gt;</codeph>
          directory:<codeblock>cd /var/hlm/clouds/&lt;cloudname&gt;</codeblock></li>
        <li>Edit <codeph>node-provision.json</codeph> file to change only the following fields: <table>
            <tgroup cols="2">
              <colspec colname="col1" colsep="1" rowsep="1"/>
              <colspec colname="col2" colsep="1" rowsep="1"/>
              <thead>
                <row>
                  <entry colsep="1" rowsep="1">Field</entry>
                  <entry colsep="1" rowsep="1">Description</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>Pxe-mac-address</entry>
                  <entry>The MAC address of the interface you want to PXE boot onto. This is not
                    same as iLO* MAC address.</entry>
                </row>
                <row>
                  <entry>pm_ip</entry>
                  <entry>The power management IP address (iLO IP address).</entry>
                </row>
                <row>
                  <entry>pm_user</entry>
                  <entry>The power management user name (iLO user name).</entry>
                </row>
                <row>
                  <entry>pm_pass</entry>
                  <entry>The power management password (iLO password).</entry>
                </row>
                <row>
                  <entry>failure_zone, vendor, model, os_partition_size, data_partition_size</entry>
                  <entry>The values in these fields are added to the <codeph>nodes.json</codeph>
                    file used during cloud deployment. </entry>
                </row>
              </tbody>
            </tgroup>
          </table><p>*iLO is HPE Integrated Lights-Out, a server management tool embedded with the
            ProLiant servers. Consult your iLO documentation for information on how to locate the
            iLO IP address, user name, and password.</p></li>
        <li>Save and close the file.</li>
      </ol>
    </section>
    <p>
      <b>Configure PXE boot</b>
    </p>
    <p>After you edit the <codeph>node-provision.json</codeph> file, you must enable one-time PXE
      boot on the servers to set the correct boot order. Execute the following on the lifecycle manager:</p>
    <ol>
      <li>Copy the <codeph>ilopxebootonce.py</codeph> from the
          <systemoutput>/root/cg-hlm/dev-tools/ilopxebootonce.py</systemoutput> to the
          <codeph>&lt;cloudname></codeph> directory where you have the
          <codeph>node-provision.json</codeph> file.</li>
      <li>Execute the following script:
        <codeblock>python ilopxebootonce.py node-provision.json</codeblock></li>
    </ol>
    <p>After the script is run, the <codeph>Current One-Time Boot Option</codeph> is set to
        <codeph>Network Device 1</codeph> on all the servers listed in
        <codeph>node-provision.json</codeph> file.</p>
    <section id="configure-def-json">
      <title>Configure the definition.json file</title>
      <p>Modify the <codeph>definition.json</codeph> file in the
          <codeph>/var/hlm/clouds/&lt;cloudname&gt;</codeph> directory of the lifecycle manager to define the
        number of ESX compute proxies required in your environment. You need one proxy per vCenter;
        make sure this value is set to 2<!--set this value to the number of vCenters you have.-->,
        as shown in the following example.</p>
      <p><b>Note:</b> To see a sample <codeph>definition.json</codeph> file, see <xref
          href="carrier-grade-install-kvm-def-json.dita#topic4797cgikdj">Sample definition.json File
          for Installing the KVM + ESX Topology</xref>.<ol id="ol_zzr_khl_zs">
          <li>Set the number of compute systems to 2 for an HA VRS-G installation under control
            plane.<codeblock>"control-planes": [
        {
            "file": ".hos/ccp-dcn-esx.json",
            "resource-nodes": [
                {
                    "file": ".hos/ccp-cpx.json",
                    "count": 2
                },
                {
                    "file": ".hos/ccp-vrsg.json",
                    "count": 1
                }
 
            ]
        }
    ]</codeblock></li>
        </ol></p>
    </section>
    <section id="configure-env-json">
      <title>Configure the environment.json file</title>
      <p>Modify the <codeph>environment.json</codeph> file in the
          <codeph>/var/hlm/clouds/&lt;cloudname&gt;</codeph> directory of the lifecycle manager to configure
        the VLANs and network addresses as appropriate for your environment. Configure the CLM
        (management), CAN (api), and BLS (blockstore) networks.</p>
      <p><b>Note:</b> To see a sample <codeph>environment.json</codeph> file, see one of the
        following: <ul id="ul_smq_yqt_bt">
          <li><xref href="carrier-grade-install-kvm-env-json.dita">For an Unbonded NIC
              Environment</xref>. <p>For each network provide:
                </p><codeblock>{
              "name": "management",
              "type": "vlan",
              "segment-id": "1551",
              "network-address": {
              "cidr": "10.x.x.x/24",
              "start-address": "10.x.x.x",
              "gateway": "10.x.x.x"
              }
              },        </codeblock><p><b>Note:</b>
              In the example, the IP addresses are masked for security purposes. Use values
              appropriate for your environment.</p></li>
          <li><xref href="carrier-grade-install-kvm-env-json-bonded.dita#topic4797cgikejb">For a Bonded NIC
              Environment</xref></li>
        </ul></p>
    </section>
    <section id="configure-ansible-json">
      <title>Configure the ansible.json file</title>
      <p>Modify the <codeph>ansible.json</codeph> file in the
          <codeph>/var/hlm/clouds/&lt;cloudname&gt;/vars</codeph> directory of the lifecycle manager.</p>
      <p>To see a sample <codeph>ansible.json</codeph> file, see <xref
          href="carrier-grade-install-kvm-ansible-json.dita">Sample ansible.json File for
          Installing the KVM + ESX Topology</xref>.</p>
    </section>
    <section id="configure-esx-json">
      <title>Configure the esx.json file</title>
      <p>Modify the <codeph>esx.json</codeph> file in the
          <codeph>/var/hlm/clouds/&lt;cloudname&gt;</codeph> directory on the lifecycle manager. This file is
        called by the script that installs the HPE Helion OpenStack cloud, later in the installation. </p>
      <p>To see a sample <codeph>esx.json</codeph> file, see <xref
          href="carrier-grade-install-kvm-esx-json.dita">Sample esx.json File for Installing the
          KVM + ESX Topology</xref>.</p>
    </section>
    <section id="configure-ldap-json">
      <title>Configure the ldap.json file</title>
      <p>Modify the <codeph>ldap.json</codeph> file in the
          <codeph>/var/hlm/clouds/&lt;cloudname&gt;/vars</codeph> directory of the lifecycle manager.</p>
      <p>To see a sample <codeph>ldap.json</codeph> file, see <xref
          href="carrier-grade-install-kvm-ldap-json.dita">Sample ldap.json File for Installing
          the KVM + ESX Topology</xref>. </p>
    </section>
    <section id="dcn-json">
      <title>Configure the dcn.json file</title>
      <p>Modify the <codeph>dcn.json</codeph> file in the
          <codeph>/opt/share/hlm/1/site/services/dcn.json</codeph> directory of the lifecycle manager to remove
          <b>both</b> instances of the following code:
        <codeblock>      {
          "description": "Ingress 47 GRE",
          "firewall": {
            "direction": "in",
            "protocol": "gre"
            },
           "port": 47,
            "scope": "private"
      },    </codeblock></p>
    </section>
    <section id="ccp-dcn-esx-json">
      <title>Configure the ccp-dcn-esx.json file</title>
      <p>Modify the <codeph>member-count</codeph> field in the <codeph>ccp-dcn-esx.json</codeph> in
        the <codeph>/var/hlm/clouds/&lt;cloudname&gt;/.hos/</codeph> directory of the lifecycle manager. Make
        sure the <codeph>member-count</codeph> field in the <codeph>tiers</codeph> section is set to
          <b>1</b> as shown:</p>
      <p><!--By default it will be “2”, change to “1” for Build#15.--></p>
      <codeblock>"tiers": [
  {
    "id": "1",
    "member-count": 1,      </codeblock>
    </section>
    <section id="configure-wr-json">
      <title>Configure the wr.json file</title>
      <p><b>For bonded NIC environments</b>, modify the <codeph>wr.json</codeph> file in the
          <codeph>/var/hlm/clouds/&lt;cloudname&gt;/&lt;cloudname>/vars</codeph> directory on the
        lifecycle manager.</p>
      <p>To see a sample <codeph>wr.json</codeph> file, see <xref
          href="carrier-grade-install-kvm-wr-json.dita">Sample wr.json File for Installing the
          KVM + ESX Topology</xref>. </p>
    </section>
    <section id="configure-mach-json">
      <title>Configure the machine_architecture.json file</title>
      <p><b>For bonded NIC environments</b>, modify the <codeph>machine_architecture.json</codeph>
        file in the <codeph>/opt/share/hlm/1/site/</codeph> directory of the lifecycle manager to fit your
        hardware model. You have to gather the correct PCI SLOT INFO and add ports accordingly on
        the hardware slots or cards used for networking.</p>
      <p><b>Note:</b> To see a sample <codeph>machine_architecture.json</codeph> file, see <xref
          href="carrier-grade-install-kvm-env-json-bonded.dita#topic4797cgikejb">Sample enviroment.json
          File for Installing the KVM + ESX Topology in an Bonded NIC Environment</xref>.</p>
    </section>
    <section id="verify-json">
      <title>Verify the JSON files</title>
      <p>After editing the JSON files, validate each JSON file to make sure there are no syntax
        errors using the tool of your preference. For example, using the Python json.tool: </p>
      <codeblock>python -m json.tool &lt;filename>.json</codeblock>
    </section>
    <section id="esx-proxy">
      <title>Configure the ESX Compute Proxy</title>
      <p>The HPE Helion OpenStack Carrier Grade vCenter ESX compute proxy (compute proxy) is a driver
        that enables the Compute service to communicate with a VMware vCenter server. The HPE Helion
        OpenStack Compute Service (Nova) requires this driver to interface with VMware ESX
        hypervisor APIs. You can download the proxy installation files from the <xref
          href="https://helion.hpwsportal.com/catalog.html#/Home/Show" format="html"
          scope="external">Helion Download Network</xref>.
        <!-- Review with Michael Duncan all JSON files and proxy re: bonded set up --></p>
      <p>For instructions on installing the ESX compute proxy, see <xref
          href="carrier-grade-install-esx-proxy.dita#topic10581cgiep">Deploy the ESX Compute
        Proxy</xref>.</p>
    </section>
    <section id="create-a-new-cloud-template-and-bring-the-cloud-nodes-up">
      <title>Provision the cloud nodes and bring the cloud nodes up</title>
      <ol>
        <li>On the KVM Host, use the following script to start the provisioning of the HPE Helion
          OpenStack cloud: <codeblock>hlm provision &lt;cloudname&gt;          </codeblock><p>Where:
              <codeph>&lt;cloudname&gt;</codeph> is the name of the cloud you created. The script
            takes approximately 15 to 30 minutes.</p><p>This script prepares the bare metal servers
            for the HPE Helion OpenStack Carrier Grade cloud installation including the hLinux
            operating system. The script also PXE boots the nodes specified in
              <codeph>node-provision.json</codeph> file and tracks the PXE boot completion process.
            The script also creates the <codeph>nodes.json</codeph> file in the directory.
            </p><p>You can log in to the iLO server management tool for each of the nodes to monitor
            the boot process. Consult yout iLO documentation for information on how to log into iLO.
          </p></li>
        <li>Make sure the nodes are booted up using iLO. For example:<p><image
              href="../../media/CGH-2-install-cobbler.png" width="600" id="image_c5k_d4b_kt"
          /></p>Once the baremetal nodes are provisioned, make sure the <codeph>nodes.json</codeph>
          file is generated. The <codeph>nodes.json</codeph> file will have entries of 3
          controllers, 1 DCN Hosts, 1 VRS-G and the 2 compute proxy nodes.</li>
        <li>Verify that each node in the <codeph>nodes.json</codeph> file is installed and active. <ol>
            <li>Ping proxy node from lifecycle manager with the IP using IP specified in
                <codeph>nodes.json</codeph> file.</li>
            <li>SSH to the compute proxy node from the lifecycle manager using IP specified in
                <codeph>nodes.json</codeph> file.</li>
          </ol></li>
      </ol>
    </section>
    <section id="back-end">
      <title>Configure the back-end drivers</title>
      <p>Configure the back-end drivers to enable management of the OpenStack Block Storage volumes
        on vCenter-managed data stores and 3PAR and/or VSA storage arrays. The HPE Block Storage
        (Cinder) service allows you to configure multiple storage back-ends. Use the following steps
        to configure back-end support:</p>
      <ol id="ul_btr_l52_xs">
        <li>Change to the <codeph>/cinder/blocks</codeph> directory:
          <codeblock>cd ~/&lt;cloudname>/services/cinder/blocks</codeblock> Where
            <codeph>&lt;cloudname></codeph> is the name you assigned to the cloud. The directory
          contains several sample Cinder configuration files that you can edit, depending upon which
          storage method(s) you are using.</li>
        <li>Depending upon the type of storage you are using, edit the
            <codeph>cinder_conf.multiBackendSample</codeph> file. This sample file provides all the
          variables needed to use ESX in the non-KVM region and HPE StoreVirtual VSA and/or HP
          StoreServ (3PAR) attched to the KVM region.<p>Use the <codeph>enabled_backends</codeph>
            variable to list each of the back-ends you are using. You must specify at least one
            back-end for the non-KVM region and one or more for the KVM region.</p><table>
            <tgroup cols="3">
              <colspec colname="col1" colsep="1" rowsep="1"/>
              <colspec colname="col2" colsep="1" rowsep="1"/>
              <colspec colname="col3" colsep="1" rowsep="1"/>
              <thead>
                <row>
                  <entry colsep="1" rowsep="1">Region/Hypervisor</entry>
                  <entry colsep="1" rowsep="1">Volume Backend</entry>
                  <entry colsep="1" rowsep="1">Backend Name</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>KVM</entry>
                  <entry>3PAR </entry>
                  <entry>hp3par</entry>
                </row>
                <row>
                  <entry>KVM</entry>
                  <entry>VSA</entry>
                  <entry>hplefthand</entry>
                </row>
                <row>
                  <entry>Non-KVM </entry>
                  <entry>ESX Datastores</entry>
                  <entry>vmdk </entry>
                </row>
              </tbody>
            </tgroup>
          </table><p><b>Note:</b> You can use either 3PAR and VSA in the KVM region or select both
            if you have respective storage arrays </p><p id="storage">A typical
              <codeph>cinder_conf</codeph> that enables all three back ends appears similar to the
            following
          example:</p><codeblock>[DEFAULT]
enabled_backends=vmdk, hp3par, hplefthand
...                                
          
[hp3par]
volume_backend_name=&lt;mybackendname1>
hp3par_api_url=https://&lt;management_ip>:8080/api/v1
hp3par_username=&lt;username>
hp3par_password=&lt;password>
hp3par_cpg=&lt;cpg>
san_ip=&lt;san_ip>
san_login=&lt;username>
san_password=&lt;password>
hp3par_iscsi_ips=&lt;iscsi_target_ips seperated by ,>
volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
hp3par_debug=False
hp3par_iscsi_chap_enabled=false
hp3par_snapshot_retention=48
hp3par_snapshot_expiration=72
                            
[hplefthand]
volume_backend_name=&lt;mybackendname2>
hplefthand_username = &lt;username>
hplefthand_password = &lt;password>
hplefthand_clustername = &lt;Cluster Name>
hplefthand_api_url = https://&lt;Iscsi Virtual IP address>/lhos 
volume_driver = cinder.volume.drivers.san.hp.hp_lefthand_iscsi.HPLeftHandISCSIDriver
hplefthand_debug = false

[vmdk]
volume_backend_name = &lt;mybackendname3>
vmware_host_ip = &lt;hostip>
vmware_host_username = &lt;username>
vmware_host_password = &lt;password>
#vmware_volume_folder = &lt;volumes_folder>
#vmware_image_transfer_timeout_secs = 7200
#vmware_task_poll_interval = 0.5
#vmware_max_objects_retrieval = 100
volume_driver = cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDriver</codeblock></li>
        <li>Save the file to <codeph>cinder_conf</codeph>.</li>
      </ol>
    </section>
    <section id="nuage"><title>Deploy the Networking Role Patch</title>Before launching the
        <codeph>hlm generate</codeph> command, deploy a patch to update the nuage role required to
      deploy the HPE Helion OpenStack Cloud.<p>For more information, see <xref
          href="carrier-grade-install-network-role-patch.dita#topic10581cginrp">Deploy the Networking Role
          Patch</xref>.</p></section>
    <section id="hos-cloud">
      <title>Deploy the HPE Helion OpenStack cloud</title>
      <ol>
        <li>Run the HPE Helion OpenStack Configuration Processor:
            <codeblock>hlm generate –c &lt;cloudname&gt;</codeblock><p>This command runs the HP
            Helion OpenStack Configuration Processor, a script (<codeph>hcfgproc</codeph>) that is
            incorporated into the installation environment. This command generates the necessary
            configuration for the cloud. </p><p><image
              href="../../media/CGH-2-install-proc-done.png" width="600" id="image_sqd_rpb_kt"
            /></p><p>When the command completes, you will see the following:
            message:</p><codeblock>#################################################################################
The cloud &lt;cloudname> was generated successfully.
################################################################################</codeblock><p>To
            view the complete output of the command, see <xref
              href="carrier-grade-install-hcfgproc-output.dita#topic10581cgiho"> Output from the hcfgproc
              command</xref>.</p></li>
        <li>Review the CloudDiagram, <codeph>hosts.hf</codeph>, and
            <codeph>net/interfaces.d/eth.cfg</codeph> files to make sure the network settings are
          correct.</li>
        <li>Initialize network interfaces on all the cloud nodes using the following command:
            <codeblock>hlm netinit –c &lt;cloud name&gt;</codeblock><p>Where:
              <codeph>&lt;cloudname&gt;</codeph> is the name of the cloud you created.</p><p>After
            this command completes, all cloud nodes and CLM network interfaces are configured. The
            output for the command should appear similar to the following:</p><p><image
              href="../../media/CGH-2-install-hnetinit-output.png" id="image_tft_r11_1t" width="400"
            /></p><p>The <codeph>hlm netinit</codeph> command also runs a ping on the cloud nodes.
              </p><p><image href="../../media/CGH-2-install-hnetinit-ping.png" width="300"
              id="image_qrd_3gw_rt"/></p></li>
      </ol>
    </section>
    <section id="hdeploy">
      <title>Deploy HPE Helion OpenStack</title>
      <ol>
        <li>Use the following command to deploy the HPE Helion OpenStack cloud:
            <codeblock>hlm deploy –c &lt;cloudname&gt;</codeblock><p>This command takes a
            significant period of time. When this command is complete, the non-KVM cloud
            installation is complete. Use the following sections to configure the Horizon interface,
            configure networking, and configure the ESX environment. </p></li>
        <li>Enter <codeph>exit</codeph> to leave the root shell.</li>
      </ol>
      <p><b>Note:</b> If the <codeph>hdeploy</codeph> script fails, review the topics in <xref
          href="carrier-grade-install-build9-workarounds.dita#topic10581cgibw">this section for
          troubleshooting</xref>. </p>
    </section>
    <section><!--<title>Applying the HLM Port Security Patch</title>This patch secures all ports on the lifecycle manager. The standard protocol ports will allow inbound access only and the only the cloud nodes will have inbound access to the other ports of the lifecycle manager.<p>For more information, see the README in the download or <xref href="carrier-grade-install-secure-port-patch.dita#topic10581cgispp">Applying the HLM Port Security Patch</xref>. </p>--></section>
    <section>
      <title>Deploy the Icinga Patch</title>
      <p>After launching the <codeph>hdeploy</codeph> command, deploy three patches to support the
        monitoring of HPE Helion OpenStack. </p>
      <p>For more information, see <xref href="CGH-2-install-icinga-patch.dita#topic10581c2iip">Deploy
          the Icinga Patch</xref>.</p>
    </section>
    <section>
      <title>Deploy the Infoblox patch</title>
      <p>Deploy the Infoblox patch.</p>
    </section>
    <section id="patch-after">
      <title>Configure LDAP to enable CLI and use Keystone v3</title>
      <p>By default, the HPE Helion OpenStack Carrier Grade services are configured to use Keystone
        v2 authorization. The services need to be modified to use Keystone v3. Users will not be
        able to execute OpenStack CLI commands until the specific changes are made on all three
        controller nodes in the non-KVM region. </p>
      <p>For information, see <xref href="carrier-grade-install-config-ldap3.dita#topic10581cgicl"
          >Configuring LDAP CLI Support</xref></p>
    </section>
    <section>
      <title>Verify the VSC VMs</title>
      <p>The installation process creates virtual machines for VSC. Use the following steps to
        verify that the VSC VMs are installed and are operational:</p>
      <ol id="ol_fkv_xjj_1t">
        <li>SSH to your VSC VM from KVM Host using the DCM IP. The default username and password:
            <codeph>admin/admin</codeph><p>
            <codeblock>ssh cghelion@&lt;CLM IP of DCN Host&gt;</codeblock>
          </p></li>
        <li>Execute the following command: <codeblock>admin display-config</codeblock></li>
        <li>Execute the following commands to verify the VMs are active:
          <codeblock>show vswitch-controller vsd
show vswitch-controller xmpp-server
ping router "management" &lt;vsd IP or domain name>  </codeblock></li>
      </ol>
    </section>
    <section><title>Verify the VRS-G Node</title> The installation creates a VRS-G node as part of
      the cloud deployment. Use the following command to verify that the VRS-G is active:
        <codeblock>show vswitch-controller vswitches  </codeblock><p>The output for the command
        should appear similar to the following image:</p>
      <image href="../../media/CGH-2-verify-vrsg-alpha.png" width="400" id="image_ip4_jnq_nt"/>
      <!-- <image href="../../media/CGH-install-verify-vrsg.png" id="image_x3t_jvp_vs"/> -->
      <p>Verify that the domain name and DNS server are listed in the
          <codeph>/etc/resolv.conf</codeph> file on all the cloud
        nodes,.</p><codeblock>cat etc/resolv.conf          </codeblock><image
        href="../../media/CGH-install-resolv-conf.jpg" id="image_idc_5wp_vs">
        <alt>Verify VRS-G </alt>
      </image></section>
    <section id="next-step">
      <title>Next Step</title>
      <p><xref href="carrier-grade-install-launch-horizon.dita#topic10581cgilh">Launching the Horizon
          Interface</xref></p>
      <!--<p><xref href="carrier-grade-install-kvm-cloud.dita">Deploying the KVM Region</xref></p>-->
    </section>
  </body>
</topic>
