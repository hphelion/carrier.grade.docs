<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic10581c2ild">
<title>HPE Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 2.1: Step 3a -Deploying the
    Lifecycle Manager VM (Denver Deployment)</title>
<prolog>
  <metadata>
    <othermeta name="layout" content="default"/>
    <othermeta name="product-version" content="HPE Helion OpenStack Carrier Grade 2.0"/>
    <othermeta name="role" content="Storage Administrator"/>
    <othermeta name="role" content="Storage Architect"/>
    <othermeta name="role" content="Michael B"/>
    <othermeta name="product-version1" content="HPE Helion OpenStack Carrier Grade 2.0"/>
  </metadata>
</prolog>
<body>
    <!--https://wiki.hpcloud.net/display/HCG/HCG2.1+RCP+WR+AVS+Denver+Cloud+HLM+and+HCG+Controller+Install+Wiki-->
    <p>The first phase of the HPE Helion OpenStack Carrier Grade installation involves deploying a
      virtual machine to host lifecycle manager tool for installing and maintaining your cloud. </p>
    <p>The installation uses Ansible playbooks, which are files that contain scripts that execute
      required installation processes.</p>
    <section>
      <title>Copy the required Linux packages</title>
      <p>All of the Linux-related packages required to install and use HP Helion OpenStack Carrier
        Grade are available in the ISO file you downloaded in the prerequisites. You need to mount
        the ISO and copy the files to a location where the installation files can pick them up.</p>
      <p>
        <ol>
          <li>Use the following commands to mount the ISO file on the lifecycle manager host.
            <codeblock>mkdir &lt;local_file> 
mount -oro ~cghelion/hpelinux-hcg-kvm-amd64-20160224.iso.iso &lt;local_file> </codeblock>Where:
              <codeph>&lt;local file></codeph> is the path to the file where you mounted the ISO.
            For
            example:<codeblock>mkdir /mnt/local_repo
mount -oro ~cghelion/hpelinux-hcg-kvm-amd64-20160224.iso.iso /mnt/local_repo/</codeblock></li>
          <li>Execute the following command to copy all of the files off the ISO file to the local
            file
            system:<codeblock>cd /mnt 
tar cf - &lt;local_repo> | ( cd /opt/; tar xf - )</codeblock>Where:
              <codeph>&lt;local file></codeph> is the path to the file where you mounted the ISO.
            For
              example:<codeblock>cd /mnt
tar cf - local_repo | ( cd /opt/; tar xf - )</codeblock><p>The
              example command copies the the <codeph>local_repo</codeph> file from
                <codeph>/mnt/local_repo</codeph> to <codeph>/opt/local_repo</codeph>. </p></li>
          <li>After the files are copied off the ISO, you can unmout the
            ISO.<codeblock>umount /mnt/&lt;local_file></codeblock>Where: <codeph>&lt;local
              file></codeph> is the path to the file where you mounted the ISO. For example:
            <codeblock>umount /mnt/local_repo</codeblock></li>
          <li>Modify the <codeph>/etc/apt/sources.list</codeph> file to make this local file repo
            available to the apt-get command by commenting out the <codeph>deb-src cdrom</codeph>
            and <codeph>deb cdrom</codeph> lines and adding the
              following:<codeblock>deb [trusted=yes] file:&lt;local file> cattleprod contrib main non-free</codeblock><p>Where:
                <codeph>&lt;local file></codeph> is the path to the file where you copied the
              contents of the ISO. For
            example:</p><codeblock>#deb-src cdrom:[hLinux _cattleprod_ - Official amd64 blaster NETINST 20151009-hlm%]/ cattleprod contrib main non-free
#deb cdrom:[hLinux _cattleprod_ - Official amd64 blaster NETINST 20151009-hlm%]/ cattleprod contrib main non-free
deb [trusted=yes] file:/opt/local_repo/ cattleprod contrib main non-free</codeblock></li>
          <li>Execute the following command to update the apt package index
            files:<codeblock>apt-get update</codeblock></li>
          <li>Install patch package on the lifecycle manager host:
            <codeblock>apt-get install patch</codeblock></li>
        </ol>
      </p>
    </section>
    <section>
      <title>Install the OVS bridge patch for the lifecycle manager host</title>
      <p>The OVS bridge patch supports the OVS bridge by updating
          <codeph>infra-ansible-playbooks</codeph> folder and updates lifecycle manager node XML
        file. </p>
      <p>Before deploying the patch, make sure the
          <systemoutput>/etc/network/interfaces</systemoutput> file is updated correctly based on
        your configuration and the ovs-network bridge is created, as described in <xref
          href="../prereqs/carrier-grade-install-network-prepare.dita#topic7646cginpovs">Preparing
          the OVS Network for Installation</xref>. The <codeph>cg-infra-src.tar.gz</codeph> file
        must be in the root folder of the lifecycle manager host. You should have extracted and
        copied the file in <xref href="carrier-grade-install-download-denver.dita#topic7148cgidd"
          >Download the Installation Packages</xref>.</p>
      <p>To deploy the patch: <ol id="ol_xz1_cyh_qv">
          <li>Log into the lifecycle manager host using your credentials. </li>
          <li>Copy the infra-playbooks-ovs.patch patch to the ~/ directory. </li>
          <li>Change to the <codeph>infra-ansible-playbooks</codeph> directory.
            <codeblock>cd ~/infra-ansible-playbooks</codeblock></li>
          <li>Execute the following command to deploy the
              patch:<codeblock> patch -p1 -b &lt; ../infra-playbooks-ovs.patch </codeblock><p>The
                <codeph>/root/infra-ansible-playbooks/group_vars/all</codeph> file is appended with
              the following entry: <codeph>ovs_on_kvm: 1</codeph></p></li>
        </ol></p>
    </section>
    <bodydiv id="hlm">
      <section id="prepare">
        <title>Prepare the system for deployment</title>
        <p>Use the following steps to prepare the server on which the lifecycle manager VM will be
          deployed (the lifecycle manager host):</p>
        <ol id="ol_hx2_33q_f5">
          <li>Log into the lifecycle manager host using your credentials.</li>
          <li>Extract the <codeph>~/cg-infra-src.tar.gz</codeph> file you downloaded in the
              prerequisites.<codeblock>tar -zxvf cg-infra-src.tar.gz </codeblock><p>This file
              deploys the Ansible files and directories required for the installation.</p></li>
          <li>Edit the <codeph>~/infra-ansible-playbooks/group_vars/all</codeph> file for your
            environment. For information on each variable, refer to the comments in the file with
            each variable. <ul id="ul_tzp_3zz_s5">
              <li conref="CGH-2-install-lifecycle-denver-cluster.dita#topic10581c2ild/all1"/>
              <li conref="CGH-2-install-lifecycle-denver-cluster.dita#topic10581c2ild/all2"/>
              <li conref="CGH-2-install-lifecycle-denver-cluster.dita#topic10581c2ild/all3"/>
              <li conref="CGH-2-install-lifecycle-denver-cluster.dita#topic10581c2ild/all4"/>
            </ul><p id="all">The <codeph>group_vars/all</codeph> file should appear similar to the
              following (IP addresses masked for security):</p><p><b>Note:</b> The
                <codeph>group_vars/all</codeph> file cannot contain tab
              characters.<codeblock>############################################# Variables for HLM  #################################################################
#These variables are relevant in both All Virtual and BareMetal scenarios.

#ovs_cloud_only: 0 #memphis
#ovs_cloud_only: 1 #ovs
#ovs_cloud_only: 2 #denver
#ovs_cloud_only: 3 #rcp_denver
ovs_cloud_only:  3

#Set this to 'bm' if cloud is being deployed over baremetal.
#Set this to 'av' if the cloud is all virtual
cloud_type: 'av'

#These are hlm related variables that must be changed according to your Baremetal Env
hlm_login_id:       root
hlm_password:       cghelion

#The following variables are for CLM network IP details for HLM
hlm_clmstaticip:    10.x.x.x
hlm_clmnetmask:     255.255.255.0
hlm_clmgateway:     10.x.x.x

#The variables starting with cobbler_ are inputs that are usually given to initcobbler.sh. Set accordingly.
cobbler_pxestartip: 192.x.x.x
cobbler_pxeendip:   192.x.x.x
cobbler_pxestaticip: 192.x.x.x
cobbler_pxenetmask: 255.255.255.0

#Set the location of your images that will be used by libvirt
imagelocation: /var/lib/libvirt/images

#Set the location of your infra-ansible-playbooks
ansible_dir: ~/infra-ansible-playbooks

##################################################################################################################################

############################################# Variables for DCN ##################################################################
#Set the location of dcn bits on KVM
#There must be 4 debs, 2 tar.gz files, 1 vsc qcow2
#You must copy your VSD qcow2 to the imagelocation on the KVM if you want to provision it on the same KVM as the HLM
dcn_dir: ~/cg/dcn
##################################################################################################################################

############################################# Variables for VSD ##################################################################
#Ignore these variables if creating an OVS cloud. This section is relevant only for DCN cloud - in both BM and All Virtual cases
#These variables are used for VSD Configuration
#If you have already configured a VSD and ignore the following variables

dns_domain_name: helion.cg
dns_address: 10.x.x.x
vsd_address: 10.x.x.x
vsd_gateway: 10.x.x.x
vsd_netmask: 255.255.255.0
vsd_name: vsd
vsdimagename: VSD-3.0.0_HP_r3.0_36
upstream_ntp_servers:
   - 10.x.x.x
   - 10.x.x.x
###################################################################################################################################

########################################################### KVM Cluster configuration #############################################

#kvm_cluster: 0 #indicates do not configure kvm cluster
#kvm_cluster: 1 #indicates configure kvm cluster for openSAF
kvm_cluster: 0

#List of KVM hosts that will be participate in cluster if 'kvm_cluster' variable is set to 1
#IP Address for the KVM hosts must be defined in the order of Rank
##In the below list indicates 10.0.0.1=rank1, 10.0.0.2=rank2, 10.0.0.3=rank3...
kvm_hosts:
   - 10.0.0.1
   - 10.0.0.2
   - 10.0.0.3
   - 10.0.0.4
###################################################################################################################################
              </codeblock></p></li>
          <li conref="CGH-2-install-lifecycle-denver-cluster.dita#topic10581c2ild/3"/>
          <!--<li conref="CGH-2-install-lifecycle-denver-cluster.dita#topic10581c2ild/4"/>-->
        </ol>
      </section>
      <section>
        <p><b>Important:</b> If you specify a non-root user in the <b>all-virtual</b> file, perform
          the following tasks to configure Libvirt to work with Ansible:</p>
        <ol id="ol_tsy_sfd_dv">
          <li>Change to the user directory<codeblock>cd /home/&lt;user>/</codeblock><p>Where
                <codeph>&lt;user></codeph> is the <codeph>root_user</codeph> created in the
                <codeph>group_vars/all-virtual</codeph> file.</p></li>
          <li>Edit the <codeph>.profile</codeph> file to add the <codeph>export</codeph> line
              below:<codeblock>export LIBVIRT_DEFAULT_URI="qemu:///system"</codeblock><p>For
              example:</p><codeblock>if [ -d "$HOME/bin" ] ; then
                PATH="$HOME/bin:$PATH"
                fi     </codeblock></li>
          <li>Execute the following
            command:<codeblock>sudo usermod -a -G libvirt &lt;user></codeblock>Where
              <codeph>&lt;user></codeph> is the <codeph>root_user</codeph> created in the
              <codeph>group_vars/all-virtual</codeph> file.</li>
          <li>Change to the <codeph>etc/</codeph> directory.</li>
          <li>Add a user to the <codeph>sudoers</codeph>
            file:<codeblock>% sudo ALL=(ALL:ALL) ALL
&lt;user> ALL=(ALL) NOPASSWD:ALL</codeblock>Where
              <codeph>&lt;user></codeph> is the <codeph>root_user</codeph> created in the
              <codeph>group_vars/all-virtual</codeph> file.</li>
        </ol>
      </section>
      <section>
        <title>Deploy the Lifecycle Manager Virtual Machine</title>
        <p>Use the following steps on the lifecycle manager host to deploy the lifecycle manager on
          that host using Ansible playbooks.</p>
        <ol id="ol_jld_mvb_y5">
          <li>On the lifecycle manager, change to <codeph>infra-ansible-playbooks</codeph> directory
            <codeblock>cd ~/infra-ansible-playbooks</codeblock></li>
          <li>Execute the following command to deploy the lifecycle manager VM using Ansible:
            <codeblock>ansible-playbook -i hosts setup_hlm_for_UI.yml</codeblock>This command
            deploys and configures the lifecycle manager VM. After deployment, you can add the
            lifecycle manager host to a KVM cluster
              manually.<p><!--<ul id="ul_ozg_p5q_y5"><li><b>Use the HCG Dashboard UI to deploy the cloud:</b><p>Use this option if you intend to deploy the Standard Region cloud using the HCG Dashboard UI. <codeblock>ansible-playbook -i hosts setup_hlm_for_UI.yml</codeblock></p></li><li><p><b>Use the CLI to deploy the cloud</b></p><p>Use the following command if you intend to deploy the Standard Region cloud using the command line:<codeblock>ansible-playbook -i hosts setup_vcloud.yml</codeblock></p><p>This command deploys and configures the lifecycle manager VM. After deployment, you can add the lifecycle manager host to OpenSAF cluster manually. </p></li></ul>--></p></li>
        </ol>
      </section>
      <section>
        <title>Install the OVS bridge patch for the lifecycle manager VM</title>
        <p>After the lifecycle manager VM is up and running, deploy the OVS bridge patch on the
          VM.</p>
        <ol id="ol_qyv_hzh_qv">
          <li>From the lifecycle manager host, log into the lifecycle manager VM.
            <codeblock>ssh cghelion@&lt;CLM_IP></codeblock>Where: <codeph>CLM_IP</codeph> is the IP
            address for the lifecycle manager VM and can be found in the
              <codeph>/root/infra-ansible-playbooks/group_vars/all</codeph> file under
              <codeph>hlm_clmstaticip</codeph> field.<p>Use the default credentials:
            </p><codeblock>User Name: cghelion
Password: cghelion1.</codeblock></li>
          <li>Copy the knc-ovs-bridge.patch file to the  ~/ location. </li>
          <li>Change to the <codeph>kenobi-node-configuration</codeph> directory.
            <codeblock>cd ~/kenobi-node-configuration</codeblock></li>
          <li>Execute the following command to deploy the patch:
            <codeblock>patch -p1 -b &lt; ../knc-ovs-bridge.patch</codeblock></li>
          <li>Change to the <systemoutput>~/cg-hlm/hlm-build</systemoutput> directory
            <codeblock>cd ~/cg-hlm/hlm-build</codeblock></li>
          <li>Execute the following command to integrate the changes into the
            environment:<codeblock>hlm_prepare-env.sh</codeblock></li>
        </ol>
      </section>
      <section conref="CGH-2-install-lifecycle-denver-cluster.dita#topic10581c2ild/disk-size"/>
    </bodydiv>
    <section id="next-step">
      <title>Next Step</title>
      <p><xref href="CGH-2-install-denver-standalone-helion.dita#topic10581c2idh">Deploying HPE Helion
          OpenStack</xref></p>
    </section>
  </body>
</topic>
