<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic10581c2isi">
    <title>HPE Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 2.1:
        Configuring SR-IOV</title>
    <prolog>
        <metadata>
            <othermeta name="layout" content="default"/>
            <othermeta name="product-version" content="HPE Helion OpenStack Carreir Grade 1.1"/>
            <othermeta name="role" content="Storage Administrator"/>
            <othermeta name="role" content="Storage Architect"/>
            <othermeta name="role" content="Michael B"/>
            <othermeta name="product-version1" content="HPE Helion OpenStack Carreir Grade 1.1"/>
        </metadata>
    </prolog>
    <body>
        <p><!--Copied from WR Admin Guide, drop 4--></p>
        <p>An <xref href="CGH-2-install-sriov-pci.dita#topic10581c2isio/sriov">SR-IOV Ethernet
                interface</xref> is a physical PCI Ethernet NIC that implements hardware-based
            virtualization mechanisms to expose multiple virtual network interfaces that can be used
            by one or more virtual machines simultaneously.</p>
        <p>With SR-IOV based NICs, the traditional virtual bridge is no longer required. Each SR-IOV
            port is associated with a virtual function (VF).</p>
        <p>The process for configuring SR-IOV includes creating a VLAN provider network and subnet,
            then attaching VMs to that network.</p>
        <p>In the following example a new virtual machine is launched by user user1 on tenant
            tenant1, with a VF interface connected to the tenant network tenant1-net1 identified
            with VLAN ID 20. </p>
        <p><b>Note:</b> To use PCI passthrough or SR-IOV interfaces, you must have Intel VT-d
            features enabled in the BIOS. </p>
        <p>The exercise assumes that the underlying provider network group0-data0 exists already,
            and that VLAN ID 20 is a valid segmentation ID assigned to tenant1. <ol
                id="ol_eyn_vh2_2v">
                <li>Log in as the admin user to the web management interface. </li>
                <li>Lock the compute node you want to configure. </li>
                <li>Configure the Ethernet interface to be used as a SR-IOV interface. <p>Select the
                            <b>System Inventory</b> page from the <b>Admin</b> panel. Then, click
                        the <b>Hosts</b> tab and click the name of the compute node where the PCI
                        interface is available, click the <b>Interfaces</b> tab, and finally click
                        the <b>Edit Interface</b> button associated with the interface you want to
                        configure. </p><p>
                        <note>The interface can also be configured from the CLI as illustrated
                            below:
                            <codeblock>system host-if-modify -nt pci-sriov -N 4 -p group0-data0 compute-0 eth8</codeblock></note>
                    </p><p>Fill in the window as illustrated below:</p><image
                        href="../../media/carrier.grade.docs/2.1/CGH-21-installation-sriov1.png"
                        id="image_g5x_vj2_2v" width="400"/><p><b>Post-requisite:</b> Access switches
                        must be properly configured to ensure that virtual machines using SR-IOV
                        Ethernet interfaces have the expected connectivity. In a common scenario,
                        the virtual machine using these interfaces connects to external end points
                        only, that is, it does not connect to other virtual machines in the same
                        cluster. In this case: Ethernet interface <codeph>eth8</codeph> is assigned
                        the network type <codeph>pci-sriov</codeph>, enabled to use 4 VFs, and
                        configured as connected to provider network <codeph>group0-data0</codeph>.
                    </p>The Maximum number of VFs displayed in this form is a read-only item which
                    is auto-discovered by the system by inspecting the specific NIC model. This
                    value is reported as 0 when the selected interface does not support SRIOV.<p>
                        <note>By default, a VM that requires an SR-IOV interface is instantiated
                            only if a host NUMA node with a directly attached PCI interface is
                            available. You can specify best-effort instantiation by using an extra
                            specification. For more information, see <i>Specifying Best Effort for
                                PCI NUMA Node Affinity</i> in the <xref
                                href="http://docs.hpcloud.com/pdf/static/CGH-21-KVM-Admin.pdf"
                                format="html" scope="external">KVM Region Administration
                                Guide</xref>. </note>
                    </p></li>
                <li>Click <b>Save</b> to proceed.</li>
                <li>Create the <codeph>tenant1-net1</codeph> tenant network. You must ensure that:
                        <ul id="ul_udm_332_2v">
                        <li>The tenant1 tenant has access to the tenant network, either assigning it
                            as the owner, or by enabling the tenant network's shared flag. </li>
                        <li>The segmentation ID is set to 20. </li>
                    </ul></li>
                <li>Configure the access switch. <p>Configure the physical port on the access switch
                        used to connect to Ethernet interface eth8 as an access port with default
                        VLAN ID of 20. </p></li>
                <li>Unlock the compute node. </li>
                <li>Launch the virtual machine. <ol id="ol_tg1_k32_2v">
                        <li>Log in as user user1 to the web management interface. </li>
                        <li>Configure the network interfaces for the new virtual machine. <p>Open
                                the Launch Instance dialog by clicking the Launch Interface button
                                on the Instances window. Configure the necessary options for the new
                                virtual machine. In particular, click the Networking tab and add a
                                SR-IOV interface on the tenant network tenant1-net1, as illustrated
                                below. Add other network interfaces as needed, there are as many
                                SR-IOV ports available as VFs have been enabled.</p><p><image
                                    href="../../media/carrier.grade.docs/2.1/CGH-21-installation-sriov2.png"
                                    width="400" id="image_swk_ck2_2v"/></p>Click the Launch button
                            to proceed. <p>SR-IOV interfaces can be attached from the CLI when
                                booting a new virtual machine, as illustrated below:
                                <codeblock>nova boot --nic net-id=704e9f3b,vif-model=pci-sriov --flavor small  --image cgcs-guest my-new-vm</codeblock></p></li>
                    </ol></li>
            </ol></p>
                    <p>The new virtual machine instance is up now. It has a SR-IOV VF connection to
            the tenant1-net1 tenant network identified with VLAN ID 20. </p>
        <p>Access switches must be properly configured to ensure that virtual machines using
            PCI-passthrough or SRIOV Ethernet interfaces have the expected connectivity. In a common
            scenario, the virtual machine using these interfaces connects to external end points
            only, that is, it does not connect to other virtual machines in the same cluster. In
            this case: <ul id="ul_p1p_432_2v">
                <li>Traffic between the virtual machine and the access switch can be tagged or
                    untagged. </li>
                <li>The connecting port on the access switch is part of a port-based VLAN. </li>
                <li>If the port is tagged, the allowed VLAN ID range must not overlap with VLAN ID
                    ranges used by the AVS ports. </li>
                <li>The port-based VLAN provides the required connectivity to external switching and
                    routing equipment needed by guest applications to establish connections to the
                    intended end points.</li>
            </ul></p> <p>For connectivity to other virtual machines in the cluster the following configuration is also
            required: <ul id="ul_p2f_t32_2v">
                <li>The VLAN ID used for the tenant network, 10 in this example, and the default
                    port VLAN ID of the access port on the switch are the same. This ensures that
                    incoming traffic from the virtual machine is tagged internally by the switch as
                    belonging to VLAN ID 10, and switched to the appropriate exit ports. </li>
                <li>The target virtual machines are reachable through another port on the compute
                    node, which is managed by the AVS. </li>
                <li>That other port is configured as usual, as a VLAN trunk port, and the tenant
                    network's VLAN ID (10) is included in the tagged range. This ensures that VLAN
                    10 is common to both the passthrough/SR-IOV interface and the AVS port.</li>
            </ul></p>
        <p><b>Related links</b></p>
        <p>For more information, see <xref
                href="https://wiki.openstack.org/wiki/SR-IOV-Passthrough-For-Networking"
                format="html" scope="external">SR-IOV Passthrough for Networking</xref> in the
            OpenStack wiki.</p>
    </body>

</topic>
