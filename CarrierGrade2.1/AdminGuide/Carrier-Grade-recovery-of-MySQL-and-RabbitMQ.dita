<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_vzc_ft5_zv">
  <title>Recovery of MySQL and RabbitMQ</title>
  <body>
    <p><b>Recovery of MySQL</b></p>
    <p>A default deployment of HPE Helion OpenStack Carrier Grade includes a MySQL database instance
      on each controller, configured as a MySQL cluster. </p>
    <p>There are two ways to recover failed MySQL cluster: </p>
    <p>The recovery options should be followed in the order they are specified.</p>
    <p><b>Automated Recovery</b></p>
    <ol id="ul_t5m_vqd_bv">
      <li>Log into the lifecycle manager host.</li>
      <li>Execute the following command for each Standard Region controller to determine if the
        MySQL cluster is
          running:<codeblock>ssh cghelion@&lt;IP of Controller&gt; "sudo mysql keystone -e \"show status where Variable_name in ('wsrep_cluster_size','wsrep_cluster_status','wsrep_incoming_addresses');"\"</codeblock><p>If
          the cluster is down you should see:</p><image
            href="C:/Users/phaneend/git/carrier.grade.docs/CarrierGrade2.1/Images/carrier-grade-restart-mysql1.png"
            id="image_l3d_qjd_1w"></image></li>
      <li>Execute the following commands to recover the MySQL
        Cluster:<codeblock>hlm stop -c &lt;cloud-name&gt; -t FND-MDB
hlm start -c &lt;cloud-name&gt; -t FND-MDB</codeblock></li>
      <li>Execute the following command to verify that the MySQL Cluster is up and running:
            <codeblock>ssh cghelion@&lt;IP of Controller&gt; "sudo mysql keystone -e \"show status where Variable_name in ('wsrep_cluster_size','wsrep_cluster_status','wsrep_incoming_addresses');"\"</codeblock><p><image
              href="C:/Users/phaneend/git/carrier.grade.docs/CarrierGrade2.1/Images/carrier-grade-restart-mysql2.png"
            id="image_in5_dvd_bv"/></p></li>
    </ol>
    <p><b>Manual Recovery</b></p>
    <p><b>Option 1</b></p>
    <p>If Automated Recovery fails to bring back the cluster, execute the following steps on the
      lifecycle manager:</p>
    <ol id="ol_w2n_4vd_bv">
      <li>Execute the following command to stop MySQL on all controller
        nodes:<codeblock>/etc/init.d/mysql stop</codeblock></li>
      <li>Find the sequence number on all
          nodes:<codeblock>sudo cat /var/lib/mysql/grastate.dat

GALERA saved state
version: 2.1
uuid:    5bd38b4a-70e9-11e4-ad52-7647a787e29a
seqno:   1419441      ------>  sequence number
cert_index:</codeblock><p>The
          seqno may be specified as -1.</p><p>
          <codeblock>sudo cat /mnt/state/var/lib/mysql/grastate.dat

# GALERA saved state
version: 2.1
uuid: 5bd38b4a-70e9-11e4-ad52-7647a787e29a
seqno: -1
cert_index:</codeblock>
        </p><p>In this case run the following command to find the sequence number (mysql service
          must be stopped to run this):</p><p>
          <codeblock>sudo /usr/bin/mysqld_safe --wsrep-recover
141127 12:30:18 mysqld_safe Logging to '/mnt/state/var/log/mysql/error.log'.
141127 12:30:18 mysqld_safe Starting mysqld daemon with databases from /mnt/state/var/lib/mysql/
141127 12:30:18 mysqld_safe Skipping wsrep-recover for 5bd38b4a-70e9-11e4-ad52-7647a787e29a:
1420242 pair141127 12:30:18 mysqld_safe Assigning 5bd38b4a-70e9-11e4-ad52-7647a787e29a:1420242 to wsrep_start_position ---> 1420242 is the sequence number
141127 12:30:21 mysqld_safe mysqld from pid file /var/run/mysqld/mysqld.pid ended</codeblock>
        </p></li>
      <li>Start MySQL on the node with the highest sequence number (in case all the nodes except one
        results in -1 in sequence number, start the one that has
        number:<codeblock>sudo /etc/init.d/mysql bootstrap-pxc</codeblock></li>
      <li>Start mysql on the other controller
        nodes<codeblock>sudo /etc/init.d/mysql start</codeblock></li>
    </ol>
    <p><b>Option 2</b></p>
    <p>If Option 1 fails to bring back the cluster, execute the following steps:</p>
    <p>
      <ol id="ol_mbm_byd_bv">
        <li>Log into the failed controller. </li>
        <li>Execute the following command to purge
          MySQL:<codeblock>apt-get purge xinetd
apt-get purge python-mysqldb
backup /var/lib/mysql
apt-get purge percona-xtradb-cluster-server-5.5</codeblock></li>
        <li>On lifecycle manager, edit the
            <codeph>/var/hlm/clouds/&lt;cloudname>/desired_state/&lt;cloudname>/001/base/stage/ansible/hosts</codeph>
          file to comment the working controller. For
          example:<codeblock>[BASE-CCP-T1]
#BASE-CCP-T1-M1-NETCLM ansible_ssh_host=10.200.51.103
BASE-CCP-T1-M2-NETCLM ansible_ssh_host=10.200.51.104
BASE-CCP-T1-M3-NETCLM ansible_ssh_host=10.200.51.105

[BASE-CCP-T1]
#BASE-CCP-T1-M1-NETCLM ansible_ssh_host=10.200.51.103
BASE-CCP-T1-M2-NETCLM ansible_ssh_host=10.200.51.104
BASE-CCP-T1-M3-NETCLM ansible_ssh_host=10.200.51.105</codeblock></li>
        <li>Execute the following
          command:<codeblock>hlm deploy â€“c &lt;cloudname&gt; -t FND-MDB</codeblock></li>
        <li>Modify the <codeph>/etc/mysql/my.cnf</codeph> to make sure the <i>wsrep_cluster_address
          </i> variable contains all the nodes participating in the MySQL cluster. <p>If any of the
            node is found to be missing add it to the <i>wsrep_cluster_address </i> variable. For
            example:</p><p><i>wsrep_cluster_address =
              gcomm://BASE-CCP-T1-M1-NETCLM,BASE-CCP-T1-M2-NETCLM,BASE-CCP-T1-M3-NETCLM</i></p></li>
        <li>Execute the following command to restart MySQL on the failed nodes:
          <codeblock>service mysql restart</codeblock></li>
      </ol>
    </p>
    <p/>
    <p><b>Recovery of RabbitMQ:</b></p>
    <p><b>Automated Recovery</b></p>
    <ul id="ul_lvm_vqd_bv">
      <li>
        <p>To check if RabbitMQ cluster is running perform the following on all three Standard
          Region controller nodes IP from HLM :</p>
      </li>
    </ul>
    <pre>    ssh cghelion@&lt;IP of Controller&gt; "sudo rabbitmqctl cluster_status"</pre>
    <p/>
    <p>
      <image
        href="C:/Users/phaneend/git/carrier.grade.docs/CarrierGrade2.1/Images/carrier-grade-restart-rabbitmq1.png"
        width="500" id="image_mvm_vqd_bv"/></p>
    <p> </p>
    <ul id="ul_nvm_vqd_bv">
      <li>To recover RabbitMQ cluster, execute the following commands on
          HLM:<pre>    hlm stop -c&lt;your-cloud-name&gt; -t FND-RMQ</pre><p>
        and</p><pre>    hlm start -c &lt;your-cloud-name&gt; -t FND-RMQ</pre></li>
    </ul>
    <p> </p>
    <ul id="ul_ovm_vqd_bv">
      <li>ssh cghelion@&lt;IP of Controller&gt; "sudo rabbitmqctl cluster_status"</li>
    </ul>
    <p><image
      href="C:/Users/phaneend/git/carrier.grade.docs/CarrierGrade2.1/Images/carrier-grade-restart-rabbitmq2.png"
        id="image_pvm_vqd_bv"/></p>
    <p/>
    <p><b>Manual Recovery</b></p>
    <p>The following steps represent a general procedure for resetting an entire RabbitMQ cluster
      (that cannot be otherwise recovered). There may be a specific set of steps to follow for a
      particular RabbitMQ cluster problem. </p>
    <ol id="ol_ks5_tw5_1v">
      <li>Perform the following tasks on each RabbitMQ node to make sure each node is removed from
        the cluster. For HPE Helion OpenStack Carrier Grade with a fully HA configuration will have
        three nodes.<ol id="ol_ls5_tw5_1v">
          <li>Execute the following command to stop the RabbitMQ Erlang
            application<codeblock>sudo rabbitmqctl stop_app</codeblock></li>
          <li>Execute the following command to sync all data in the cluster and remove the node from
            the cluster. This task cleans the local RabbitMQ Mnesia
            database.<codeblock>sudo rabbitmqctl reset</codeblock></li>
          <li>Execute the following command to kill all RabbitMQ
            processes<codeblock>sudo pkill rabbitmq</codeblock></li>
        </ol></li>
      <li>Select one of the controller nodes (for example: controller0) and start up rabbitmq
        service:<codeblock>sudo service rabbitmq-server start</codeblock></li>
      <li>On the other two controller nodes (for example: controller1 and controller2), start up the
        RabbitMQ service and form a cluster by joining with
          controller0:<codeblock>sudo service rabbitmq-server start
sudo rabbitmqctl stop_app
sudo rabbitmqctl join_cluster rabbit@[controller#_hostname]
sudo rabbitmqctl start_app</codeblock><p>Where
          controller#_hostname is the controller number and the name of the host system. </p></li>
    </ol>
  </body>
</topic>
