<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept-wr PUBLIC "-//WindRiver.com//DTD DITA 1.2 Wind River Concept//EN" "concept-wr.dtd">
<concept-wr domains="(topic concept concept-wr)                            (topic hi-d)                            (topic indexing-d)                            (topic pr-d)                            (topic sw-d)                            (topic ui-d)                            (topic wr-sw-d)                            (topic xml-d)   " id="jow1404333768530" xml:lang="en-us" xmlns:ditaarch="http://dita.oasis-open.org/architecture/2005/">
    <!-- Modification History
   -->
    <title ixia_locid="1">Processor Tab</title>
    <shortdesc ixia_locid="2">The <uicontrol ixia_locid="3">Processor</uicontrol> tab on the
            <wintitle ixia_locid="4">Inventory Detail</wintitle> page presents processor details for
        a host.</shortdesc>
    <prolog>
        <author ixia_locid="56">Jim Owens</author>
        <author ixia_locid="5">Pedro Sanchez</author>
    </prolog>
    <conbody>
        <fig id="fig_N10027_N10024_N10001" ixia_locid="6">
            <image href="jow1404333763192.image" id="image_iqn_jlz_l4" ixia_locid="7" placement="inline"/>
        </fig>
        <p ixia_locid="8">The <uicontrol ixia_locid="9">Processor</uicontrol> tab includes the following items:</p>
        <ul id="ul_stv_nlz_l4">
            <li ixia_locid="10">
                <p ixia_locid="11">processor model, number of processors, number of cores per
                    processor, and Hyper-Threading status (enabled or disabled)</p>
            </li>
            <li ixia_locid="12">
                <p ixia_locid="13">the CPU assignments. <draft-comment author="jmaran" ixia_locid="206">Updated xref
                        to interbook format.</draft-comment>For more details, see <ph ixia_locid="207" otherprops="printonly"><cite ixia_locid="186">Titanium Server Installation:
                        </cite></ph><cite ixia_locid="208"><ph ixia_locid="209"><xref href="jow1448294124867.xml" ixia_locid="86"/></ph></cite>.</p>
            </li>
        </ul>
        <p ixia_locid="15">Two buttons are also available as follows:</p>
        <dl>
            <dlentry ixia_locid="16">
                <dt ixia_locid="17"><uicontrol ixia_locid="18">Create CPU Profile</uicontrol></dt>
                <dd ixia_locid="19">
                    <p ixia_locid="20"><draft-comment author="jowens" dir="ltr" ixia_locid="84" translate="no">US66653 info moved to new topic</draft-comment>Clicking
                        this button displays the <wintitle ixia_locid="21">Create CPU
                            Profile</wintitle> window. For more information, see <ph ixia_locid="210" otherprops="printonly"><cite ixia_locid="187">Titanium Server
                                Installation: </cite></ph><cite ixia_locid="211"><ph ixia_locid="212"><xref href="jow1404333787693.xml" ixia_locid="107"/></ph></cite>.</p>
                </dd>
            </dlentry>
            <dlentry ixia_locid="26">
                <dt ixia_locid="27"><uicontrol ixia_locid="28">Edit CPU Assignments</uicontrol></dt>
                <dd ixia_locid="29">
                    <p ixia_locid="30">This button is available only when the host is in the locked
                        state.</p>
                    <p ixia_locid="75">Clicking this button displays the <wintitle ixia_locid="78">Edit CPU Assignments</wintitle> window. On a compute node, you can use
                        this window to assign cores to specific functions, as illustrated below.
                        Unassigned cores are available for allocation to virtual machine threads.
                        Changes do not take effect until the host is unlocked. </p>
                    <fig id="fig_N1009D_N1008E_N10085_N10058_N10024_N10001" ixia_locid="33">
                        <image href="jow1404333766847.image" id="image_khm_xsz_l4" ixia_locid="34" placement="inline"/>
                    </fig>
                    <note id="note_N100CC_N100AF_N100A3_N1006C_N1002E_N10001" ixia_locid="79">
                        <p ixia_locid="76">On a controller or storage node, only the Platform
                            function is shown, and all available cores are automatically assigned as
                            platform cores.</p>
                    </note>
                    <dl>
                        <dlentry ixia_locid="63">
                            <dt ixia_locid="64">Platform</dt>
                            <dd ixia_locid="65">
                                <p ixia_locid="66">You can reserve one or more cores in each NUMA
                                    node for platform use. <ph ixia_locid="81" product="tis_std">One
                                        core on each host is required to run the operating system
                                        and associated services.</ph><ph ixia_locid="82" product="tis_cpe">For a combined controller and compute node
                                        in a Titanium Server CPE configuration, two cores are
                                        required.</ph></p>
                                <p ixia_locid="73">The ability to assign platform cores to specific NUMA nodes
                                    offers increased flexibility for high-performance
                                    configurations. For example, you can dedicate certain NUMA nodes
                                    for vSwitch or VM use, or affine VMs that require
                                    high-performance IRQ servicing with NUMA nodes that service the
                                    requests.</p>
                            </dd>
                        </dlentry>
                        <dlentry ixia_locid="67">
                            <dt ixia_locid="68">Vswitch</dt>
                            <dd ixia_locid="69">
                                <p ixia_locid="36">AVS (vSwitch) cores can be configured for each
                                    processor independently. This means that the single logical
                                    vSwitch running on a compute node can make use of cores in
                                    multiple processors, or NUMA nodes. Optimal data path
                                    performance is achieved when all AVS cores, the physical ports,
                                    and the virtual machines that use them are running on the same
                                    processor. You can affine VMs to NUMA nodes with AVS cores; for
                                    more information, see <ph ixia_locid="188" otherprops="printonly"><cite ixia_locid="189">Titanium
                                            Server Cloud Administration:</cite>
                                    </ph><cite ixia_locid="190"><ph ixia_locid="191"><xref href="jow1436210207074.xml" ixia_locid="192"/></ph></cite>. Alternatively, having AVS cores on all
                                    processors ensures that all virtual machines, regardless of the
                                    core they run on, are efficiently serviced. The example
                                    allocates two cores from processor 1 to the AVS threads.</p>
                                <note id="note_N1010F_N10101_N100F8_N100D2_N100A2_N10096_N1006B_N1002E_N10001" ixia_locid="90">
                                    <p ixia_locid="88"><draft-comment author="jowens" dir="ltr" ixia_locid="89" translate="no">US71812 added NUMA mesh
                                            info</draft-comment>When allocating vSwitch cores,
                                        consider optimizing the processing of packets to and from
                                        physical ports used for data interfaces. For more
                                        information, see <ph ixia_locid="193" otherprops="printonly"><cite ixia_locid="194">Titanium Server Cloud
                                                Administration:</cite>
                                        </ph><cite ixia_locid="195"><ph ixia_locid="196"><xref href="jow1453828152714.xml" ixia_locid="197"/></ph></cite>.</p>
                                </note>
                            </dd>
                        </dlentry>
                        <dlentry ixia_locid="70">
                            <dt ixia_locid="71">Shared</dt>
                            <dd ixia_locid="72">
                                <p ixia_locid="54">One physical core per processor can be configured
                                    as a shared CPU, which can be used by multiple VMs for low-load
                                    tasks. To use the shared physical CPU, each VM must be
                                    configured with a shared vCPU ID. For more information, see <ph ixia_locid="198" otherprops="printonly"><cite ixia_locid="199">Titanium Server Cloud
                                            Administration:</cite>
                                    </ph><cite ixia_locid="200"><ph ixia_locid="201"><xref href="jow1426625051841.xml" ixia_locid="202"/></ph></cite></p>
                            </dd>
                        </dlentry>
                    </dl>
                </dd>
            </dlentry>
        </dl>
        <p ixia_locid="61">To see how many cores a processor contains, hover over the Information
            icon.</p>
        <fig id="fig_kfq_vzq_ps" ixia_locid="58">
            <image href="jow1436300231676.image" id="image_cls_d1r_ps" ixia_locid="59" placement="inline">
                <alt ixia_locid="60">TiS available cores tooltip</alt>
            </image>
        </fig>
<!--        <section id="section_N100DD_N10029_N10001" ixia_locid="38">
            <title ixia_locid="39">CPU Topology From the CLI</title>
            <p ixia_locid="40">You can use the <cmdname ixia_locid="41">vm-topology</cmdname>
                command from the CLI on the controller nodes to explore the enumeration of sockets,
                cores, and logical processors on a compute node. The command can be executed without
                root privileges or Keystone authentication. Here is an example:</p>
            <codeblock ixia_locid="42"><systemoutput ixia_locid="43">$ </systemoutput><userinput ixia_locid="44">vm-topology -s topology</userinput><systemoutput ixia_locid="45">
compute-1:  Model:SandyBridge, Arch:x86_64, Vendor:Intel, Sockets=2, Cores/Socket=12, Threads/Core=1
+-\-\-\-\-\-\-\-\-\-\-\-+-\-\-+-\-\-+-\-\-+... +-\-\-\-+-\-\-\-+-\-\-\-+-\-\-\-+... +-\-\-\-+
|     cpu_id | 0 | 1 | 2 |... | 11 | 12 | 13 | 14 |... | 23 |
+-\-\-\-\-\-\-\-\-\-\-\-+-\-\-+-\-\-+-\-\-+... +-\-\-\-+-\-\-\-+-\-\-\-+-\-\-\-+... +-\-\-\-+
|  socket_id | 0 | 0 | 0 |... |  0 |  1 |  1 |  1 |... |  1 |
|    core_id | 0 | 1 | 2 |... | 13 |  0 |  1 |  2 |... | 13 |
|  thread_id | 0 | 0 | 0 |... |  0 |  0 |  0 |  0 |... |  0 |
| sibling_id | - | - | - |... |  - |  - |  - |  - |... |  - |
+-\-\-\-\-\-\-\-\-\-\-\-+-\-\-+-\-\-+-\-\-+... +-\-\-\-+-\-\-\-+-\-\-\-+-\-\-\-+... +-\-\-\-+

compute-2:  Model:SandyBridge, Arch:x86_64, Vendor:Intel, Sockets=2, Cores/Socket=12, Threads/Core=2
+-\-\-\-\-\-\-\-\-\-\-\-+-\-\-\-+-\-\-\-+-\-\-\-+... +-\-\-\-+-\-\-\-+-\-\-\-+-\-\-\-+... +-\-\-\-+-\-\-\-+
|     cpu_id |  0 |  1 |  2 |... | 11 | 12 | 13 | 14 |... | 46 | 47 |
+-\-\-\-\-\-\-\-\-\-\-\-+-\-\-\-+-\-\-\-+-\-\-\-+... +-\-\-\-+-\-\-\-+-\-\-\-+-\-\-\-+... +-\-\-\-+-\-\-\-+
|  socket_id |  0 |  0 |  0 |... |  0 |  1 |  1 |  1 |... |  1 |  1 |
|    core_id |  0 |  1 |  2 |... | 13 |  0 |  1 |  2 |... | 12 | 13 |
|  thread_id |  0 |  0 |  0 |... |  0 |  0 |  0 |  0 |... |  1 |  1 |
| sibling_id | 24 | 25 | 26 |... | 35 | 36 | 37 | 38 |... | 22 | 23 |
+-\-\-\-\-\-\-\-\-\-\-\-+-\-\-\-+-\-\-\-+-\-\-\-+... +-\-\-\-+-\-\-\-+-\-\-\-+-\-\-\-+... +-\-\-\-+-\-\-\-+</systemoutput></codeblock>
            <p ixia_locid="46">The example shows two compute nodes, <nameliteral ixia_locid="47">compute-1</nameliteral> with two sockets, 12 cores per socket, and no
                Hyper-Threading (Threads/Core=1), and <nameliteral ixia_locid="48">compute-2</nameliteral> with two sockets, 12 cores per socket, and
                Hyper-Threading enabled with two logical processors per core (Threads/Core=2). The
                rows <nameliteral ixia_locid="49">cpu_id</nameliteral> and <nameliteral ixia_locid="50">sibling_id</nameliteral> on <nameliteral ixia_locid="51">compute-2</nameliteral> node show the hyperthread sibling processors; cores in
                this compute node can therefore be described by the sequence [0, 24] [1, 25] [2, 26]
                ... [46, 22] [47, 23].</p>
            <p ixia_locid="52">Use the command <cmdname ixia_locid="53">vm-topology -\-help</cmdname> to list other available options
                to include information about virtual machine flavors, instances (servers), server
                groups, and  migrations in progress.</p>
        </section>
-->    </conbody>
    <related-links>
        <link href="jow1425236831500.xml" ixia_locid="203"/>
        <link href="jow1426618822854.xml" ixia_locid="204"/>
        <link href="jow1440527541028.xml" ixia_locid="205"/>
    </related-links>
</concept-wr>