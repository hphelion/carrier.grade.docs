<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE task-wr PUBLIC "-//WindRiver.com//DTD DITA 1.2 Wind River General Task//EN" "task-wr.dtd">
<task-wr id="wsd1464872409268" xml:lang="en-us">
<!-- Modification History
        
-->
    <title ixia_locid="1">Adding Ceph Storage</title>
    <shortdesc ixia_locid="2">You can add Ceph storage to a system using the CLI. This operation is
        not available from the Web administration interface.</shortdesc>
    <prolog>
        <author ixia_locid="38">Jim Owens</author>
        <author ixia_locid="3">Edward Knowlton</author>
    </prolog>
    <taskbody>
        <context id="context_N10026_N10023_N10001" ixia_locid="39">
            <p ixia_locid="40"><draft-comment author="eknowlto" ixia_locid="5">US77367 / US78833 new
                    topic</draft-comment>For more information about adding Ceph storage, see <xref href="cny1464872771057.xml" ixia_locid="41"/>.</p>
            <note id="note_N10054_N10050_N1001C_N10001" ixia_locid="15" type="caution">
                <p ixia_locid="16">Once you start configuring the Ceph backend, do not perform any
                    other installation or reconfiguration activities such as patching or upgrade
                    until all compute nodes are reconfigured successfully. </p>
            </note>
        </context>
        <prereq id="prereq_N1001F_N1001C_N10001" ixia_locid="4">
            <ul id="ul_b4q_qg2_2w">
                <li ixia_locid="6">
                    <p ixia_locid="7">a standard Titanium Server configuration, not Titanium Server CPE</p>
                </li>
                <li ixia_locid="8">
                    <p ixia_locid="9">an infrastructure network</p>
                </li>
                <li ixia_locid="10">
                    <p ixia_locid="11">no other ongoing installation or configuration activities
                        (such as patching or upgrade activities) </p>
                </li>
                <li ixia_locid="12">
                    <p ixia_locid="13">at least 20 GiB free space for the Ceph monitor on the
                        controller primary disk or an additional disk</p>
                </li>
            </ul>
        </prereq>
        <context id="context_N10050_N1001C_N10001" ixia_locid="14"/>
        <steps>
            <step id="step_N10085_N10082_N10024_N10001" ixia_locid="110">
                <cmd ixia_locid="111">If the system does not already have one, add an infrastructure network.</cmd>
                <info ixia_locid="112">
                    <p ixia_locid="113">For details, see <xref href="jow1420473404578.xml" ixia_locid="31"/>.</p>
                </info>
            </step>
            <step id="step_N1009F_N10082_N10024_N10001" ixia_locid="197">
                <cmd ixia_locid="198">Estimate whether there is enough space on the primary disk for the Ceph monitor
                    logical volume.</cmd>
                <substeps id="substeps_ckl_4kk_gw">
                    <substep ixia_locid="199">
                        <cmd ixia_locid="200">List the space on the rootfs drive used for database,
                            image, backup, and image conversion:</cmd>
                        <info ixia_locid="202">
                            <codeblock ixia_locid="163"><systemoutput ixia_locid="195">~(keystone_admin)$ </systemoutput><userinput ixia_locid="170">system controllerfs-show</userinput>
    <systemoutput ixia_locid="196">+---------------------+----------------------------------+
| Property            | Value                            |
+---------------------+----------------------------------+
| database_gib        | 10                               |
| cgcs_gib            | 10                               |
| backup_gib          | 30                               |
| img_conversions_gib | 10                               |
| created_at          | 2016-10-27T20:10:24.307707+00:00 |
| updated_at          | None                             |
+---------------------+----------------------------------+</systemoutput></codeblock>
                        </info>
                    </substep>
                    <substep ixia_locid="203">
                        <cmd ixia_locid="204">Calculate the space already used on the rootfs disk.</cmd>
                        <info ixia_locid="205">
                            <p ixia_locid="206">(<nameliteral ixia_locid="207">database_gib</nameliteral> x 2) +
                                    <nameliteral ixia_locid="208">image_gib</nameliteral> +
                                    <nameliteral ixia_locid="209">backup_gib</nameliteral> +
                                    <nameliteral ixia_locid="210">img_conversions_gib</nameliteral> =
                                    <nameliteral ixia_locid="211">space_used</nameliteral></p>
                        </info>
                        <stepxmp ixia_locid="212">
                            <p ixia_locid="213">(3 x 2) + 8 + 5 + 8 = 27</p>
                        </stepxmp>
                        <info ixia_locid="214">
                            <ul id="ul_q13_3lk_gw">
                                <li ixia_locid="215">
                                    <p ixia_locid="216">Double the <nameliteral ixia_locid="217">database_gib</nameliteral> space, to
                                        allow for space reserved for upgrade operations.</p>
                                </li>
                                <li ixia_locid="218">
                                    <p ixia_locid="219">Do not include <nameliteral ixia_locid="220">cinder_gib</nameliteral>, which
                                        uses space on the Cinder volume.</p>
                                </li>
                            </ul>
                        </info>
                    </substep>
                    <substep ixia_locid="221">
                        <cmd ixia_locid="222">Subtract the <nameliteral ixia_locid="223">space_used</nameliteral> from the total available disk space,
                            allowing at least 5 additional GiB of physical disk space for losses
                            related to drive sectoring.</cmd>
                        <info ixia_locid="224">
                            <p ixia_locid="225">(<nameliteral ixia_locid="226">disk_size_gib</nameliteral> - 5) -
                                    <nameliteral ixia_locid="227">space_used</nameliteral> =
                                    <nameliteral ixia_locid="228">space_available</nameliteral></p>
                        </info>
                        <stepxmp ixia_locid="229">
                            <p ixia_locid="230">50 - 5 - 27 = 18</p>
                        </stepxmp>
                    </substep>
                </substeps>
                <stepresult ixia_locid="233">
                    <p ixia_locid="234">If the <nameliteral ixia_locid="235">space_available</nameliteral> is less than the minimum 20
                        GiB requirement for the Ceph monitor, you must use another disk on the
                        controller (you cannot use the Cinder disk for this purpose). To add a disk,
                        see <xref href="gfu1465307467334.xml" ixia_locid="236"/>.</p>
                </stepresult>
            </step>
            <step id="step_N10180_N10082_N10024_N10001" ixia_locid="242">
                <cmd ixia_locid="243">On the active controller, become the Keystone <nameliteral ixia_locid="244">admin</nameliteral>
                    user.</cmd>
            </step>
            <step id="step_N10061_N1005E_N1001C_N10001" ixia_locid="17">
                <cmd ixia_locid="18">On the active controller, run the CLI command to add the Ceph
                    backend.</cmd>
                <info ixia_locid="42">
                    <p ixia_locid="43">Use a command of the following form:</p>
                    <codeblock ixia_locid="97"><systemoutput ixia_locid="98">~(keystone_admin)$ </systemoutput><userinput ixia_locid="99">system storage-backend-add ceph \
[--ceph-mon-gib <varname ixia_locid="100">partition_size</varname>] [--ceph-mon-dev <varname ixia_locid="48">device_name</varname>]</userinput></codeblock>
                    <p ixia_locid="101">where</p>
                    <parml>
                        <plentry ixia_locid="53">
                            <pt ixia_locid="54"><varname ixia_locid="55">partition_size</varname></pt>
                            <pd ixia_locid="56">
                                <p ixia_locid="57">is the size of the partition to allocate on a
                                    controller disk for the Ceph monitor logical volume, in GiB (the
                                    default value is 20)</p>
                            </pd>
                        </plentry>
                        <plentry ixia_locid="58">
                            <pt ixia_locid="59"><varname ixia_locid="60">device_name</varname></pt>
                            <pd ixia_locid="61">
                                <p ixia_locid="62">is the device to use on each controller (for
                                    example, <option ixia_locid="63">/dev/sdc</option>)</p>
                            </pd>
                        </plentry>
                    </parml>
                    <note id="note_N1011B_N10118_N10085_N10082_N10024_N10001" ixia_locid="109">
                        <p ixia_locid="114"> If you need to specify a different device on each
                            controller, you can use an alternative form of the command:</p>
                        <codeblock ixia_locid="103"><systemoutput ixia_locid="104">~(keystone_admin)$ </systemoutput><userinput ixia_locid="105">system storage-backend-add ceph \
[--ceph-mon-gib <varname ixia_locid="106">partition_size</varname>] \
[--ceph-mon-dev-controller-0 <varname ixia_locid="50">disk_uuid1</varname> --ceph-mon-dev-controller-1 <varname ixia_locid="51">disk_uuid2</varname>]</userinput></codeblock>
                        <p ixia_locid="107">where</p>
                        <parml>
                            <plentry ixia_locid="70">
                                <pt ixia_locid="71"><varname ixia_locid="72">disk_uuid1</varname></pt>
                                <pd ixia_locid="73">
                                    <p ixia_locid="74">is the unique identifier for the <nameliteral ixia_locid="94">controller-0</nameliteral> disk to
                                        use</p>
                                </pd>
                            </plentry>
                            <plentry ixia_locid="75">
                                <pt ixia_locid="76"><varname ixia_locid="77">disk_uuid2</varname></pt>
                                <pd ixia_locid="78">
                                    <p ixia_locid="79">is the unique identifier for the <nameliteral ixia_locid="95">controller-1</nameliteral> disk to
                                        use</p>
                                </pd>
                            </plentry>
                        </parml>
                    </note>
                </info>
                <stepxmp ixia_locid="19">
                    <p ixia_locid="96">For example:</p>
                    <draft-comment author="eknowlto" ixia_locid="20">Suggest rewording of the system output messages
                        below, e.g., instead of "Please follow the administrator guide to complete
                        configuring system" perhaps "Refer to the Titanium Server documentation for
                        more information..."</draft-comment>
                    <codeblock ixia_locid="21"><systemoutput ixia_locid="80">~(keystone_admin)$ </systemoutput><userinput ixia_locid="81">system storage-backend-add ceph --ceph-mon-dev /dev/sdc</userinput>

<systemoutput ixia_locid="23">WARNING : THE OPERATION IS NOT REVERSIBLE AND CANNOT BE CANCELLED.       
By continuing this operation, CEPH backend will be created.
Minimum 2 storage nodes are required to complete the configuration.
Please refer to hardware guide for minimum spec for storage nodes. 

Continue [yes/N]:</systemoutput> <userinput ixia_locid="24">yes</userinput>
<systemoutput ixia_locid="25">System configuration has changed. Please follow the administrator guide to complete configuring system.

+---------+-------------+---------------------+
| backend | state       | task                |
+---------+-------------+---------------------+
| ceph    | configuring | reconfig-controller |
| lvm     | configured  | None                |
+---------+-------------+---------------------+</systemoutput></codeblock>
                    <p ixia_locid="237">If you omit the <option ixia_locid="116">ceph-mon-dev</option> option, the default rootfs disk is used for the
                        Ceph monitor. </p>
                    <p ixia_locid="238">If there is not enough space on the specified disk, an error
                        message appears.</p>
                    <codeblock ixia_locid="239"><systemoutput ixia_locid="240">Total target configured size 47 GigaBytes for database_gib (doubled for upgrades), image_gib, img_conversions_gib, backup_gib and ceph_mon_gib exceeds limit of 45 GigaBytes.</systemoutput></codeblock>
                    <p ixia_locid="241">Specify a smaller partition size, or use the <option ixia_locid="119">ceph-mon-dev</option> option to specify another
                        disk.</p>
                </stepxmp>
                <stepresult ixia_locid="26">
                    <p ixia_locid="27">This command starts the process of adding a Ceph storage
                        backend. Both controllers are marked as <nameliteral ixia_locid="82">Config
                            out-of-date Reboot Required</nameliteral> in the Web administration
                        interface.</p>
                </stepresult>
            </step>
            <step id="step_N1008F_N1005E_N1001C_N10001" ixia_locid="28">
                <cmd ixia_locid="29">Lock and unlock each controller to apply the
                    configuration.</cmd>
                <substeps id="substeps_vmd_phj_fw">
                    <substep ixia_locid="83">
                        <cmd ixia_locid="84">Lock and unlock the standby controller.</cmd>
                        <info ixia_locid="245">
                            <p ixia_locid="246">Wait for the controller to be reported as
                                    <nameliteral ixia_locid="247">Unlocked</nameliteral>,
                                    <nameliteral ixia_locid="248">Available</nameliteral>, and
                                    <nameliteral ixia_locid="249">Online</nameliteral>.</p>
                        </info>
                    </substep>
                    <substep ixia_locid="85">
                        <cmd ixia_locid="86">Swact the controllers.</cmd>
                    </substep>
                    <substep ixia_locid="87">
                        <cmd ixia_locid="88">Lock and unlock the new standby controller.</cmd>
                        <info ixia_locid="250">
                            <p ixia_locid="251">Wait for the controller to be reported as
                                    <nameliteral ixia_locid="252">Unlocked</nameliteral>,
                                    <nameliteral ixia_locid="253">Available</nameliteral>, and
                                    <nameliteral ixia_locid="254">Online</nameliteral>.</p>
                        </info>
                    </substep>
                </substeps>
            </step>
            <step id="step_N100A0_N1005E_N1001C_N10001" ixia_locid="32">
                <cmd ixia_locid="33">Add storage hosts <nameliteral ixia_locid="34">storage-0</nameliteral> and <nameliteral ixia_locid="35">storage-1</nameliteral>, and unlock them. For complete instructions, see
                        <ph ixia_locid="182" otherprops="printonly"><cite ixia_locid="183">Titanium
                            Server Installation for Ceph-backed Systems:</cite>
                    </ph><cite ixia_locid="184"><ph ixia_locid="185"><xref href="uid1473354209921.xml" ixia_locid="255">Installing Software on
                                Controller-1 or a Compute or Storage Host</xref></ph></cite> .</cmd>
                <stepresult ixia_locid="36">
                    <p ixia_locid="37">Once both storage hosts are unlocked, the system enables a
                        Ceph-monitor check process, which ensures that at least two Ceph monitors
                        are available at all times.</p>
                </stepresult>
            </step>
            <step id="step_N1019D_N1007F_N10023_N10001" ixia_locid="90">
                <cmd ixia_locid="91">Add more storage-host pairs as required, up to the maximum supported by the
                    system (four pairs).</cmd>
            </step>
        </steps>
    </taskbody>
</task-wr>